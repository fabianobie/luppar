MÁQUINA DE VETORES DE SUPORTE BASEADA EM MÍNIMOS QUADRADOS E EVOLUÇÃO
DIFERENCIAL APLICADA  IDENTIFICAÇÃO DE UM PROCESSO TÉRMICO
GLAUBER S. SANTOS1, LUIZ G. J. LUVIZOTTO2, VIVIANA C. MARIANI2 ,
LUÍS M. MOURA2 E LEANDRO DOS S. COELHO3
1
Graduação em Engenharia Mecatrônica (Controle e Automação)
2
Programa de Pós-Graduação em Engenharia Mecânica, PPGEM
3
Programa de Pós-Graduação em Engenharia de Produção e Sistemas, PPGEPS
Pontifícia Universidade Católica do Paraná
Rua Imaculada Conceição, 1155, 80215-901, Curitiba, PR, Brasil
E-mails glauber.santos@pucpr.br, luizluvizotto@hotmail.com,
viviana.mariani@pupr.br, luis.moura@pucpr.br, leandro.coelho@pupr.br
Abstract
 Support vector machines (SVMs) are non-parametric supervised learning schemes rely on statistical learning theory
which enables learning machines to generalize well to unseen data. In SVMs designs for nonlinear identification, a nonlinear
model is represented by an expansion in terms of nonlinear mappings of the model input. The nonlinear mappings define a
feature space, which may have infinite dimension. In this context, a relevant identification approach is the least squares support
vector machines (LS-SVMs). Compared to the other identification method, LS-SVMs possess prominent advantages its
generalization performance (i.e. error rates on test sets) either matches or is significantly better than that of competing methods,
and more importantly, the performance does not depend on the dimensionality of the input data. Consider a constrained
optimization problem of quadratic programming with a regularized cost function, the training process of LS-SVM involves the
selection of kernel parameters and the regularization parameter of the objective function. A good choice of these parameters is
crucial for the performance of the estimator. In this paper, the LS-SVMs design proposed is the combination of LS-SVM and
differential evolution (DE) optimization method. DE is adopted in tuning of regularization parameter and the radial basis
function bandwith. Experiments of LS-SVMs on NARX (Nonlinear AutoRegressive with eXogenous inputs) to identification of
a thermal process show the effectiveness and practicality of the proposed algorithm.
Keywords
 Support Vector Machines, Nonlinear identification, Differential Evolution, Thermal process.
Resumo
 Máquinas de vetor de suporte (SVMs) são esquemas de aprendizagem supervisionada não-paramétricas baseadas em
teoria de aprendizagem estatística que permitem aprendizagem de máquinas para generalizar aos dados ocultos. Em projetos
SVMs para identificação_não-linear, um modelo_não-linear é representado por uma expansão em termos de mapeamentos nãolineares do modelo de entrada. Os mapeamentos não-lineares definem um espaço de busca, que pode ter dimensão infinita.
Neste contexto, uma abordagem de identificação relevante é a SVM por mínimos_quadrados (LS-SVM). Comparado com o
método de identificação, LS-SVM possuem vantagens importantes a generalização (taxas de erros nos conjuntos de teste) casa
ou é significativamente melhor do que a dos métodos concorrentes, e ainda mais, o desempenho não depende da
dimensionalidade dos dados de entrada. Considere um problema de otimização condicionada de programação_quadrática com
uma função de custo regularizada, o processo de formação do SVM-LS envolve a seleção de parâmetros do núcleo e o parâmetro
de regularização da função objetivo. Uma escolha adequada destes parâmetros é crucial para o desempenho do estimador. Neste
trabalho, o projeto LS-SVM proposto é a combinação de SVM-LS e o método de otimização evolução_diferencial (DE). DE é
adotado no ajuste do parâmetro de regularização e da largura da função_de_base_radial. Experimentos de LS-SVMs em NARX
(Nonlinear AutoRegressive com entradas exógenas) para identificação de um processo_térmico mostram a eficácia e praticidade
do algoritmo proposto.
Palavras-chave
 .

1

conhecido como aprendizado estatístico (detalhes em
Vapnik, 1998).
A teoria de SVM foi originalmente elaborada
para solução de problemas de classificação, através
da aplicação do conceito de hiperplano ótimo de
separação, baseado na maximização da margem de
separação.
Com a utilização de estruturas denominadas
núcleos (kernels), o uso de SVMs foi além dos
hiperplanos gerados inicialmente, sendo então
capazes de classificações e regressões não-lineares,
mapeando os dados de entrada num espaço de
características de alta dimensionalidade.
No âmbito de previsão de identificação de
sistemas, tem sido apresentados na recente literatura
diferentes abordagens de metaheurísticas bioinspiradas para o projeto de SVMs, incluindo-se

Introdução

Com o avanço tecnológico e industrial, o
interesse pela modelagem não-linear e o
desenvolvimento de ferramentas matemáticas para
entender melhor o comportamento dos fenômenos
não-lineares cresceram significativamente, uma vez
que as técnicas existentes para modelos lineares não
conseguem
reproduzir
toda
a
gama
de
comportamentos dinâmicos dos sistemas reais
(Coelho et al., 2002).
Uma abordagem promissora para aplicações em
identificação não_linear são as máquinas de vetores
de suporte (SVMs). As SVMs foram desenvolvidas
com base no paradigma de aprendizado de máquina,

114

um conjunto finito de dados (Vapnik, 1999). Uma
das vantagens da SVM é seu alto poder de
generalização. Isto ocorre porque a complexidade da
hipótese não depende do número de atributos, mas
sim da margem com que eles separam os dados.
Segundo Vapnik (1999), a SVM é um
procedimento construtivo universal de aprendizagem.
O termo universal significa que a SVM pode ser
utilizada para o aprendizado de várias representações
como as redes_neurais artificiais, as funções de base
radial, splines e funções polinomiais.
A diferença principal entre redes_neurais
artificiais e SVM é o princípio de minimização do
risco. Enquanto uma rede_neural artificial implementa
a minimização de risco empírico para minimizar o
erro em um conjunto de dados de treinamento a SVM
implementa o princípio do risco estrutural para
construção de um hiperplano de separação no espaço
oculto.

algoritmos_genéticos (Wu et al., 2009 Yang et al.,
2010), enxame_de_partículas (Guo et al., 2008 Tang
et al., 2009 Wu, 2010), sistemas imunológicos
(Hong, 2009) e colônia_de_formigas (Niu et al.,
2010).
A contribuição do artigo é o projeto e a avaliação
de uma abordagem de evolução_diferencial (Storn e
Price, 1995) para a otimização dos parâmetros de
controle do LS-SVMs (least squares support vector
machines) (Suykens et al., 2002) na identificação de
um processo_térmico.
LS-SVMs é uma classe de método que usa
funções de núcleo definidas positivas para construir
representação não-linear de entradas em um espaço
multidimensional.
Em termos de otimização, a evolução_diferencial
(ED) é um algoritmo_evolutivo que utiliza uma
abordagem gulosa para resolução de problemas de
otimização. As potencialidades da ED incluem sua
estrutura simples, a facilidade de utilização, a
propriedade de convergência e também a robustez,
que são características atrativas para um projeto em
conjunto com o LS-SVMs.
O restante do artigo é organizado da seguinte
forma. Na seção 2 são descritos os fundamentos de
SVMs, LS-SVMs e LS-SVMs combinados com ED.
A descrição do processo_térmico e a análise dos
resultados da aplicação do LS-SVM combinado com
ED são detalhadas nas seções 3 e 4, respectivamente.
Na seção 5, a conclusão é apresentada.
2

2.1. LS-SVM
Em termos do problema da otimização do
treinamento da SVM, inicialmente o problema era
tido como de programação_quadrática, porém foram
desenvolvidas outras formas como otimização
seqencial mínima (sequential minimal optimization)
(Lima, 2009) e o método dos mínimos_quadrados
(LS, least squares). Dessa forma, as máquinas de
vetor de suporte se estabelecem como uma das mais
estudadas ferramentas de aprendizado de máquina.
No LS são usadas restrições de igualdade ao invés
de restrições de desigualdade e a função custo é uma
soma do erro quadrático. O método LS também tem
como vantagem o menor custo_computacional.
Conforme, mencionado anteriormente, a técnica
SVM visa ajustar os vetores de suporte para aqueles
definidos em um hiperplano que visa separar os
dados de entrada. A SVM aproxima a relação entre a
saída e a entrada usando a seguinte equação

Fundamentos de SVMs e LS-SVMs

A SVM é uma técnica usada para o treinamento
de classificadores baseada no conceito da
minimização do risco estrutural (Burges, 1998). A
técnica SVM foi desenvolvida por Vladimir Vapnik
(Vapnik, 1993).
O princípio de minimização do risco estrutural é
baseado no fato de que a taxa de erro de uma
máquina de aprendizado no seu conjunto de teste é
limitado pela soma dos erros de teste e por um valor
que depende da dimensão VC (VapnikChervonenkis). Em outras palavras, se observa que
tanto a dimensão VC quanto o risco empírico devem
ser minimizados simultaneamente (Lima, 2009).
Apesar da boa fundamentação teórica do
princípio da minimização do risco estrutural, o
mesmo pode ser difícil de ser implementado pela
dificuldade em se calcular a dimensão VC de uma
hipótese, e pela dificuldade da solução do problema
de otimização aliado a isto. O sucesso é obtido pelo
treinamento das máquinas de vetor de suporte, que
consegue minimizar simultaneamente a taxa de erro
de treinamento e a taxa de erro de generalização
(Lima, 2009).
Em sua forma básica, as SVMs são
classificadores lineares que separam os dados em
duas classes através de um hiperplano de separação,
que é construído com base em treinamento prévio em

y  w ( x ) + b ,

(1)

onde b é um limiar (threshold) escalar, w é um
coeficiente de ponderação e  (x) é uma nãolinearidade mapeada a partir da entrada.
Os coeficientes w e b são estimados pela
minimização da seguinte função de risco, J, dada por
Minimizar
1
J W
2

2

+

1 N
  ( yi , f ( xi ) ) ,
2 i 1

(2)

tal que

0,
yi , f ( xi )  
 yi , f ( xi )   , outros.

 ( yi , f ( xi ) )  

(3)

onde W é o vetor de ponderações e  é um parâmetro
de regularização que estabelece um equilíbrio entre a
complexidade do modelo e o erro de treinamento.

115

Neste artigo, uma função_de_base_radial (RBF,
radial basis function) foi adotada como núcleo,
sendo que esta é dada por

A primeira parte da função objetivo dada pela
equação (2) é um decaimento utilizado para
regularizar os pesos e penalizar aqueles que são
elevados. Devido a esta regularização, os pesos
convergem para valores menores. Este procedimento
é necessário, pois pesos elevados causam excessiva
variância do modelo, deteriorando a capacidade de
generalização das LS-SVM e podendo ocorrer sobreajustes. A segunda parte da equação (2) representa os
erros de regressão para todos os dados de
treinamento. A restrição de igualdade imposta pela
equação (3) fornece a definição do erro de regressão
(Borin, 2007).
Para o caso de padrões não-linearmente
separáveis, é necessário adicionar variáveis ao
problema. Ou seja, pela introdução de variáveis
soltas,  i e  i* , é possível transformar a equação
(3) em uma função objetivo primal dada por
Minimizar

J
subjeito a

1
W
2

2

+

(

)

1 N
*
 i +i ,
2 i 1

yi  W   ( xi )  b   +  i

W   ( xi ) + b  yi  
onde i  1,..., N e

 i ,  i*

 x x
 i
j
K xi , x j  exp
2
 2


(

O procedimento de treinamento da LS-SVM
envolve a seleção dos parâmetros do núcleo e do
parâmetro de regulação. Neste contexto, Deng e Tian
(2004), Kwok (2001) e Smola e Schlkpf (1998)
mencionam recomendações para um apropriado
ajuste destes parâmetros, mas não existe um consenso
quanto ao que foi mencionado nestas referências.
No entanto, neste artigo, a ED foi adotada no
intuito de otimizar os parâmetros  e  utilizados no
LS-SVM. Assim, neste artigo, um software
denominado Matlab SVM toolbox (Pelckmans et al.,
2002) foi utilizado em conjunto com a ED no
problema de identificação do processo_térmico.
A ED é um paradigma da computação_evolutiva
(ou evolucionária) desenvolvido por Rainer Storn e
Kenneth Price (Storn e Price, 1995 Storn, 1997) para
problemas_de_otimização não-linear contínua.
Basicamente, a ED realiza mutações nos vetores pela
adição ponderada de diferenças aleatórias entre eles.
A escolha da ED para problemas_de_otimização é
baseada nas características de que a ED

é um algoritmo de busca estocástica que é
motivado pelos mecanismos de seleção natural

é menos susceptível a mínimos (ou máximos)
locais, pois busca a solução ótima global pela
manipulação de uma população de soluções
candidatas, ou seja, busca um número de
diferentes áreas simultaneamente no espaço de
busca

não requer informação de derivadas para o
cálculo da função de aptidão

manipula diretamente os números de ponto
flutuante (fenótipo), diferente dos algoritmos
genéticos canônicos ou binários que manipulam
cadeias de bits (genótipo), o que diminui o
custo
computacional
necessário
para
transformação de uma representação genotípica
em fenotípica

geralmente não necessita de populações grandes
para funcionar eficientemente.

(4)

 0.

Introduzindo-se os vetores de multiplicadores de
Lagrange  i e  i* (os vetores de suporte), a função
núcleo e maximizando-se a função dual da equação
(4), a função de regressão dada pela equação (1)
apresenta a seguinte forma explicita

(

)

N

(

) (

(

i 1

)

)

(7)

onde K xi , x j é a função núcleo. Os vetores  i são
obtidos resolvendo-se o sistema linear de equações,
seguindo as condições de Karush-Kuhn-Tucker.
O valor de K xi , x j é igual ao produto interno

(

(8)

2.2. Abordagem LS-SVM usando ED

(6)

f x,  i ,  i*    i   i*  K xi , x j + b







onde  é a largura das gaussianas do núcleo.

(5)

+  i*

)

2

)

de dois vetores xi e xj no espaço de características,
 (xi )
e
 xj ,
significando
que

( )
K (xi , x j )   ( xi )T  (x j ) .

A utilização da função núcleo visa substituir o
cálculo de  (xi ) e  x j , que é complexo, por uma

( )

forma simples por meio de uma função aproximada.
Esses núcleos geram um mapeamento entre o
espaço de entrada e um espaço de alta
dimensionalidade, chamado espaço de características.
O hiperplano gerado pela SVM nesse espaço de
características, ao ser mapeado de volta ao espaço de
entrada, se torna uma superfície não-linear. Assim
sendo, o hiperplano de separação passa a ser não
mais uma função linear dos vetores de entrada, mas
uma função linear de vetores do espaço de
características (Lima, 2009).

As estratégias da ED podem variar de acordo com
o tipo de indivíduo a ser modificado na formação do
vetor doador, o número de indivíduos considerados
para a perturbação e o tipo de cruzamento a ser
utilizado, podendo ser escritas como ED,
onde

116

  especifica o vetor a ser perturbado, podendo ser
rand (um vetor da população escolhido
aleatoriamente) ou best (o vetor de menor custo
da população)
  determina o número de diferenças ponderadas
usadas para a perturbação de 
  denota o tipo de cruzamento (exp. exponencial
bin binomial).

indivíduos

xi3 ( g )

são selecionados

Etapa 6 Aplicar a operação de cruzamento Após a
operação de mutação, o cruzamento (ou
recombinação) é aplicado a população. O cruzamento
é empregado para gerar um novo vetor tentativa ou
vetor doador (trial vector) pela substituição de certos
parâmetros do vetor destino (target vetor) pelos seus
parâmetros correspondentes ao vetor doador, estes
gerados aleatoriamente.
Nesta operação, para cada vetor, zi(g+1), um
rnbr (i )  1, 2, L , n 
é
escolhido
índice
aleatoriamente usando uma função densidade de
probabilidade uniforme, e um vetor denominado de
vetor
tentativa
dado
por

Etapa 1 Iniciar os parâmetros de controle da
evolução_diferencial O projetista deve escolher os
parâmetros de controle da ED, tais como tamanho da
população (M), limites (máximos e mínimos) das
variáveis de otimização, taxa de mutação (fm(t)), taxa
de cruzamento (CR) e o critério de parada do
procedimento de otimização.





ui (g +1)  ui1(g +1), ui2 (g +1),...,uin (g +1) T . Neste caso

é gerado um novo vetor tal que,

Etapa 2 Iniciar o contador de gerações Atribuir
geração inicial, g1.

zi j ( g + 1), se randb(j)  CR ou j  rnbr(i) ,
ui j ( g + 1)  
xi j ( g), caso contrário,
(10)

Etapa 3 Iniciar a população inicial de indivíduos
(soluções) Gerar uma população inicial aleatória,
com distribuição uniforme, de soluções factíveis 
resolução do problema em questão, onde as regras de
reparo garantem que os valores atribuídos as
variáveis estão internas as fronteiras delimitadas pelo
projetista.

onde randb(j) é a j-ésima avaliação da geração de
um número aleatório com distribuição uniforme no
intervalo 0, 1 e CR é a taxa de cruzamento (ou
recombinação) no intervalo 0, 1. Geralmente, o
desempenho do algoritmo de ED depende do projeto
de três variáveis o tamanho da população, M, a taxa
de mutação, fm(g), e a taxa de cruzamento, CR.

Etapa 4 Avaliar os indivíduos da população Avaliar
a função objetivo (custo) de cada um dos indivíduos
da população.

Etapa 7 Aplicar a operação de seleção A seleção é
um procedimento em que os melhores
descendentes (indivíduos filhos) são produzidos. Para
decidir se o vetor ui(g+1) será (ou não) um membro
da população na próxima geração, ele é comparado
com o vetor xi(g). Assim considerando que F denota
a função objetivo sob maximização, então

Etapa 5 Aplicar a operação de mutação (ou
operação diferença) A mutação é uma operação que
adiciona um vetor diferencial para o vetor dos
indivíduos da população, de acordo com a equação
1

e

aleatoriamente com i1  i 2  i3  i , e o vetor da
diferença xi2 - xi3 é calculado.

Na ED clássica, cada variável (indivíduo) é
representada por um valor real (ponto_flutuante). A
variante implementada neste trabalho foi a
EDrand1bin, que é regida pelas seguintes etapas

zi ( g + 1)  x i ( g ) + MF ( g )   xi 2 ( g )  xi3 ( g )

xi2 ( g )

(9)

onde i1,2,...,M é o índice do indivíduo da
população j1,2,...,n é a j-ésima posição do
indivíduo (solução potencial) em um espaço de
dimensão n g é a geração (iteração)

u ( g + 1), se F (ui ( g + 1)) > F ( xi ( g )),
xi ( g + 1)   i
 xi ( g ), nos outros casos.
(11)

xi ( g )  xi1 ( g ), xi2 ( g ), ..., xin ( g ) T
consiste
da
posição do i-ésimo indivíduo de uma população de M
vetores
n-dimensionais

Neste caso, o valor da função objetivo de cada
vetor tentativa ui(g+1) é comparado com seu vetor
destino xi(g). Se o valor da função objetivo, F, do
vetor destino xi(g) tiver valor maior que o valor da
função objetivo do vetor tentativa, é permitido ao
vetor destino continuar na próxima geração. Caso
contrário, o vetor destino é substituído, na próxima
geração, pelo vetor tentativa.









zi ( g )  zi1 ( g ), zi2 ( g ), ..., zin ( g ) T é responsável
pela posição do i-ésimo indivíduo de um vetor que
sofrerá mutação r1, r2 e r3 são valores inteiros
mutuamente diferentes, selecionados aleatoriamente
com
distribuição
uniforme
do
conjunto
1, 2,L , i  1, i + 1,L , N   MF(g) > 0 é um
parâmetro real denominado de taxa de mutação, que
pondera a diferença entre os dois indivíduos para
evitar a estagnação da busca. Deve-se mencionar que
a operação de mutação seleciona aleatoriamente o
vetor destino xi1 ( g ) com i  i1 . Então, dois

Etapa 8 Verificar se o critério de parada foi
atendido Atribuir  geração g  g + 1. Retornar para
a Etapa 4 até que o critério de parada seja atendido,
usualmente o número de gerações, gmax.

117

3 Descrição do processo_térmico (secador)

65
60

O processo_térmico utilizado no procedimento de
identificação foi o processo de controle de
temperatura e umidade do ar fabricado pela empresa
TS Equipamentos Eletrônicos.
O processo é constituído por um soprador de
velocidade variável, uma resistência e sensores de
temperatura (termopar tipo J) e de umidade (marca
Honeywell) para medir a temperatura e umidade do
ar na câmara de secagem.
O controle_de_temperatura na entrada do
processo é realizado por meio de uma saída PWM
(Pulse Width Modulation) atuando em uma
resistência elétrica. A capacidade do secador é de
aproximadamente 1 kg, o qual deve ser colocado
numa bandeja perfurada. Neste caso, o ar quente
insuflado pelo ventilador atravessa o leito no sentido
vertical saindo pela parte superior do processo (Neto
e Teruel, 2008). A figura 1 ilustra a estrutura do
processo de secagem.

temperatura (oC)

55
50
45
40
35
temperatura na parte inferior
temperatura na parte superior

30
25

0

100

200

300

400
amostra

500

600

700

800

600

700

800

700

800

(a) temperatura
28
26

umidade_relativa do ar ()

24
22
20
18
16
14
12

0

100

200

300

400
amostra

500

(b) umidade_relativa do ar
110
100
90

sinal de entrada

80
70
60
50
40
30
20
10
0

0

100

200

300

400
amostra

500

600

(c) velocidade do soprador (em )
Figura 1. Processo térmico.
Figura 2. Sinais obtidos do processo_térmico.

Os dados coletados do processo_térmico para
análise foram obtidos em malha_aberta com um sinal
de entradas u do tipo degrau, ou seja, 100, 50 e
1 da velocidade máxima do soprador e período de
amostragem de 1 segundo.
A figura 2 mostra os sinais relativos  temperatura
(em C) obtida por dois termopares, um na parte
inferior (próximo ao motor do ventilador) e outro na
parte superior do processo que são representados
pelas variáveis y2 e y3, respectivamente. Além disso,
os dados da umidade_relativa do ar, y1, e o sinal de
entrada u (em  da velocidade máxima do soprador)
também são apresentados.

Foram coletadas 720 amostras. As simulações de
identificação foram configuradas para utilizar as
amostras 1 a 400 para a fase de estimação e as
amostras 401 a 720, para a fase de validação do LSSVM baseado em ED.
Para identificação foi escolhido um modelo
matemático para a representação do processo. A
estrutura de modelo testada foi o modelo
multivariável NARX (Nonlinear AutoRegressive with
eXogenous inputs) em concepção série-paralela
MISO (Multiple Inputs, Single Output) com 4
entradas e 1 saída, isto é, as entradas u(t-1) y1(t-1)
y2(t-1), y3(t-1) e a saída é y 3 (t ) .
O índice de desempenho adotado para avaliação
da qualidade da identificação do processo_térmico foi
118

110,7965 e   107,5673 com FED  0,530077. Com
estes valores de configuração o LS-SVMs obteve
R 2 (estimação)  0,954309

o coeficiente_de_correlação múltipla R 2 regido pela
equação
N

2
 ( y3 (t )  y 3 (t ))

R 2  1  t 1N
,
2
(
y
(
t
)
y
)

 3
3

R 2 (validação)  0,841559 e

(12)

R 2 (estimação)  0,894394.

t 1

Estes valores de

onde y(t) é a saída real do processo, y 3 (t ) é a saída

confirmam o desempenho promissor do LS-SVMs na
identificação do processo_térmico quanto a previsão
um passo  frente.
Na figura 3 são apresentados os sinais de saída e
do erro de identificação do melhor projeto LS-SVMs.
O erro máximo de identificação foi de 0,545707,
enquanto os erros médio e a variância do erro foram,
respectivamente, de 0,007133 e 0,029965.

estimada pelo LS-SVM, y3 é a média das medidas
do processo_térmico. Neste artigo, o objetivo é obter
um valor para R 2 o mais próximo do valor unitário.
Neste caso, a função objetivo FED a ser
maximizada pela ED no projeto LS-SVMs é baseada
na média harmônica entre os valores das fases de
estimação e validação do R 2 , tal que

FED 

(1  (

2
Restimação

2
.
2
+  + 1  Rvalidação
+

)) ( (

))

R 2 próximos da unidade

46

(13)

44
42

onde  é uma constante (  1,010 ) inserida na
equação com intuito de evitar divisões por zero.
-300

4

temperatura

40

Resultados de identificação do processo

38
36
34
32

A abordagem de LS-SVMs foi otimizada com
uma EDrand1bin com a seguinte configuração
 tamanho de população, M 10 indivíduos
 taxas de mutação e cruzamento, respectivamente,
com valores MF  0,4 e CR  0,8
 simulações 30 (usando diferentes sementes de
números aleatórios para gerar a população inicial
de vetores solução da ED para o LS-SVMs)
 critério de parada da otimização usando ED
gmax 30 gerações (total de 300 avaliações da
função objetivo por geração)
 espaço de busca dos parâmetros para o projeto
LS-SVMs   0500 onde   0200.

saída real
saída estimada

30
28

0

100

200

300

400
amostra

500

600

700

800

600

700

800

(a) temperatura
0.8
0.6
0.4

sinal de erro

0.2
0
-0.2
-0.4

Na tabela 1 é apresentada uma análise_estatística
de desempenho da ED no procedimento de
otimização do projeto LS-SVMs. Nota-se que a ED
apresentou resultados próximos em termos de valores
maior e médio da função objetivo (pequena
variância) para 30 simulações. Isto mostra que a
técnica foi robusta quanto ao processo de otimização.

-0.6
-0.8

0

100

200

300

400
amostra

500

(b) sinal de erro
Figura 3. Resultado de identificação para o projeto LS-SVM
baseado em ED.

5 Conclusão e pesquisa futura

Tabela 1. Convergência da ED no projeto LS-SVMs em
termos de FED.
Máximo
Mínimo
Média
Variância
0,530077
0,527873
0,527974
4,16910-4

A abordagem LS-SVMs apresenta características
atrativas, tais como a habilidade de modelar relações
não-lineares e a função de regressão é relacionada a
um problema quadrático com solução global e única.
Entretanto, um projeto adequado dos parâmetros  e
 é essencial para um bom desempenho da
abordagem LS-SVMs. Neste contexto, este artigo

A figura 3 mostra o melhor resultado (em termos
do valor máximo de FED descrito na tabela 1) de
identificação obtido com a LS-SVMs combinada com
ED. Neste caso, os valores obtidos foram  

119

Computação e Automação, Universidade Federal
do Rio Grande do Norte, Natal, RN.
Lima, N., Neto, A. D., Melo, J. (2009). Creating an
ensemble of diverse support_vector_machines
using adaboost, Proceedings on International
Joint Conference on Neural Networks, Atlanta,
USA, pp. 2342-2346.
Neto, A. R. e Teruel, B. (2008). Supervisão e
controle_automático de sistema de secagem de
produtos agrícolas através do software
TERMICONT, BioEng, vol. 2, no. 1, pp. 71-79.
Niu, D., Wang, Y., Wu, D. D. (2010). Power load
forecasting using support_vector_machine and ant
colony optimization, Expert Systems with
Applications, vol. 37, no. 3, pp. 2531-2539.
Pelckmans, K., Suykens, J. A. K., Van Gestel, T., De
Brabanter, J., Lukas, L., Hamers, B., De Moor, B.
e Vandewalle J. (2002). LS-SVMlab a MatlabC
toolbox for least squares support_vector_machines,
Internal Report 02-44, ESAT-SISTA, K. U.
Leuven, Belgium, (presented at NIPS2002
Vancouver in the demo track).
Smola, A. e Schlkopf, B. (1998). A tutorial on
support vector regression, NeuroCOLT Technical
Report NC-TR-98-030, Royal Holloway College,
University of London, UK.
Storn, R. e Price, K. (1995). Differential evolution a
simple and efficient adaptive scheme for global
optimization over continuous spaces, Technical
Report TR-95-012, International Computer
Science Institute, Berkeley, USA.
Storn, R. e Price, K. (1997). Differential evolution 
a simple and efficient heuristic for global
optimization over continuous spaces, Journal of
Global Optimization, vol. 11, no. 4, pp. 341-359.
Suykens, J. A. K., Van Gestel, T., De Brabanter, J.,
De Mooor, B. e Vandewalle, J. (2002). Least
squares support_vector_machines, World
Scientific, Singapore.
Tang, X., Zhuang, L. e Jiang, C. (2009). Prediction
of silicon content in hot metal using support
vector regression based on chaos particle swarm
optimization, Expert Systems with Applications,
vol. 36, no. 9, pp. 11853-11857.
Vapnik, V. N. (1993). Statistical learning theory,
Journal of the Association for Computing
Machinery, vol. 40, pp. 741-764.
Vapnik, V. N. (1998). Statistical learning theory,
Wiley, New York, NY, USA.
Vapnik, V. N. (1999). An overview of statistical
learning theory, IEEE Transactions on Neural
Networks, vol. 10, no. 5, pp. 988-999.
Wu, C. -H., Tzeng, G. -H. e Lin, R. -H. (2009). A
novel hybrid genetic algorithm for kernel function
and parameter optimization in support vector
regression, Expert Systems with Applications, vol.
36, no. 3, pp. 4725-4735.
Wu, Q. (2010). Product demand forecasts using
wavelet kernel support_vector_machine and
particle_swarm_optimization in manufacture

propõe a utilização da ED como uma ferramenta
viável e útil para o projeto LS-SVMs.
Os resultados obtidos foram promissores para
identificação de um processo_térmico em termos de
R2 (estimação) e de R2 (validação).
A pesquisa futura relacionada a este artigo deve
focar a identificação do processo_térmico para
diferentes pontos de operação, ou seja, diferentes
valores do sinal de entrada (tensão aplicada ao
soprador).
Agradecimentos
Os autores agradecem o apoio financeiro do
Conselho Nacional de Desenvolvimento Científico e
Tecnológico  CNPq (processos 5682212008-7,
4744082008-6, 3027862008-2-PQ e 30396320093PQ).
Referências Bibliográficas
Borin, A. (2007). Aplicações de máquinas de vetores
de suporte por mínimos_quadrados (LS-SVM) na
quantificação de parâmetros de qualidade de
matrizes lácteas, Tese de doutorado, Instituto de
Química, Universidade Estadual de Campinas
(UNICAMP), Campinas, SP.
Burges, C. J. C. (1998). A tutorial on support vector
machines for pattern recognition, Data Mining
and Knowledge Discovery, vol. 2, no. 2, pp. 121167.
Coelho, M. C. S., Aguirre, L. A. e Correa, M. V.
(2002). Metodologia para representação de
modelos NARX polinomiais na forma de
Hammerstein e Wiener, Tendências em
Matemática Aplicada e Computacional, vol. 3,
no. 1, pp. 71-80.
Deng, N. Y., Tian, Y. J. (2004). New method of data
mining  support_vector_machine, Science Press.
Beijing, China, pp. 18-32.
Guo, X. C., Yang, J. H., Wu, C. G., Wang,, C. Y. e
Liang, Y. C. (2008). A novel LS-SVMs hyperparameter selection based on particle swarm
optimization, Neurocomputing, vol. 71, no. 1618, pp. 3211-3215.
Hong, W. -C. (2009). Electric load forecasting by
support vector model, Applied Mathematical
Modelling, vol. 33, no. 5, pp. 2444-2454.
Kwok, J. T. (2001). Linear dependency between e
and the input noise in -SVR (support vector
regression), Proceedings of International
Conference on Neural Networks (ICANN 2001),
vol. 2130, Lecture Notes in Computer Science,
Springer, Heidelberg, Germany, pp. 405-410.
Lima, N. H. C. (2009). Comitê de máquinas SVM
utilizando reforço adaptativo, Monografia de
conclusão de curso, Curso de Engenharia de
Computação, Departamento de Engenharia de

120

system, Journal of Computational and Applied
Mathematics, vol. 233, no. 10, pp. 2481-2491.
Yang, Z., Gu, X. S., Liang, X. Y. e Ling, L. C.
(2010). Genetic algorithm-least squares support
vector regression based predicting and optimizing
model on carbon fiber composite integrated
conductivity, Materials  Design, vol. 31, no. 3,
pp. 1042-1049.

121