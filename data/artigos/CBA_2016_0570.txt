XXI Congresso Brasileiro de Automática - CBA2016
UFES, Vitória - ES, 3 a 7 de outubro

NEURAL NETWORKS APPLIED A INTELLIGENT SYSTEM OF SPEECH
RECOGNITION BASED ON DCT PARAMETRIC MODELS OF LOW ORDER
Priscila Rocha, Washington Silva, Allan Barros


Federal University of Maranhao
Sao Lus, Maranhao, Brazil


Federal Institute of Maranhao
Sao Lus, Maranhao, Brazil

Emails priscilalimarocha@.hotmail.com, washington.silva@ifma.edu.br,
akduailibe@gmail.com
Abstract This paper proposes the development of a Numerical Command Recognition System of Speech
Signal based on Neural Networks and DCT models. Thus, two configurations of neural networks, the Multilayer
Perceptron (MLP) and Learning Vector Quantization (LVQ) are evaluated by their performance in speech signal
recognition, whose encoding is made by the mel-cepstral coefficients that are used to generate a two-dimensional
time matrix by Discrete Cosine Transform (DCT). The selection of the best configuration of neural network
for classification of the patterns was carried out by comparative analysis of performance of the MLP and LVQ
networks through training, validation and test of the network topology and learning algorithms previously established. For demonstration of the performance of the proposed analysis methodology, the obtained results were
compared with other methods of classification given by Gaussian Mixture Models (GMM) and Support Vector
Machines (SVM).
Keywords

Automatic Speech Recognition, Neural Network, Discrete Cosine Transform.

Resumo Este artigo propoe o desenvolvimento de um Sistema de Reconhecimento de Comandos Numericos de Sinal de Voz baseado nas configuracoes de redes_neurais Perceptron de Multiplas Camadas (MLP) e a
Aprendizado por Quantizacao Vetorial (LVQ). A codificacao do sinal de voz e feita por meio dos coeficientes
mel-cepstrais que sao usados para gerar uma matriz temporal bidimensional pela aplicacao da Transformada Cosseno Discreta (TCD). Para demonstrar o desempenho da metodologia de analise proposta, os resultados obtidos
foram comparados com outros metodos de classificacao, dados pelos Modelos de Misturas Gaussianas (GMM) e
Maquinas de Vetores de Suporte (SVM).
Palavras-chave

.

Introduction

The extensive development of research in the
speech processing area shows the effort to improve
the performance of speech recognition systems for
practical applications. The use of such systems allow autonomy in areas such as telephony, wherein
service requests are directed through speech commands (Cardoso et al., 2010) in the automotive
industry through the activation of devices inside
vehicles in computing systems by utility programs
on computers, besides the application in robotics,
residential and hospital automation for accessibility of people with locomotor and visual disease
(A.Cubukcu et al., 2015).
Therefore, it is proposed in this paper the
development of a Numerical Command Recognition System of Speech Signal with two configurations of neural networks, the Perceptron Multilayer and Learning Vector Quantization. They
will be evaluated by its performances in speech signal recognition, whose encoding is done using the
mel-cepstral coefficients (MFCC) and discrete cosine transform (DCT) (Silva e Serra, 2014). The
mel-cepstral coefficients and the DCT are used
for generating of a two-dimensional time matrix
which represents, through a reduced set of parameters, the pattern of speech signal to be used as

ISSN 2525-8311

training, validation and testing input of the networks neural.
1.1

Proposal Analysis Methodology

The proposed speech recognition system in this paper aims to classify patterns of speech signals, represented by locutions from Brazilian Portuguese
of the digit 0, 1, 2, 3, 4, 5, 6, 7, 8, 9,
through the recognizer characterized by the neural network. The adopted methodology uses a reduced number of parameters to represent each pattern obtained in the speech signal pre-processing
stage for generating two-dimensional time matrices 2, 3 e 4. The time two-dimensional matrix reproduces the global and local variations in time, as
well as the spectral envelope of the speech signal.
The use of neural network as classifier in speech
recognition system using the low-order parameters
generated by the two-dimensional matrix of low
temporal order and the comparison of the results
are the main contributions of this work.
The analysis of performance between two configuration MLP and LVQ Neural Networks is carried out in two phases first, the specified topologies the observation of the behavior of the MLP
and LVQ networks in training and validation process and the selection of the topologies that accom-

2001

XXI Congresso Brasileiro de Automática - CBA2016
UFES, Vitória - ES, 3 a 7 de outubro

plish global validation with hit above 80. Then
the selected topologies are tested with different parameters of the speech signal that are not used in
the training process and the results of classifications of the MLP and LVQ neural networks are
shown. The study performance of LVQ Neural
Network, presented in this paper, applied in the
speech recognition with low-order models provides
an alternative approach to the MLP classifier that
is the neural network most used in speech recognition problems (Haykin, 2001).
2

Speech Recognition System based on
Networks Neural

It is shown in Figure 1 the block diagram of the
proposed speech recognition system.

Speech Signal

Generation
Two-Dimensional
DCT Time Matrix

Adjusting Elements
of the
Neural Network

Defined
Patterns Class

Speech Signal
Recognition
System
(Neural Network)

Identification
Digits Class

Train

Test
Segmentation
and
Windowing

Coding
Mel-cepstrais
Coefficients

Figure 1 Block diagram of the Speech Recognition System using Neural Networks

2.1

Generation of two-dimensional DCT time
matrix

After obtaining the mel-cepstral coefficients from
the speech signal, encoding through discrete cosine transform (DCT) was carried out, which enables to synthesize the long-term variations of
the spectral envelope of the speech signal. The
result of this encoding was the generation of a
two-dimensional DCT time matrix (Silva e Serra,
2014), obtained by (1)


T
(2t  1)n
1 X
(1)
Ck (n, T ) 
mfcck (t) cos
N t1
2T
where k, that varies of 1  k  K, is the k-th
line component of t-th segment of the matrix. K
is the number of mel-cepstral coefficient n, that
varies of 1  n  N , is the n-th column. n is the
order of the matrix DCT T is the number of vectors of observation of the mel-cepstral coefficients
in time axis mfcck (t) represent the mel-cepstral
coefficients.
Thus, for each locution of the digit D to be
recognized there is a two-dimensional DCT time
jm
matrix Ckn
, where j  0, 1, 2,    , 9 represent the
digit to be encoded and m  0, 1, 2,    , 9 represent the example taken for each digit. These majm
,
trices were transformed in column vectors CN
where N is the number of parameters of the twojm
jm
dimensional matrix Ckn
. The column vector CN

ISSN 2525-8311

preserves the temporal alignment of mel-cepstral
coefficients and its general term is given by (2)
jm
jm
jm jm
jm
jm
jm
CN
cjm
11 , c12 ,  , c1n c21 , c22 ,  , c2n ,  , ckn 



(2)
jm
were used as input patterns
The vectors CN
jm
of the neural network. The dimension of CN
defines the number of input of the neural network.
In order to compare the performance of the neural network when the number of parameters that
compose the input patterns of speech is increased,
jm
two-dimensional matrices Ckn
were generated of
order n  2, 3 e 4, thereby obtaining the patterns
to be recognized by neural networks represented
jm
by CN
, where N  4, 9 e 16, respectively.
2.2

Design of Neural Networks

The design of the neural networks is carried out
through simulations of combinations of elements
of the network topology and learning algorithms
previously established and also based on obtained
results in other similar works of patters classification. The configurations of MLP and LVQ network were used to the pattern recognition according to speech signal encoding by two-dimensional
time DCT matrix. Then, in Table 1 and Table 2
present, respectively, the elements related to topology and training algorithms previously established
for MLP and LVQ networks that were combined
during the training phase.
Table 1 Training elements of MLP Neural Networks
Elements
Training
Algorithms
No of Hidden Layers
No of Hidden Neuron
1 Hidden Layer
No of Hidden Neuron
2 Hidden Layers
Learning Rate
Momentum Term
No of Epoch
Activation Function

Symbol

Defined Range

-

GD,GDM,RP,LM

1

-

1e2

n1

20,25,30,35,40,45,50,55,60

n1 e n2

n2  15



-

0.01
0.8
1000
Hyperbolic Tangent

Table 2 Training elements of LVQ Neural Networks
Elements
Training
Algorithms
No of Hidden Layers
Learning Rate
No of Epoch

Symbol

Defined Range

-

LVQ-1

n1

-

20,25,30,35,40,45,50,55,60
0.01
1000

It can be seen in Tables 1 and 2 that the LVQ
network has a smaller number of elements that can
be associated to the simulations of training. This
is due to simplicity of the architecture and topology of the LVQ network. For MLP configuration
networks with two hidden layers were simulated
1 GD Gradient Descent GDM Gradient Descent
with Momentum RP Resilient Backpropagation LM
Levenberg-Marquardt

2002

XXI Congresso Brasileiro de Automática - CBA2016
UFES, Vitória - ES, 3 a 7 de outubro

in order to verify the need to increase the number
of hidden layers to extract the features contained
in the input patterns presented to the network.
During the research conducted for the preparation of this paper, it was observed that the initialization of the set of weights for the MLP neural
network impacted the final results. Therefore, to
realize this fact, four training (T1 , T2 , T3 , T4 ) with
different initializations of the set of weights made
over a random uniform distribution between interval 0.01, 0.01 was carried out for each proposed
topology for MLP network. Thus, it was possible
to observe the behavior of neural networks in relation to training time and the ability to generalize,
because appropriate set of initial weights allows a
reduction in training time and a high probability
of achieving the overall minimum of error function.
In addition, this set can significantly improve performance in generalization.
2.3

(a)

(b)

Training and Testing Sets

The Training and testing of the MLP and LVQ
Neural Networks were carried out with patterns
of speech signals from selection of locutions from
voice bank EPUSP, INATEL and IFMA. These
sets are changed depending on the order of the
two-dimensional matrix used to generate the patterns. The training and test sets are composed as
follows
TNrL 

1. Training Sets
This set represents the
number of parameters of patterns of the set,
where N  4, 9, 16, L the total number of
locutions and T r indicates that is training set,
is composed of 200 locutions, where half are
female speakers and the other half are male
speakers. The training set was partitioned in
the estimation subset E
N (it contains 80 of
the total of TNrL ) and in the validation subset
VN (it contains
 20 ofVthe total of training
set) (TNr200  E
N  N ).
2. Testing Set TN L  For this set were selected 20
speakers, where 10 speakers are female (TNFL )
and 10 speakers are male (TNM
L ). All speakers belong to IFMA bank, but are speakers
who did not participate with pronunciations
for the training set. In total, the testing set
has 1000 male locutions
and 1000 female lo
TF
cutions (TN 2000  TNM
1000  N 1000 ).
3
3.1

Experimental Results

LVQ Training and Validation

After training the LVQ networks by all combinations of defined topology elements, it is shown in
Figure 2a, Figure 2a, Figure 2b and Figure 2c, respectively, the obtained results with training of
jm
networks using the patterns C4jm , C9jm and C16
.
ISSN 2525-8311

(c)
Figure 2 Result of Training and Validation
Global Hit (a) LVQ C4jm , (b) LVQ C9jm e (c)
jm
LVQ C16

3.2

LVQ Testing

The tests were applied only in trained topologies
with correct classification results in the global validation greater than 80. The obtained results (in
percent) with the application of test sets TNM
1000
and TNF1000 with N  4, 9 e 16 are shown, respectively, in Table 3, Table 4 and Table 5.
Finishing tests, the topology with best performance was chosen according to criterion that besides presenting the highest mean results in tests
must also have a reduced number of neurons.
Thus, topologies with mean hit of test detached in blue are those that showed the highest results, however topologies with mean hit detached in red have very close results but with fewer
neurons. Therefore, the topology of 40, 20 and 25
neurons shown in Table 3, Table 4 and Table 5 are
the topology with best result of generalization for
LVQ neural network configuration.

2003

XXI Congresso Brasileiro de Automática - CBA2016
UFES, Vitória - ES, 3 a 7 de outubro

Table 3 LVQ C4jm  Female and Male Speakers
Tests
Loc F1
Loc F2
Loc F3
Loc F4
Loc F5
Loc F6
Loc F7
Loc F8
Loc F9
Loc F10
Loc M1
Loc M2
Loc M3
Loc M4
Loc M5
Loc M6
Loc M7
Loc M8
Loc M9
Loc M10
Mean

35

40

45

55

60

89
95
72
81
90
98
56
72
54
68
71
77
74
72
63
72
62
70
72
76
74.2

91
90
80
84
79
90
57
76
69
78
62
77
77
70
64
65
72
79
63
81
75.2

94
90
67
70
85
95
46
72
53
79
59
86
76
79
63
65
66
74
62
88
73.45

92
87
73
67
83
93
48
64
59
81
60
85
79
83
66
65
71
73
63
78
73.5

94
88
78
76
87
89
55
79
74
77
62
85
78
79
67
69
68
75
65
83
76.4

From the results shown in Figure 3, Figure 4 and
Figure 5, it is possible to verify the influence of
the set of initial weights for the MLP network. It
is observed that for the same topology, one set of
weights initialized randomly in a training led to
satisfactory response, but in another training, the
set of initial weights resulted in an undesirable response.

(a)

(b)

(c)

(d)

Table 4 LVQ C9jm  Female and Male Speakers
Tests
Loc F1
Loc F2
Loc F3
Loc F4
Loc F5
Loc F6
Loc F7
Loc F8
Loc F9
Loc F10
Loc M1
Loc M2
Loc M3
Loc M4
Loc M5
Loc M6
Loc M7
Loc M8
Loc M9
Loc M10
Mean

20

25

30

35

40

45

50

55

60

88
86
81
87
82
90
72
87
73
64
69
81
80
80
64
90
82
81
69
77
79.15

77
88
84
86
82
83
67
83
72
68
64
81
81
69
60
81
74
69
67
73
75.45

81
86
81
70
80
80
69
79
70
66
63
76
72
72
69
72
76
73
66
76
73.85

81
85
80
71
80
80
72
70
69
62
59
79
75
71
66
75
75
72
69
78
73.45

88
78
84
81
73
89
78
90
77
66
69
78
78
71
58
86
83
80
55
73
76.75

89
87
83
87
87
90
75
86
85
64
68
82
79
80
61
86
75
71
73
77
79.25

78
85
79
69
80
78
70
72
68
59
59
73
65
72
68
74
75
71
64
73
71.6

76
85
80
69
84
79
71
78
72
64
66
76
72
75
64
77
74
69
66
76
73.65

87
84
82
88
83
84
70
82
78
64
70
82
75
73
69
83
84
81
71
76
78.3

Figure 3 MLP C4jm  Result of Training and Validation Global Hit - 1 hidden layer LM and RP
algorithm

jm
Table 5 LVQ C16
 Female and Male Speakers
Tests
Loc F1
Loc F2
Loc F3
Loc F4
Loc F5
Loc F6
Loc F7
Loc F8
Loc F9
Loc F10
Loc M1
Loc M2
Loc M3
Loc M4
Loc M5
Loc M6
Loc M7
Loc M8
Loc M9
Loc M10
Mean

3.3

20

25

30

35

40

45

50

55

60

88
86
90
89
85
90
92
98
88
75
69
84
76
87
61
78
79
80
68
76
81.95

94
95
90
85
96
99
73
95
80
80
80
87
85
88
74
78
84
84
79
90
85.8

89
90
83
87
87
89
80
97
84
70
77
81
77
79
72
80
75
73
68
80
80.9

89
88
83
87
89
90
80
99
86
73
80
83
76
83
67
83
82
82
69
78
82.35

95
95
90
84
97
99
74
93
80
80
78
88
86
89
79
86
82
82
79
88
86.2

81
79
79
84
78
82
77
88
80
76
72
79
76
81
54
73
73
73
60
75
76

91
94
89
88
87
90
83
97
81
72
75
87
84
89
66
87
85
82
77
77
84.05

92
88
84
87
89
89
78
95
87
73
79
89
85
87
75
82
81
77
73
82
83.6

90
87
87
88
88
90
82
97
85
71
76
84
76
84
68
78
79
77
68
77
81.6

MLP Training and Validation

In Figure 3, Figure 4 and Figure 5 are shown, respectively, the global hit results of training and
validation obtained to MLP networks with one
hidden layer, trained by LM and RP training aljm
gorithms using the patterns C4jm , C9jm and C16
.

ISSN 2525-8311

(a)

(b)

(c)

(d)

Figure 4 MLP C9jm  Result of Training and Validation Global Hit - 1 hidden layer LM and RP
algorithm

3.4

MLP Testing

As well as carried out for the LVQ neural network,
after completion of training and validation stage,

2004

XXI Congresso Brasileiro de Automática - CBA2016
UFES, Vitória - ES, 3 a 7 de outubro

Table 7 Selection of Best Test Results MLP C9jm
Column 1

Column 2

Column 3

Column 4

1 layer
LM
25-T3

(a)

(b)

(c)

(d)

jm
Figure 5 MLP C16
 Result of Training and Validation Global Hit - 1 hidden layer for the LM and
RP algorithm

RP
45-T1

90
94
93
95
91
89
76
91
82
78
74
82
89
88
65
83
69
69
88
82
83.4

Loc F1
Loc F2
Loc F3
Loc F4
Loc F5
Loc F6
Loc F7
Loc F8
Loc F9
Loc F10
Loc M1
Loc M2
Loc M3
Loc M4
Loc M5
Loc M6
Loc M7
Loc M8
Loc M9
Loc M10
Mean

Column 5

2 layers
LM
50-T4

93
90
93
94
97
92
87
91
76
79
76
84
87
90
65
86
73
71
82
80
84.3

RP
60-T1

92
93
91
95
96
93
85
92
77
66
83
80
85
89
64
78
77
72
84
87
83.95

95
93
92
96
96
91
85
95
83
77
79
82
91
88
65
84
68
69
85
90
85.2

jm
Table 8 Selection of Best Test Results MLP C16
Column 1

Column 2

Column 3

Column 4

1 layer
LM
20-T1

between the obtained results in the simulations,
the topologies of MLP Networks that showed
global hit results of validation greater than 80
were selected for application testing. The best
results (in percent) found in executed tests, considering the networks trained with one and two
layer by RP and LM algorithms, using the test
TF
sets TNM
1000 and N 1000 with N  4, 9 e 16 are
shown, respectively, in Table 6, Table 7 and Table
8.

RP
50-T2
95
91
97
93
90
97
92
92
82
73
84
91
90
85
59
89
74
71
82
93
86

Loc F1
Loc F2
Loc F3
Loc F4
Loc F5
Loc F6
Loc F7
Loc F8
Loc F9
Loc F10
Loc M1
Loc M2
Loc M3
Loc M4
Loc M5
Loc M6
Loc M7
Loc M8
Loc M9
Loc M10
Mean

Column 5

2 layers
LM
40-T1

88
87
98
88
93
89
89
93
85
80
74
89
86
92
64
88
79
80
82
91
85.75

RP
30-T2

88
89
98
86
93
91
94
97
82
82
74
89
86
94
67
88
83
85
83
92
87.05

92
89
97
94
95
93
93
93
79
84
86
90
91
93
69
89
88
83
82
92
88.6

Table 6 Selection of Best Test Results MLP C4jm
Column 1

Loc F1
Loc F2
Loc F3
Loc F4
Loc F5
Loc F6
Loc F7
Loc F8
Loc F9
Loc F10
Loc M1
Loc M2
Loc M3
Loc M4
Loc M5
Loc M6
Loc M7
Loc M8
Loc M9
Loc M10
Mean

Column 2 Column 3
1 layer
LM
RP
30-T4
45-T3
94
82
95
97
83
84
87
79
91
88
95
97
67
57
91
91
75
59
84
67
72
77
89
81
86
82
82
80
72
72
67
64
73
68
80
74
73
73
87
91
82.15
78.15

Column 4 Column 5
2 layers
LM
RP
45-T1
45-T3
88
89
90
94
82
84
82
83
88
88
94
99
65
63
85
90
73
70
77
81
77
74
81
85
86
85
78
80
71
68
66
64
73
75
76
80
70
70
93
93
79.75
80.75

Finishing tests, as carried out for the LVQ networks, the topology of MLP network with best performance was chosen according to criterion that
besides presenting the highest mean results in
tests must also to have less complexity in the topological structure. Thus, topologies with mean hit
of test detached in blue are those that showed the
highest results, however topologies with mean hit

ISSN 2525-8311

detached in red have very close results but with
fewer neurons and only one layer. Therefore, the
topology of 30, 25 and 20 neurons with only one
hidden layer trained by LM algorithm shown in
Table 6, Table 7 and Table 8 are the topology with
best result of generalization for MLP neural network configuration. At the end the tests carried
out with the MLP and LVQ networks and selected
the topology of best performance for each configujm
ration, for each of the three sets of patterns CN
,
N  4, 9 and 16, the obtained results are summarized in the Table 9

Table 9 Final Performance Summary of the MLP
and LVQ networks
C4jm
o

LVQ
MLP

N of
Neurons
40
30

 Test
Hit
75.2
82.15

C9jm
o

N of
Neurons
20
25

 Test
Hit
79.15
83.4

jm
C16
o

N of
Neurons
25
20

 Test
Hit
85.8
86

2005

XXI Congresso Brasileiro de Automática - CBA2016
UFES, Vitória - ES, 3 a 7 de outubro

4

Comparison to other methods used in
speech recognition

To check the performance of the methodology presented in this work, it was carried out the comparison of obtained results with MLP and LVQ neural networks with recognizers based on Gaussian
Mixture Models (GMM) and Support Vector Machine (SVM). Thus, for comparison, the input parameters for the three recognizers were the same,
that is, the elements Ckn for each pattern j. In
Tables 10 and 11 test results are presented for female and male speakers, respectively (Batista e
Silva, 2015 Beserra et al., 2015).
Table 10 Comparison of speech recognition results using neural networks, SVM e GMM for female speakers. OMOrder of Matrix

Loc F1

Loc F2

Loc F3

Loc F4

Loc F5

LVQ
MLP
SVM-Poli
SVM-RBF
GMM
LVQ
MLP
SVM-Poli
SVM-RBF
GMM
LVQ
MLP
SVM-Poli
SVM-RBF
GMM
LVQ
MLP
SVM-Poli
SVM-RBF
GMM
LVQ
MLP
SVM-Poli
SVM-RBF
GMM

OM  2

OM  3

OM  4

91
94
68
74
92
90
95
65
80
94
80
83
60
78
88
84
91
66
78
70
79
95
66
78
70

88
90
62
76
84
86
94
65
80
89
81
93
60
78
88
82
91
72
72
72
90
89
72
72
72

94
95
65
78
88
95
91
66
80
82
90
97
77
80
95
96
90
72
82
83
99
97
72
82
83

Table 11 Comparison of speech recognition results using neural networks, SVM e GMM for male
speakers. OMOrder of Matrix

Loc M1

Loc M2

Loc M3

Loc M4

Loc M5

LVQ
MLP
SVM-Poli
SVM-RBF
GMM
LVQ
MLP
SVM-Poli
SVM-RBF
GMM
LVQ
MLP
SVM-Poli
SVM-RBF
GMM
LVQ
MLP
SVM-Poli
SVM-RBF
GMM
LVQ
MLP
SVM-Poli
SVM-RBF
GMM

ISSN 2525-8311

OM  2

OM  3

OM  4

77
89
70
76
57
77
86
67
76
80
77
82
62
78
52
77
72
68
76
66
79
87
66
76
72

81
82
66
80
64
80
89
63
63
87
80
88
63
80
67
64
65
63
80
70
77
82
66
80
74

87
91
73
80
72
85
90
71
81
91
88
85
70
78
77
74
59
69
80
71
90
93
74
82
86

5

Conclusion

According obtained results, it is concluded that
the MLP and LVQ configurations were able to extract the features of two-dimensional time DCT
matricies of low order provided as patterns of digits to be classified. An important point verified
in this paper is the influence of the values of initial weights in the result achieved by MLP Neural
Network. Comparison of results for the speech
recognition system using MLP and LVQ Neural
Networks with other methodologies, such as SVMpolynomial, the SVM-RBF and the GMM, showed
that the proposed methodology in this paper has
good performance in solving of the problem in
question, becoming feasible to use in speech recognition systems.
Acknowledgment
The authors would like to thank the Federal Institute of Maranhao (IFMA) and Program in Electrical Engineering of Federal University of Maranhao
(PPGEEUFMA).
References
A.Cubukcu, Kuncan, M., Kaplan, K. e Ertunc,
H. M. (2015). Development of a voicecontrolled home automation using zigbee
module, Signal Processing and Communications Applications Conference (SIU), 2015
23th, pp. 18011804.
Batista, G. B. e Silva, W. L. S. (2015). Application of support_vector_machines to recognize speech patterns of numeric digits, Natural Computation (ICNC), 2015 11th International Conference on, pp. 831836.
Beserra, A. A., Silva, W. L. e Serra, G. (2015).
A gmmcpso speech recognition system, Industrial Electronics (ISIE), 2015 IEEE 24th
International Symposium on, pp. 2631.
Cardoso, S. A., Castanho, J. E. C., Franchin,
M. N. e Fontes, I. R. (2010). Sesame sistema de reconhecimento de comandos de
voz utilizando pds e rna, Anais do XVIII
Congresso Brasileiro de Automatica, Mato
Grosso, Brazil, pp. 13161323.
Haykin, S. (2001). Redes Neurais, BOOKMAN
COMPANHIA ED.
Silva, W. e Serra, G. (2014). Intelligent genetic
fuzzy inference system for speech recognition An approach from low order feature
based on discrete cosine transform, Journal
of Control, Automation and Electrical Systems 25(6) 689698.

2006