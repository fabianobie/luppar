Anais do XIX Congresso Brasileiro de Automática, CBA 2012.

IDENTIFICATION OF ARBITRARY MODEL PARAMETERIZATIONS THE USES
OF RITTS ALGORITHM
Alexandre Sanfelice Bazanella, Rafael Rui


Universidade Federal do Rio Grande do Sul, School of Engineering,
PPGEE - Graduate Program of Electrical Engineering
Av. Osvaldo Aranha 103
Porto Alegre, RS - Brazil
Emails bazanela@ece.ufrgs.br, rafael.rui@ufrgs.br

Abstract In this paper we propose the use of Ritts algorithm for the identification of parameters in models
with nonstandard parameterizations. The statistical properties of the resulting parameter estimate are discussed,
as well as the determination of informativity of experiments.
Keywords

Parametric identification, Nonlinear Systems, Ritts algorithm, Identifiability, Informativity.

Resumo Neste artigo propomos a utilizacao do algoritmo de Ritt para a identificacao de parametros em
modelos com parametrizacao fora do padrao. Apresentamos tambem as propriedades estatsticas dos parametros
estimados, bem como a determinacao da indo experimento.
Palavras-chave
.

1

Identificacao parametrica, Sistemas nao lineares, Algoritmo de Ritt, identificabilidade, in-

Introduction

The development of Ritts algorithm dates back
to the 1950s, so that it seems fair to say that
it constitutes a classical subject in differential
algebra. The usefulness of Ritts algorithm in
analyzing the identifiability of certain nonlinear
continuous-time model structures has been known
in the engineering community for at least eighteen years (Ljung and Glad, 1994) and more recently its application to the analysis of identifiability in discrete-time systems has also been presented (Lyzell et al., 2011). Ritts algorithm transforms a given parametric model structure into a
linear regression that is equivalent, in the sense
that the parameter values explaining the linear
regression are exactly the same that explain the
original structure. In so doing, it convexifies any
identification problem that has a unique solution.
Not surprisingly, the benefit of convexification provided by Ritts algorithm comes at a cost,
which is that of highly complex analytical expressions which tend to appear in the equivalent
linear regression. As a rule, this cost has been
deemed excessively high by the scientific community, since it is hard to find applications of Ritts
algorithm and it is not as widespread as could be
expected. Indeed, its application to actual (practical) examples is largely unexplored, and only very
simple examples have been presented (Ljung and
Glad, 1994) and (Lyzell et al., 2011). The potential of Ritts algorithm as an alternative for
the identification procedure has not been much
explored either, as it is usually seen as a theoretical tool for better understanding of the identification problem andor as a means to verify identifiability, particularly for nonstandard parameteri-

ISBN 978-85-8001-069-5

zations. Even for small sized problems, for which
the expressions are complex yet treatable even by
hand, the use of Ritts algorithm for solving actual identification problems is hardly - if ever considered as an alternative.
Yet, Ritts algorithm is potentially useful even
for identification of linear systems. Indeed, nonconvexity and the consequent possibility of occurrence of local minima, or even several global
minima, of the cost function in Prediction Error
Identification (PEI) of linear systems is a current
concern in the community (Ljung, 2010), (Lyzell
et al., 2011),(Gevers et al., 2009) and (Eckhard
and Bazanella, 2011).
This paper discusses the properties and potentials of Ritts algorithm, (hopefully) shedding
some light into its practical applicability and
opening new avenues for future exploration. We
propose the use of Ritts algorithm for the identification of nonstandard model parameterizations,
beyond its use as a theoretical analysis tool. The
resulting properties of informativity and the statistical properties - consistency and precision - of
the resulting parameter estimate are analyzed and
illustrated by case studies. We start by presenting the basics of PEI in Section 2, to set the stage
and fix nomenclature and notation. A brief presentation of Ritts algorithm is given in Section
3, followed in Section 4 by a discussion of its use
along with the least square (LS) identification of
the resulting linear regression. Then we present,
in Section 5, the application of Ritts algorithm
to a number of case studies. We apply the algorithm to determine identifiability but also to study
the experiment informativity and to actually perform the identification. In Section 6 concluding
remarks are given and future lines of work are dis-

3577

Anais do XIX Congresso Brasileiro de Automática, CBA 2012.

cussed.

2.1
2

A linear parameterization is one in which the predictor can be written as

Preliminaries

Let u(t) and y(t) stand respectively for the input
and output of a discrete-time, single-input-singleoutput dynamic_system at time t. We consider the
Prediction Error Identification (PEI) of a timeinvariant single-input single-output real system
S  y(t)  H((t), (t))  0

(1)

where (t)  y(t  1), . . . , y(t  n), u(t), u(t 
1), . . . , u(tm), and (t)  e(t), e(t1), . . . , e(t
l), e(t) being a white noise sequence with variance
e2 , and the vector field H(, )  n+m+1 l+1 
 is Lipschitz. We will use the notation
x(t)  x(t  1)
and notice that, although only discrete-time systems are treated in this paper to simplify the presentation, continuous-time systems can be treated
in the same way.
In this paper we consider the identification of
parametric models
M  y(t)  h((t), (t), )  0

(2)

y(t, )  h((t), 0, )  h ((t))T ,
where h ()  n+m+1  p does not depend on

. The vector (t)  h ((t)) is called the regressor vector and each one of its elements is called a
regressor.
When the predictor is linear in the parameters the cost function is quadratic, in which case
there is an analytical solution to the PEI the LS
solution
!1 N
N
X
X
T
N 
(t) (t)
(t)y(t).
t1

Assumption 1 The real system S belongs to the
model set M (or simply S  M), i.e.  0 such
that
h((t), (t), 0 )  H((t)(t)).
Prediction Error Identification (PEI) of 
based on N input-output data consists in finding,
among all the models in the pre-specified model
set, the one that provides the minimum value for
the prediction error criterion, that is, finding the
solution of the following optimization

VN ()




arg min VN (),


1
N

N
X

y(t, )  y(t)2 ,

t1

where y(t, ) is a one-step-ahead predictor. The
problem of finding the optimal predictor for a
nonlinear system can be a formidable one. In
this paper we do not concern ourselves with this
issue and use the simulation model predictor
(Ljung, 1999), that is, the predictor obtained by
disregarding the process noise
y(t, )  h((t), 0, ).

ISBN 978-85-8001-069-5

t1

It is well known that the estimate will be consistent - that is EN   0 - if and only if the
noise is uncorrelated to the regressor, that is
E(t)(t)  0 t
with E denoting expectation.
It is also a standard fact from identification
theory that in this case the Cramer-Rao bound
for the precision of the estimate is given by the
Fisher information matrix F IM

where   1 2    p T is the parameter vector
to be identified. Except when otherwise specified
we shall consider the following assumption.

N

Linear Parameterizations

F IM  E(t)T (t) 
2.2

N
1 X
(t)T (t).
N t1

Identifiability and informativity

Several concepts of identifiability have been proposed in the scientific literature, and these definitions have evolved over the years. A brief history of the evolution of this concept is presented
in (Bazanella et al., 2012). The more contemporary view of this problem is the one given in
(Ljung, 1999) and involves two separate concepts
identifiability and informativity.
Definition 1 (Identifiability) A parametric
model structure M is locally identifiable at a value
1 if   > 0 such that, for all  in    1  
h((t), 0, )  h((t), 0, 1 )    1 .
The model structure is globally identifiable at 1
if the same holds for   . It is called globally
identifiable if it is globally identifiable at almost
all 1 .
General results on global identifiability of
model structures using algebraic methods were derived in (Ljung and Glad, 1994). Identifiability is
a property of the parametrization of the model
only and does not guarantee that two different
models in the model set M cannot produce the

3578

Anais do XIX Congresso Brasileiro de Automática, CBA 2012.

same predictions y(t, ) when driven by the same
data. This requires, additionally, that the data set
is sufficiently informative w.r.t. the model structure (Ljung, 1999).

The characteristic set is used to reduce the
polynomial with the highest order (rank). The algorithm use the following Lemmas to reduce polynomials.

Definition 2 (Informative data - classical) A
quasistationary data set (t) is called informative
with respect to a parametric model set M if, for
any two models h((t), 0, 1 ) and h((t), 0, 2 ) in
that set,

Lemma 1 Let f and g be polynomials in the variable x of degree m and n, respectively, written in
the form

where m  n. Then there exist polynomials Q 6
0, Q and R such that

Ey(t, 1 )  y(t, 2 )2  0
 h((t), 0, 1 )  h((t), 0, 2 ).

(3)
Qf  Qg + R,

When the model structure is identifiable and
the data are informative, then the solution of the
PEI is unique (Bazanella et al., 2012).
3

Ritts algorithm

Ritts algorithm consists in a systematic elimination of variables in polynomial equations, keeping
intact the solutions of these equations. Starting
from the original system equations (2), at each
step of the algorithm a new polynomial equation
is obtained, whose solutions are exactly the same
of the previous one and whose degree is reduced.
At the end of the algorithm, the degree of the polynomial equations will have been reduced to one (a
linear equation). Then, in this linear equation,
the properties of identifiability and (supposedly)
those of informativity can be easily checked.
A fundamental concept of Ritts algorithmic
is variables ranking. This is a total ordering of all
variables and their derivatives. Any rank is possible provided that it satisfies the following conditions
u()  u(+)
(4)
u()  y ()  u(+)  y (+)

(5)

for all nonnegative integers  and  and all positive integers . If u  y we say that u is ranked
lower than y.
The leader lA of a polynomial A is the highest
ranked time shifted variable in A and the separant
SA is the partial derivative of A with respect to
the leader. The initial IA is the coefficient of the
highest power of the leader in A.
The degree of a variable u in a polynomial
A is denoted by degu A. Let A and B be two
polynomials. The polynomial B is said to be reduced with respect to A if there is no positive
time displacement of lA (leader of A) in B and
if deglA B < deglA A.
A set A  A1 , . . . , Ap  of polynomials is
called auto-reduced if all the Ai are pairwise reduced with respect to each other. If, in addition,
the polynomials A1 , . . . , Ap in the auto-reduced set
A are in increasing rank, then A is said to be ordered.

ISBN 978-85-8001-069-5

f  am xm + . . . + a0 and g  bn xn + . . . + b0 ,

where degx R < n. Furthermore, with Q given by
bmn+1
, then Q and R are unique.
n
Lemma 2 Let g be a polynomial with leader lg
()
and let f be a polynomial containing lg for some
  N. Then there exist polynomials R and Q
such that
()
 R does not contain lg .
 Every solution of f  0, g  0 is also a solution of R  0, g  0.
 Every solution of R  0, g  0 with Q 6 0 is
also a solution of f  0, g  0.
Lemma 3 Let f be a polynomial which is not reduced with respect to the polynomial g. Then there
exist polynomials R and Q such that
 R is reduced with respect to g.
 Every solution of f  0, g  0 is also a solution of R  0, g  0.
 Every solution of R  0, g  0 with Q 6 0 is
also a solution of f  0, g  0.
For proofs of the three lemmas, as well for the lemmas themselves see (Lyzell et al., 2011).The lemmas above provide a way to reduce the systems of
difference equations (2), preserving the solutions
of the initial system.
The algorithm has two sets of polynomials,
F  f1 , f2 , . . . , fn  and G  , as input. And
the output will be a new set F and G as a characteristic set with reduced polynomials of the initial
set F and the G containing information about the
separants, initial and the quotients resulting from
the reduce process of the algorithm, respectively.
Below we present the Ritts Algorithm.
1. Compute a characteristic set A  A1 , . . . , Ap 
of F.
2. If F  A is non-empty, then go to step 4.
3. Add SA , IA for all A  A to G
Output(F, G) and ST OP .
4. Let fk be the highest unreduced with respect
to A element of F apply Lemma 2 to get polynomials Q and R such that
Qfk 

n
X

QA(i ) + R,

i1

3579

Anais do XIX Congresso Brasileiro de Automática, CBA 2012.

where A is the highest ordered polynomial in
A such that fk is not reduced with respect to
A. And update G  G  Q.
5. If R  0 update F  F  fk , else
F  (F  fk   R) and go to step 1.
According to (Lyzell et al., 2009) the algorithm will reach the point marked STOP after a
finite number of steps.
As a consequence of Lemmas 1, 2 and 3, we
have the following result which relates identifiability of the nonlinear system to the outcome of
Ritts algorithm.
Theorem 4 (Lyzell et al., 2011). Let the output
of Algorithm 1 contain an expression of the form
Q  P , where the diagonal matrix Q and the vector P does not depend on  and x, and assume that
it is possible to chose u(t) such that det(Q) 6 0.
Then the model structure is globally identifiable.
For continuous time systems the conditions
in the above Theorem are not only sufficient
for identifiability, but also necessary (Ljung and
Glad, 1994). For discrete-time systems they are
conjectured to be also necessary, but no proof has
been obtained yet (Lyzell et al., 2011).
4
4.1

Identification with LS or IV

Application of Ritts algorithm with this ordering
results in the following equivalent linear regression
model
y u  y u  (y u  y u)  + (ue  ue),
 z   z  1  z 
Z(y,u)

(y,u)

t1

4.2

t1

Consistency and precision

The fundamental property which is desired from
an estimate is its consistency. It is standard textbook knowledge (Aguirre, 2004), (Ljung, 1999)
that the LS estimated is consistent if and only
if the equation error is orthogonal to the regressor
vector, that is, if
E(y(t), u(t))(e(t))  0.
It is easy to see that for the system S1 in Example 4.1

Description

Zi (y(t), u(t))  i (y(t), u(t))i + i (e(t)),

(6)

then the parameter i can be estimated (Lyzell
et al., 2011) by the direct application of the standard LS or the Instrumental Variable (IV) solution
to (6). Each parameter i is identified separately.
To do this, take the simulation model predictor
Zi (t)  i (y(t), u(t))i ,

(7)

and minimize the prediction error
JNi (i ) 

Zi (y(t), u(t))  Zi (t)2 .

(8)

t1

Example 4.1 Consider the following model set,
which is linear but with a nonlinear parameterization.
M1 

y(t)  1 y(t  1) + 12 u(t  1) + e(t), (9)

In order to apply Ritts algorithm the first step
is to define an ordering according to (4)-(5) we
will take
()
e()  u()  y ()  1 .
(10)

ISBN 978-85-8001-069-5



E(y u  y u)  (ue  ue)


6


E(y u)  (ue)
0

because y(t) does not depends on past values of
e(t), but it does depend on e(t).
Consider now as another example the following nonlinear FIR model.
Example 4.2
M2 

y(t)  1 u2 (t) + 12 u(t  1) + e(t), (12)

Applying Ritts algorithm to (12), with the
same ordering (10), yields
y u  y u  (u2 u  u3 )  + (ue  ue)
 z   z  1  z 
Z(y,u)

N
X

(e)

where we have omitted the time dependence in order to improve readability. Recall that signals u(t),
y(t) and parameters 1 satisfy (11) if and only if
they also satisfy the original equation (9). The LS
estimate can be calculated as
"N
1 N
X
X
N 
(y(t), u(t))2 
(y(t), u(t))Z(y(t), u(t)).

E(y(t), u(t))(e(t))

If the reduced expressions obtained at the end of
Ritts algorithm can be written in the form

(11)

(y,u)

(13)

(e)

and thus

E(y(t), u(t))(e(t))




E(u2 u  u3 )  (ue  ue)
0

because by assumption the input u(t) is uncorrelated to the noise. Hence in this example the LS
estimate is unbiased.
The consistency properties of the LS estimate
in these two examples will be illustrated in the
numerical studies in Section 5. When the LS estimate is biased, an IV estimate must be used instead (Ljung, 1999).

3580

Anais do XIX Congresso Brasileiro de Automática, CBA 2012.

4.3

Persistency of excitation

Calculation of the LS estimate requires the inversion of a matrix formed with the regressor vector.
The following concepts are well known from the
theory of linear regressions for linear systems.
Definition 3 The regressor vector is said to be
persistently exciting if E(t)(t)T  is nonsingular.
Definition 4 The signal x(t) is said to be sufficiently rich of order p if its spectrum is nonzero
for at least p distinct frequencies in the interval
, ).
Theorem 5 Let the real system be linear and
the regressor vector be given by (t)  y(t 
1), . . . , y(t  n), u(t), u(t  1), . . . , u(t  m). The
regressor vector is PEn if and only if u(t) is SRn.
The relationship between persistency of excitation
and signal richness is called transfer of excitation and Theorem 5 expresses the well known
solution of this problem in the case of linear regressions. In the identification of linear systems
with nonstandard model structures the problem of
transfer of excitation is much more complex, but
a thorough characterization is also known (Gevers
et al., 2009). For nonlinear_systems the problem
of transfer of excitation seems to be open.
Take again example 4.2, the regressor vector
being a scalar it will be persistently exciting as
long as it is not identically zero. However, there
are nonzero inputs that do result in an identically
zero regressor. Indeed, it is easy to check that a
constant input u(t)  c t, although sufficiently
rich of order one, results in   0 and thus an
experiment with constant input is not informative
with respect to this model structure.
5

Case studies

In this Section, numerical studies for three case
studies are presented. The first two are systems
with the model structures presented in Examples
4.1 and 4.2, where the real parameter values are
respectively 1  0.5 and 1  1, that is,
S1 

y(t)  0.5y(t1)+0.25u(t1)+e(t), (14)

S2 

y(t)  u2 (t) + u(t  1) + e(t),

(15)

In both cases the real system belongs to the model
set S1  M1 and S2  M2 .
The third case study is the following linear
system

which will be identified with a standard output
error structure
M3 

+u(t) + e(t) + 1 e(t  1).

y(t)  0.5y(t  1) + 0.9u(t  1)
+u(t) + e(t) + 0.5e(t  1),

ISBN 978-85-8001-069-5

(16)

(17)

For this third example, once again the real
system belongs to the model set S3  M3 .
In each case identification has been performed
with two different inputs, namely a random signal
uniformly distributed between 0  1
u1 (t)  rand0  1

(18)

and a double sinusoid
u2 (t)  sin(t3) + sin(t).

(19)

In all simulations the noise e(t) is zero mean
Gaussian white noise with standard derivation
e  0.1 - called e1 (t) - or e  0.01 - called
e2 (t). In each case, the statistical properties of
the estimate are illustrated by running 500 Monte
Carlo runs with N  100 data points each.
5.1

Results for the system S1

The result of Ritts algorithm for the model M1
has been given in (11). We start by illustrating
the difficulty in performing parameter estimation
from the original model. Prediction error identification could be performed by forming the simulation model predictor (3) and then minimizing
the following prediction error cost function


VN (, Z N ) 

N
1 X
(y(t)  y(t))2 .
N t1

(20)

Instead, we propose to minimize the prediction error cost function (8) resulting from the linear regression (11). The two cost functions resulting from an experiment with a random input
(input u1 (t) defined above) can be seen in Figure
1a. It is observed that the cost function VN ()
presents a local minimum in addition to the global
minimum at   0.5, which would be troublesome
in the optimization, whereas JN () is convex, as
previously known (indeed, JN () is a quadratic
function by construction). It has been shown previously that the estimate using LS would be biased
in this case due to the correlation between the regressor and the noise. This is observed in the first
two columns of Table 1, since the sample mean is
quite different from the real value 1 . Thus an VI
estimate is more appropriate, and the result of its
application is also shown in Table 1.
5.2

S3 

y(t)  1 y(t  1) + 2 u(t  1)

Results for the system S2

Again, we start by presenting the two cost functions, JN () and VN () in Figure 1b. It is seen

3581

Anais do XIX Congresso Brasileiro de Automática, CBA 2012.

Table 1 Sample Mean and Sample Standard Deviation in 500 Monte Carlo runs.
LS
b
0.4023
0.4667
0.4987
0.4998

u1
u2
u1
u2

e1
e2

IV

b
0.0713
0.0349
0.0078
0.0035

b
0.5001
0.4967
0.4999
0.4999

b
0.0926
0.0385
0.0078
0.0033

that the original cost function presents a nasty local minimum and that Ritts algorithm has convexified the problem.
For this second example, it has been shown
previously that the LS estimate would be unbiased. The Monte Carlo runs are consistent with
this finding, as can be seen in Table 2.
Table 2 Sample Mean and Sample Standard Deviation in 500 Monte Carlo runs.

e1
e2

5.3

LS
b
b
0.9999 0.0032
0.9992 0.0125
0.9999 0.0032
0.9999 0.0011

u1
u2
u1
u2

IV
b
b
0.9996 0.0354
1.0007 0.0107
1.0001 0.0036
1.0000 0.0011

Results for the system S3

Applying Ritts algorithm to (16), with the same
ordering, results in the following equivalent linear
regression model for the parameters, 1
y u + u2  y u  uu  (y u  yu  ue + ue) 1
z
 

z


0.35

Z1 (y,u)

Original Form
Reduced Form
0.3

1 (y,u)

ue + ue
 z 

VN(,ZN)

(21)

1 (y,u)

0.25

0.2

and 2

0.15

0.1

0.05

y 2  y u  yy + yu   (y u  y u  ue + ue) 2
z


z


Z2 (y,u)

0

1

0.5



0

0.5

1

2
Original Form
Reduced Form

1.6
1.4

VN(,ZN)

1.2
1
0.8
0.6
0.4
0.2
0
2

1.5

1

0.5



0

0.5

+2y e  e  ue  ye  ye + ee + ue

z


(22)

2 (y,u)

(a) For example 1.
1.8

2 (y,u)

1

1.5

Here we notice that the regressors 1 (y, u) 
2 (y, u) and are correlated with 1 (y, u) and
2 (y, u), respectively. We can observe this correlation in the Figures 2a and 2b, where we present
the dispersion of the estimators values, for the input u1 and u2 respectively for the LS identification. As observed in Figures 2a and 2b, the sample means are significantly different from the real
value 0 . So, an IV estimate is more appropriate.
Figures 3a and 3b present the dispersion of the
estimates for both inputs using the IV method.

(b) For example 2.

6
Figure 1 The cost function (20) VN () (solid line)
and JN () (dashed line)

ISBN 978-85-8001-069-5

Conclusions

Ritts algorithm is a known theoretical tool for
determining the identifiability of parametric models with nonstandard parameterizations. But it is
also an alternative for the analysis of informativity of experiments and for the actual identification
of such models, and its use as such is widely unexplored.

3582

Anais do XIX Congresso Brasileiro de Automática, CBA 2012.
Input Rand, noise 0.1

Input Rand, noise 0.1

1.1

1.3
LS
Mean   0.477080.047658
1

IV
Mean   0.499980.063546

  0.892510.051321
2

1

Expected   0.5   0.9

1.05

1

  0.917090.080897
2

Expected   0.5   0.9

1.2

2

1

2

1
1.1
0.95

2

2

1


0.9

0.9
0.85
0.8
0.8

0.7

0.75

0.7

0.35

0.4

0.45

0.5
1

0.55

0.6

0.65

0.7

(a) LS Method. Input (18) and e  0.1

0.25

0.3

0.35

0.4

0.45

0.5
1

0.55

0.6

0.65

0.7

0.75

(a) IV Method. Input (18)and e  0.1

Input sin(t)+sin(t3), noise 0.1

Input sin(t)+sin(t3), noise 0.1
1.4

1.3
LS
Mean 1 0.472150.070844

2 0.900850.091834

1

IV
Mean 1 0.506250.097028

1.3

Expected   0.5   0.9

1.2

2 0.940210.12792

Expected   0.5   0.9

2

1

2

1.2
1.1
1.1
1
2

2

1

0.9

0.9

0.8
0.8
0.7
0.7
0.6

0.25

0.3

0.35

0.4

0.45



0.5

0.55

0.6

0.65

0.7

1

0.5
0.1

0.2

0.3

0.4



0.5

0.6

0.7

0.8

1

(b) LS Method. Input (19) and e  0.1

(b) IV Method. Input (19) and e  0.1

Figure 2 Parameters dispersion

Figure 3 Parameters dispersion.

We have shown by means of theoretical analysis, illustrated by numerical results of three case
studies, what are the statistical properties of this
approach to the identification. Our analysis and
our examples have also served to identify some future work required to fill some gaps still present
in the current theory. The two main issues in
this regard seem to be the need for a more precise
and formal separation between informativity and
identifiability, and a formal characterization of the
problem of transfer of excitation. We have given
a precise definition of identifiability for nonlinear
parameterizations. The concepts of persistency of
excitation and signal richness, which in some contexts are equivalent and even taken as synonyms
by some authors, are by no means equivalent here,
and their relationship is far from understood.
Application to real systems is on its way, as
well as the development of a symbolic toolbox to
cope with the high complexity of the analytical
expressions resulting from the application of Ritts
algorithm.

References

Acknowledgments
This work was supported by CAPES - Coordenacao de Aperfeicoamento de Pessoal de Nvel Superior and CNPq - Conselho Nacional de Desenvolvimento Cientfico e Tecnologico.

ISBN 978-85-8001-069-5

Aguirre, L. A. (2004). Introducao a identificacao
de sistemas, UFMG, Belo Horizonte, Brasil.
Bazanella, A., Gevers, M. and Miskovic, L.
(2012). Necessary and sufficient conditions
for uniqueness of the minimum in prediction
error identification, to appear in Automatica
.
Eckhard, D. and Bazanella, A. S. (2011). On the
global convergence of identification of output
error models, Proceedings of the 18th IFAC
World congress, Milan, Italy.
Gevers, M., Bazanella, A., Bombois, X. and
Miskovic, L. (2009). Identification and the information matrix how to get just sufficiently
rich?, IEEE Transactions on Automatic Control 54(12) 28282840.
Ljung, L. (1999). System Identification Theory
for the User, 2nd Edition, Prentice-Hall, Englewood Cliffs, NJ.
Ljung, L. (2010). Perspectives on system identification, Annual Reviews in Control 34(1) 1
12.

3583

Anais do XIX Congresso Brasileiro de Automática, CBA 2012.

Ljung, L. and Glad, T. (1994). On global identifiability for arbitrary model parametrizations,
Automatica 30(2) 265  276.
Lyzell, C., Glad, T., Enqvist, M. and Ljung, L.
(2009). Identification aspects of ritts algorithm for discrete-time systems, Proceedings
of the 15th IFAC Symposium on System Identification.
Lyzell, C., Glad, T., Enqvist, M. and Ljung, L.
(2011). Difference algebra and system identification, Automatica 47(9) 1896  1904.

ISBN 978-85-8001-069-5

3584