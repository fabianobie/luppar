XVIII Congresso Brasileiro de Automática  12 a 16 Setembro 2010, Bonito-MS.

TREINAMENTO DE REDES PERCEPTRON DE MULTIPLAS CAMADAS
UTILIZANDO O CONCEITO DE SPARSE DISTRIBUTED MEMORY
Leonardo Jose Silvestre, Cristiano Leite de Castro, Antonio de Padua Braga


Universidade Federal de Sao Joao del Rei
Sao Joao del Rei, MG




Universidade Federal de Lavras
Lavras, MG

Universidade Federal de Minas Gerais
Belo Horizonte, MG

Emails lsilvestre@ufsj.edu.br, ccastro@dcc.ufla.br, apbraga@ufmg.br
Abstract This paper presents an alternative approach to Multi-Layer Perceptron (MLP) neural network
learning through the orthogonalization principle of Karnervas Sparse Distributed Memory. The learning process
occurs in two steps firstly, the weights of hidden layer are updated in order to map orthogonal projections of
the input patterns into a high dimension feature space secondly, the weights of output layer are adjusted by
solving a simple optimization problem. In experiments conducted on binary classification problems, our method
was compared with traditional learning algorithms for MLPs BackPropagation and Levenberg-Marquardt. The
results achieved on synthetic and real (from UCI repository) data sets point out that our approach is promising.
Keywords

Multi-Layer Perceptrons, Training, Sparse Distributed Memory, Orthogonalization

Resumo Esse artigo apresenta uma abordagem alternativa para o aprendizado de Redes Perceptron de
Multiplas-Camadas (Multi-Layer Perceptron - MLP) atraves do princpio de ortogonolizacao da Sparse Distributed Memory de Kanerva. O aprendizado ocorre em duas etapas inicialmente, os pesos da camada escondida
sao atualizados para que os padroes de entrada tornem-se aproximadamente ortogonais em um espaco de caracterticas de alta dimensao em seguida, os pesos da camada de sada sao atualizados a partir da solucao de um
problema de otimizacao mais simples. Nos experimentos conduzidos com problemas binarios de classificacao,
nosso metodo foi comparado com algoritmos comumente usados para o aprendizado de redes MLP BackPropagation e Levenberg-Marquardt. Os resultados obtidos em termos da taxa de erro de validacao para bases de
dados sinteticas e reais (repositorio UCI) indicam que nossa abordagem e promissora.
Palavras-chave



Introducao

que seus vetores de sada tornem-se ortogonais.
Apos essa etapa, um problema mais simples e entao considerado na camada de sada. A maioria
dos trabalhos conduzidos nessa linha utilizam o
metodo de Gram-Schmidt (Hoffmann, 1989) para
ortogonalizacao e tem como objetivo principal determinar a estrutura otima (numero de neuronios
escondidos) das RNAs. Em (Chen et al., 1991), os
autores propuseram o algoritmo Orthogonal Least
Squares para redes RBF (Funcao de Base Radial).
(Zhang and Morris, 1998) adotaram uma abordagem de treinamento sequencial para redes MLP,
na qual a cada passo um novo neuronio da camada escondida e adicionado. Em (Romero and
Alquezar, 2007), os autores avaliaram diferentes
algoritmos para treinamento sequencial de redes
MLP e concluram que os baseados no princpio
de ortogonalizacao da camada escondida superam
os metodos tradicionais.
Nao obstante, a descricao do modelo de memoria esparsamente distribuda (SDM - Sparse
Distributed Memory) de Kanerva (Kanerva, 1988
Kanerva, 1993) oferece uma alternativa promissora para a ortogonalizacao das projecoes na camada escondida. De acordo com a SDM, quando
o numero de neuronios da camada escondida e
muito elevado (espaco de caractersticas de alta

Desde a descricao do algoritmo Back-Propagation
(Rumelhart et al., 1986), grande parte dos novos
algoritmos de aprendizado para redes Perceptron
de Multiplas Camadas (MLPs) ajustam os pesos
de todas as camadas da rede, a partir dos sinais
de erro obtidos nas unidades de sada. O surgimento das maquinas de kernel (Cortes and Vapnik, 1995 Muller et al., 2001), no entanto, proporcionou uma nova perspectiva para o problema
de aprendizado ao descreve-lo em duas etapas
mapeamento nao-linear dos padroes de entrada,
atraves de um kernel previamente ajustado, seguido da estimacao de uma separacao linear no
chamado espaco de caractersticas de alta dimensao (Cristianini and Shawe-Taylor, 2000). Uma
vez definidos os parametros do kernel e o mapeamento correspondente, um problema de otimizacao e entao resolvido.
No contexto de Redes Neurais Artificiais
(RNAs), abordagens que tratam o problema de
aprendizado atraves de etapas independentes sao
geralmente baseadas no princpio da ortogonalizacao da camada escondida (intermediaria). Segundo esse princpio, os pesos da camada escondida devem ser ajustados independentemente para

3886

XVIII Congresso Brasileiro de Automática  12 a 16 Setembro 2010, Bonito-MS.

na qual o vetor wS  w0 , w1 , . . . , wh  representa
os pesos da camada de sada da rede. Por questao
de simplicidade, o termo bias foi considerado como
uma unidade (entradaescondida) extra com valor
igual a 1.
N
Seja T  x(i), d(i)i1 , o conjunto de N padroes de treinamento, com x(i)  Rn e d(i) 
0, 1. A grande maioria dos algoritmos propostos para o aprendizado de redes MLP (Rumelhart
et al., 1986 Hagan and Menhaj, 1994 Teixeira
et al., 2000 Costa et al., 2007) realizam o ajuste
de todos os pesos da rede w  wE , wS  de uma
so vez. Em geral, esse ajuste e baseado na minimizacao de um funcional que leva em consideracao somente os sinais de erro obtidos na unidade
de sada da rede, como e o caso do funcional somatorio dos erros quadraticos medios, dado pela
Equacao 3, a seguir (Haykin, 1994),

dimensao) e seus mapeamentos sao selecionados
aleatoriamente, os vetores de sada tendem a se
tornar ortogonais. Essa caracterstica possibilita
que um problema de classificacao_de_padroes nao
linearmente separavel no espaco de entrada tornese linearmente separavel no espaco de caractersticas. A abordagem adotada nesse trabalho explora
esse conceito, que e muito similar a abordagem
usada por maquinas de kernel. Em particular, redes MLP foram treinadas em duas etapas, atraves do princpio de ortogonalizacao das memorias
SDM, e comparadas com algoritmos de treinamento tradicionais, tais como, Back-Propagation
(Rumelhart et al., 1986) e Levenberg-Marquardt
(Hagan and Menhaj, 1994). Experimentos foram
conduzidos com problemas binarios de classificacao e os resultados obtidos em termos da taxa de
erro de validacao para bases de dados sinteticas e
reais (repositorio UCI) indicam que nossa abordagem e promissora.
O restante do artigo e apresentado da seguinte
forma as Secoes 2 e 3, a seguir, descrevem os
conceitos teoricos que fundamentam nossa abordagem redes Perceptron de Multiplas Camadas e
Sparse Distributed Memory (SDM). Em seguida,
na Secao 4, nossa proposta para o treinamento
de redes MLP a partir do conceito de ortogonalizacao da SDM e apresentada. Na Secao 5, sao
descritos a metodologia adotada na conducao dos
experimentos e os resultados obtidos. Finalmente,
a Secao 6 traz as discussoes e as conclusoes.
2

J(w) 

3

Desde que o escopo do trabalho e limitado a problemas binarios de classificacao, considere uma
rede Perceptron de Multiplas Camadas (MLP)
com n entradas, uma camada escondida com h
unidades (neuronios) e uma camada de sada contendo uma unica unidade. O valor de sada obtido na unidade escondida k da rede MLP, devido
a apresentacao de um padrao arbitrario x, e dado
por (Haykin, 1994),


n
X
0
zk  f wk  x  f 
wkj xj  .


(1)

j0

onde o vetor wk  wk0 , wk1 , . . . , wkn  representa
a colecao de pesos conectada ao k-esimo neuronio da camada escondida. A colecao de todos
os pesos da camada escondida e denotada por
wE  w1 , w2 , . . . , wh . Similarmente, o valor
obtido na unidade de sada da rede e calculado
com base nos valores de sada emitidos pelas unidades escondidas (Haykin, 1994),




y  f wS  z  f
0

h
X

!
wk zk

.

(3)

Sparse Distributed Memory

A memoria esparsamente distribuda (SDM Sparse Distributed Memory) surgiu como um modelo_matematico da memoria humana de longo
prazo (Kanerva, 1988 Kanerva, 1993). A ideia
na qual ela foi baseada foi a de que distancias entre conceitos nas nossas mentes correspondem a
distancias entre pontos em um espaco de alta dimensao. Como qualquer ponto de interesse em um
espaco de alta dimensao esta relativamente distante de outros pontos de interesse nesse mesmo
espaco, a representacao de um ponto de interesse
especfico nao precisa de ser exata.
A SDM pode ser comparada a uma memoria
de acesso randomico (RAM - Random Access Memory) de grande capacidade, composta por uma
matriz de enderecos, uma matriz de conteudos
e registradores de endereco muito grandes (da
ordem de 1000 bits). Desde que os enderecos
sao grandes, e impossvel mapear uma posicao de
hardware para cada endereco. Assim, na SDM,
raramente um endereco do registrador vai apontar para uma posicao de hardware, pois o numero
de enderecos possveis e muito maior do que as
posicoes de hardware disponveis (por isso o nome
esparsa). Um endereco, entao, ativa um conjunto de posicoes proximas, baseadas na distancia
de Hamming.
Alem de ser representada como uma memoria
associativa (Haykin, 1994) - em (Bose et al., 2006),
por exemplo, os autores usam SDM como parte de
uma memoria associativa para reconhecimento e
predicao online de sequencias temporais-, a SDM
pode ser representada como uma rede_neural feedforward, sncrona e totalmente conectada, com
uma camada escondida. Uma outra propriedade

Perceptron de Multiplas Camadas



N
1 X
(d(i)  y(i))2 .
2N i1

(2)

k0

3887

XVIII Congresso Brasileiro de Automática  12 a 16 Setembro 2010, Bonito-MS.

interessante da SDM e que, se o numero de neuronios da camada escondida cresce, ou seja, os padroes de entrada sao mapeados para um espaco de
alta dimensao, e seus mapeamentos sao selecionados aleatoriamente, os padroes tendem a se tornar
ortogonais.
4

permite a definicao um hiperplano de separacao
nesse espaco de caractersticas.
5

Nessa Secao, experimentos foram conduzidos com
bases de dados sinteticas e reais extradas do repositorio UCI (Asuncion and Newman, 2007). Os
resultados obtidos com nosso algoritmo (SDM) foram comparados aos algoritmos Back-Propagation
com momentum e Levenberg-Marquardt. A seguinte metodologia foi adotada para cada base de
dados

Algoritmo de Treinamento Proposto

Como mencionado anteriormente, nossa abordagem considera uma rede MLP com somente uma
camada escondida, conforme pode ser visto pela
Figura 1. Com base no princpio da SDM, a camada escondida e configurada com um numero h
elevado de neuronios, para que os padroes de entrada sejam projetados em um espaco de alta dimensao. Uma vez selecionado o valor de h, e gerado um vetor (de dimensao h) de sadas desejadas
dE (i) na camada escondida para cada padrao arbitrario x(i) do conjunto de treinamento. Cada
componente do vetor dE (i) e obtido de forma aleatoria, podendo assumir os valores 0 ou 1 com
probabilidade 0.5.
Desde que o treinamento dos neuronios das
camadas escondida e de sada ocorre de forma independente, funcoes de ativacao contnuas e diferenciaveis (do tipo sigmoide), comumente adotadas por metodos de treinamento tradicionais baseados no calculo do gradiente, nao sao mais necessarias. Assim, funcoes de ativacao heaviside (tipo
degrau) foram usadas em todos os neuronios da
rede. O processo de aprendizado ocorre em duas
etapas

 Numero de Neuronios da Camada Escondida
(h) para o algoritmo SDM foram testados os
seguintes valores de h 100, 500, 1000 e 3000.
Para os demais algoritmos, foram usados 3,
5, 11 e 15.
 Numero de Execucoes para cada valor de h,
foram conduzidas 10 execucoes com diferentes subconjuntos de treinamento e validacao
obtidos a partir do procedimento 10-fold crossvalidation (Stone, 1974).
 Metrica de Desempenho foram calculados a
taxa media de erro e o desvio padrao sobre
as 10 execucoes, em relacao aos subconjuntos
de treinamento e validacao.
As Tabelas, nas Secoes a seguir, mostram somente os resultados obtidos com a melhor configuracao de h (numero de neuronios escondidos)
para cada algoritmo. Nos casos raros em que diferentes configuracoes obtiveram valores iguais em
relacao ao valor medio do erro de validacao, o desempate foi realizado pelo menor erro medio de
treinamento.

1. A colecao de pesos wE  w1 , w2 , . . . , wh 
da camada escondida e ajustada com base
nos vetores de sada desejada dE (i) gerados
para cada padrao de treinamento x(i). Considerando o k-esimo neuronio da camada escondida, o ajuste de um peso arbitrario wkj e
baseado na regra de aprendizado do Perceptron (Haykin, 1994),
wkj  wkj +  (dE
k  zk ) xj .

5.1

Dados Sinteticos

Nesse experimento, foi utilizado um conjunto de
dados sintetico bi-dimensional conhecido como
problema dos dois espirais (vide Figura 2). Esse
conjunto apresenta duas classes, sendo a primeira
representada por crculos, com sada desejada
d(i)  1, e a segunda, representada por asteriscos, possui sada desejada d(i)  0. O problema
e nao linearmente separavel no espaco de entrada
e permite comparar a eficiencia do mapeamento
definido na camada escondida da rede MLP pelos
diferentes algoritmos de treinamento.
A Tabela 1 apresenta os resultados obtidos
(medias e desvios-padrao em ) com os algoritmos Sparse Distributed Memory (SDM), BackPropagation (BackProp) e Levenberg-Marquardt
(Lev-Marq). Sao tambem apresentados os melhores valores de h, numero de neuronios na camada
escondida. Como pode ser observado, nosso algoritmo (SDM) obteve o melhor desempenho. Os

(4)

2. Apos o ajuste de todos os pesos da camada escondida, o vetor de pesos da camada
de sada wS  w0 , w1 , . . . , wh  e ajustado
com base nas sadas desejadas d(i) fornecidas a partir do conjunto de treinamento T 
N
x(i), d(i)i1 . Assim, de forma similar, o
ajuste de um peso arbitrario wk e tambem baseado na regra do Perceptron (Haykin, 1994),
wk  wk +  (dk  yk ) zk .

Experimentos e Resultados

(5)

A etapa inicial de ajuste dos pesos da camada escondida possibilita que as projecoes dos
padroes de entrada tornem-se aproximadamente
ortogonais em um espaco de alta dimensao. Em
seguida, o ajuste dos pesos da camada de sada

3888

XVIII Congresso Brasileiro de Automática  12 a 16 Setembro 2010, Bonito-MS.

Figura 1 Rede MLP com n entradas, h neuronios na camada escondida e uma unidade de sada. O treinamento
foi realizado em duas etapas inicialmente, a colecao de pesos da camada escondida wE e ajustada a partir dos
vetores (mapeamentos) aleatorios dE (i) e, em seguida, o vetor de pesos da camada de sada wS e ajustado com
base nas sadas desejadas (rotulos) d(i) fornecidas com o conjunto de treinamento.

Tabela 1 Valores medios de erro e desvios-padrao

Tabela 2 Melhores valores de h (numero de neuronios

apresentados pelos algoritmos SDM, BackProp e LevMarq. Sao tambem apresentados os melhores valores
de h, numero de neuronios na camada escondida.

na camada escondida) obtidos pelos algoritmos SDM,
BP e LM para as bases de bados do Repositorio UCI.

Base de Dados
Breast Cancer
Diabetes
Glass
Heart
Ionosphere
Segmentation

Erro Treinamento
SDM
BP
LM
0.00 27.354.77
28.483.43
Erro Validacao ()
SDM BackProp Lev.-Marq.
28.7014.7 36.7312.3
43.9013.5
Num. Neuronios Esc.
SDM BackProp Lev.-Marq.
1000
15
11

BP
3
15
3
15
11
15

LM
7
3
15
7
7
15

tation(1) (Asuncion and Newman, 2007). Para
bases contendo mais de duas classes Glass e Segmentation, o rotulo entre parenteses representa a
primeira classe com sada desejada d(i)  1, enquanto as demais classes foram unidas para representar a segunda classe, com sada desejada
d(i)  0.
A Tabela 2 apresenta, para cada base de dados, a melhor configuracao em relacao ao numero
de neuronios na camada escondida (h) obtida pelos algoritmos Sparse Distributed Memory (SDM),
Back-Propagation (BP) e Levenberg-Marquardt
(LM).
A Tabela 3 compara, respectivamente, as taxas de erro de treinamento e validacao obtidos pelos algoritmos quando aplicados as bases de dados
do repositorio UCI. A media e o desvio padrao,

resultados obtidos em termos de erro de treinamento sugerem que a SDM foi capaz de obter uma
representacao linearmente separavel para os padroes de entrada no espaco de caractersticas de
alta dimensao. Em termos do erro de validacao,
SDM foi melhor 8.3 pontos percentuais em relacao
ao algoritmo Back-Propagation e 15.2 em relacao
ao Levenberg-Marquardt.
5.2

SDM
750
3000
750
1000
750
750

Bases do Repositorio UCI

Nesse experimento, foram usadas 7 bases de dados extradas do Repositorio UCI Breast Cancer,
Diabetes, Glass(7), Heart, Ionosphere e Segmen-

3889

XVIII Congresso Brasileiro de Automática  12 a 16 Setembro 2010, Bonito-MS.

de ortogonalizacao da SDM tem a vantagem de
nao depender do ajuste previo de parametros e,
conforme observado pelos resultados obtidos em
relacao ao erro de treinamento, ele permite que
problemas nao-linearmente separaveis possam ter
uma representacao mais simples em um espaco de
caractersticas de alta dimensao.

Resultados obtidos em relacao a taxa de erro
de validacao mostraram que nossa abordagem e
promissora, principalmente para aplicacoes que
possuem uma representacao mais complexa e naolinearmente separavel no espaco de entrada como
e o caso do Problema dos dois espirais.
Figura 2 Problema dos dois espirais A classe 1 representada por crculos possui rotulos d(i)  1 e classe
2 representadas por asteriscos possui rotulos d(i)  0.

apresentados em , foram calculados a partir de
10 execucoes com diferentes subconjuntos de treinamento e validacao (10-fold crossvalidation). Os
melhores resultados encontram-se marcados em
negrito.
Analisando os resultados em termos do erro
de validacao e desconsiderando a base de dados
Breast Cancer em que todos algoritmos obtiveram desempenhos muito proximos, pode-se observar pela Tabela 3, que o algoritmo SDM obteve melhor desempenho que o algoritmo BackPropagation com momentum. Alem disso, SDM
mostrou-se competitivo em relacao ao algoritmo
Levenberg-Marquardt, uma vez que ambos apresentaram taxas de erro similares em relacao a
quase todas as bases de dados. Os resultados sugerem uma leve superioridade do algoritmo SDM
em 2 das 7 bases de dados Glass, Ionosphere.
Nos casos em que SDM obteve piores desempenhos Diabetes e Heart, a diferenca parece nao ser
significativa. Nota-se tambem que SDM obteve
erro de treinamento nulo para todas as bases de
dados, o que novamente sugere que nosso metodo
foi capaz de obter uma representacao linearmente
separavel para os padroes de entrada no espaco de
caractersticas.
6

E importante frisar que, apesar do tamanho
da topologia (numero elevado de neuronios na camada escondida), os resultados obtidos em termos do erro de validacao sugerem que nao houve
ocorrencia de overfitting. Uma possvel explicacao
para esse fato e que a esparsividade dos vetores
de sada da camada escondida, obtidos a partir do
princpio de ortogonalizacao da SDM, fazem com
que os pesos da camada de sada assumam valores pequenos proporcionando superfcies de separacao mais suaves no espaco de caractersticas.
Essa conclusao e ainda especulativa e esta sendo
investigada no momento. Ela, no entanto, se alinha a abordagem utilizada pelas Maquinas de Vetor de Suporte (Cortes and Vapnik, 1995), que nao
sofrem da chamada maldicao da dimensionalidade
(Haykin, 1994), por minimizarem a norma euclidiana do hiperplano em um espaco de caractersticas
de alta dimensao.

Nossos esforcos futuros estao focados em compreender melhor o comportamento das solucoes
obtidas com o algoritmo proposto, atraves de estudos que analisam a margem de separacao obtida pelo hiperplano no espaco de caractersticas.
Alem disso, testes devem ser realizados com outros tipos de funcao de ativacao nos neuronios da
camada escondida da rede, tais como, sigmoides,
gaussianas, entre outras.

Discussoes e Conclusoes

Esse trabalho apresentou uma abordagem alternativa que permite descrever o problema do aprendizado de redes MLP em etapas independentes, de
forma similar a abordagem adotada por Maquinas
de Kernel. Cabe ressaltar, porem, que o princpio

3890

XVIII Congresso Brasileiro de Automática  12 a 16 Setembro 2010, Bonito-MS.

Tabela 3 Taxas de erro de treinamento e validacao (em ) obtidos para as bases do repositorio UCI. Os melhores
resultados estao marcados em negrito.

Base de Dados
Breast Cancer
Diabetes
Glass
Heart
Ionosphere
Segmentation

SDM
0.00
0.00
0.00
0.00
0.00
0.00

Erro Treinamento
BP
LM
16.382.07
3.113.53
21.152.13 13.591.29
6.922.94
0.160.44
14.232.16
0.580.40
11.661.69
0.000.00
8.652.69
0.000.00

Referencias

Erro Validacao ()
SDM
BP
LM
22.807.49 22.676.59
22.847.72
27.355.14
30.865.66 24.225.08
2.322.44
6.955.42
3.292.28
20.216.26
20.957.79 19.388.68
8.233.63
17.643.68
8.845.79
1.433.21
12.386.81
1.432.30

Muller, K. R., Mika, S., Ratsch, G., Tsuda, K.
and Scholkopf, B. (2001). An introduction
to kernel-based learning algorithms, IEEE
Trans. on Neural Networks 12(2) 181201.

Asuncion, A. and Newman, D. (2007). UCI machine learning repository.
URL
httpwww.ics.uci.edumlea
rnMLRepository.html

Romero, E. and Alquezar, R. (2007). Heuristics
for the selection of weights in sequential feedforward neural networks An experimental
study, Neurocomput. 70(16-18) 27352743.

Bose, J., Furber, S. B. and Shapiro, J. L. (2006).
An associative memory for the on-line recognition and prediction of temporal sequences,
CoRR abscs0611020.

Rumelhart, D., Hintont, G. and Williams, R.
(1986). Learning representations by backpropagating errors, Nature 323(6088) 533
536.

Chen, S., Cowan, C. F. N. and Grant, P. M.
(1991). Orthogonal least squares learning algorithm for radial basis function networks,
IEEE Transactions on Neural Networks
2(2) 302309.

Stone, M. (1974). Cross-validatory choice and assessment of statistical predictions, Journal of
the Royal Statistical Society B 36(1) 111
147.

Cortes, C. and Vapnik, V. (1995). Support-vector
networks, Mach. Learn. 20(3) 273297.

Teixeira, R., Braga, A., Takahashi, R. and Saldanha, R. (2000). Improving generalization of
mlps with multi-objective optimization, Neurocomputing 35(1-4) 189194.

Costa, M. A., Braga, A. P. and Menezes, B. R.
(2007). Improving generalization of mlps
with sliding mode control and the levenbergmarquardt algorithm, Neurocomputing 70(79) 13421347.

Zhang, J. and Morris, A. J. (1998). A sequential learning approach for single hidden layer
neural networks, Neural Netw. 11(1) 6580.

Cristianini, N. and Shawe-Taylor, J. (2000). An
Introduction to Support Vector Machines and
Other Kernel-based Learning Methods, Cambridge University Press.
Hagan, M. T. and Menhaj, M. B. (1994). Training feedforward networks with the marquardt algoritm, IEEE Transactions on Neural Networks 5(6) pages 989-993 .
Haykin, S. (1994).
Neural Networks A
Comprehensive Foundation, Macmillan, New
York.
Hoffmann, W. (1989). Iterative algorithms for
gram-schmidt orthogonalization, Computing
41(4) 335348.
Kanerva, P. (1988). Sparse Distributed Memory
(Bradford Books), The MIT Press.
Kanerva, P. (1993). Sparse Distributed Memory
and Related Models, Oxford University Press,
New York, chapter 3, pp. 5076.

3891