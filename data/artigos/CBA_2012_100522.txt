Anais do XIX Congresso Brasileiro de Automática, CBA 2012.

UMA NOVA ABORDAGEM BASEADA NA TEORIA DOS JOGOS PARA SELE
DE MODELOS NEURAIS
Dener E. Bortolini Luiz C.B. Torres Cristiano L. Castro Antonio P. Braga


Programa de Pos-Gradua em Engenharia Eletrica
Universidade Federal de Minas Gerais
Av. Antonio Carlos 6627, 31270-901
Belo Horizonte, MG, Brasil


Universidade Federal de Lavras
Departamento de Ciencia da Computa
Lavras, MG, Brasil
Email dbortolini@cpdee.ufmg.br, luizlitc@gmail.com, ccastro@dcc.ufla.br,
apbraga@ufmg.br
Abstract This paper presents a new approach to select a model in the multiobjective training method of
Artificial Neural Networks. The objective is to find the Pareto-optimal solution that provides the best generalization capability. The proposed approach is based on game theory, where the decision maker searching through
the Nash equilibrium solution to solve the trade-off between complexity and training error of the network. Preliminary results showed that the proposed decision-making, combined with multiobjective training was effective
in controlling the generalization of neural models.
Keywords

Decision-making, Multiobjective machine learning, Game theory, Classification.

Resumo Este trabalho apresenta uma nova abordagem para sele de modelos neurais para o aprendizado
multiobjetivo de redes_neurais artificiais. O objetivo e encontrar no conjunto Pareto-otimo, a solu que fornece
a melhor capacidade de generaliza. A abordagem proposta e baseada na teoria_dos_jogos, onde o decisor busca
atraves do equilbrio de Nash uma solu para resolver o trade-off entre a complexidade e o erro de treinamento
da rede. Resultados preliminares mostraram que o decisor proposto, aliado ao treinamento multiobjetivo, foi
eficiente no controle da generaliza de modelos neurais.
Palavras-chave
.

1

Tomada de decisao, Aprendizado de maquina multiobjetivo, Teoria dos jogos, Classifica-

Introdu

O metodo multiobjetivo (MOBJ) para o aprendizado de Redes Neurais Artificiais (RNAs) fornece uma alternativa para o controle de complexidade e, a consequente obten de modelos
com elevada capacidade de generaliza (Teixeira
et al., 2000 Jin and Sendhoff, 2009). Atraves da
otimiza explcita dos funcionais erro de treinamento (e2 ) e norma do vetor de pesos (w), o
metodo MOBJ gera um conjunto de solucoes candidatas, sendo que uma delas deve apresentar a
complexidade adequada para a tarefa de aprendizado em questao. Tal conjunto, denominado
Pareto-otimo (PO), descreve os melhores compromissos que podem ser obtidos entre os funcionais
conflitantes, e2 e w, de forma que nao se pode
reduzir um desses funcionais sem que haja incremento no valor do outro.
Uma vez obtido o conjunto PO, a escolha da
solu que fornece o equilbrio adequado entre
erro e norma constitui a etapa mais crtica do metodo MOBJ. Solucoes nao equilibradas podem induzir a (sub)sobreajuste do modelo aos dados de
treinamento. Estrategias para sele da solu
final tem sido propostas na literatura, tais como
o decisor por mnimo erro de valida (Teixeira
et al., 2000) e o decisor baseado em conhecimento

ISBN 978-85-8001-069-5

previo (Medeiros et al., 2009). Cabe ressaltar, no
entanto, que a eficiencia desses decisores pode ficar
limitada em situacoes em que o conjunto de dados e muito pequeno e informa a priori sobre o
processo de amostragem dos dados nao se encontra disponvel. Infelizmente, essas caractersticas
sao frequentes em problemas reais de aprendizado.
Para superar essas dificuldades, esse artigo
apresenta uma nova estrategia de decisao guiada pela Teoria dos Jogos (Fudenberg and Tirole, 1991). A motiva para a abordagem aqui
proposta esta no fato de que a Teoria dos Jogos
possibilita a constru de modelos_matematicos
para a tomada de decisoes otimas em condicoes de
conflito. A modelagem do problema consistiu em
descrever cada solu do conjunto PO como um
jogador, com suas respectivas estrategias, representadas pelos valores dos funcionais erro e norma.
O algoritmo N-Player Game (Chatterjee, 2009) foi
usado para o calculo da matriz de payoff e, posterior obten do equilbrio de Nash. O jogador
(solu) que retorna o melhor valor associado ao
equilbrio de Nash deve entao ser escolhido pelo
decisor.
Em experimentos conduzidos com bases de
dados do repositorio UCI (Frank and Asuncion,
2010), o decisor proposto foi comparado com o
bem conhecido metodo de Regulariza Bayesi-

5421

Anais do XIX Congresso Brasileiro de Automática, CBA 2012.

ana (MacKay, 1992), que usa uma abordagem probabilstica para obten do equilbrio adequado
entre erro e norma e tambem, com o decisor de
valida, que tem sido comumente usado com o
metodo MOBJ. Resultados preliminares mostram
que abordagem proposta para sele de modelos
com elevada capacidade de generaliza e promissora.
O restante do artigo e apresentado da seguinte
forma as Secoes 2, 3 e 4, a seguir, descrevem os
conceitos teoricos que fundamentam nossa abordagem O metodo de aprendizado multiobjetivo
(MOBJ), Regulariza Bayesiana e a Teoria dos
Jogos. Em seguida na Se 5, nossa proposta
para o decisor atraves da teoria_dos_jogos e apresentada. Na se 6, sao descritos a metodologia
adotada na condu dos experimentos e os resultados obtidos. Finalmente, a Se 7 traz as
discussoes e as conclusoes.
2

Metodo Multiobjetivo (MOBJ)

O metodo de aprendizado multiobjetivo (MOBJ),
proposto em (Teixeira et al., 2000), consiste em
controlar a complexidade das redes atraves da minimiza simultanea do erro para os padroes de
treinamento e da norma do vetor de pesos. Dado
o conjunto de dados T  xi , yi  i  1 . . . N , a
Equa (1) a seguir, fornece a formula biobjetivo para o aprendizado de redes MLP,
(
min

norma, fica explcito. Para cada solu pertencente ao conjunto Pareto-Otimo, existira um valor
para erro e para norma especfico. Logo existira
no conjunto de solucoes PO aquela com a complexidade efetiva adequada, ou seja, dentre as solucoes de norma (complexidade) maxima e norma
mnima, podera existir uma solu que possui
a norma adequada, com um erro menor possvel
para a mesma.

2
PN 
J1 (w)  i1 yi  f(xi , w)
J2 (w)  kwk

(1)

onde f(xi , w) e a sada estimada pela rede para o
i-esimo padrao de entrada, yi e a sada esperada
(rotulo), w e o vetor que armazena todos os pesos
da rede e, kk e o operador que fornece a norma
euclidiana de um vetor.
Ao final do aprendizado, o algoritmo MOBJ
gera uma estimativa para o conjunto de solucoes nao dominadas denominado conjunto Paretootimo (PO), onde a distancia_euclidiana entre a
norma de cada solu e dada por kwk, como
e mostrado na Figura 1. Tais solucoes representam o trade-off entre o erro de treinamento e a
complexidade da rede, ou seja, essas solucoes sao
aquelas as quais nao ha mais como melhorar um
dos objetivos sem que haja uma perda do outro.
O conjunto PO possui dois conjuntos de solucoes
que podem ser classificadas como sub-ajustadas
e super-ajustadas. Solucoes super-ajustadas sao
aquelas com alta complexidade e baixo erro para o
conjunto de treinamento que podem gerar overfitting. Enquanto as sub-ajustadas apresentam erros
grandes para os padroes de treinamento. Ainda
de acordo com (Teixeira et al., 2000), o metodo
possui as seguintes vantagens O compromisso entre o erro e a complexidade, expressa atraves da

ISBN 978-85-8001-069-5

Figura 1 Pareto-otimo
Como resultado do algoritmo MOBJ temse um numero n de rede treinadas, sendo algumas sub-ajustadas, outras sobre-ajustadas e redes
ajustadas. E necessario um decisor para escolher a
solu w  W  com o melhor compromisso entre
a norma da rede w e o erro de treinamento MSE
(e2 ). E preciso que o decisor escolha uma solu
com ajuste adequado a fun geradora fg (x) que
vai proporcionar uma boa capacidade de generaliza, ou seja, uma maior taxa de classifica
para padroes desconhecidos.
3

MLP com Regulariza Bayesiana

O treinamento por Regulariza Bayesiana tem
como principal caracterstica a abordagem probabilstica, baseando-se na premissa de que as quantidades de interesse do treinamento sao reguladas por distribuicoes de probabilidade (Koerich,
2008). A fun objetivo a ser minimizada e descrita como a soma_ponderada dos objetivos J1 e
J2 , conforme descrito na Equa (2) a seguir,
J  J1 + J2

(2)

onde  e  sao os parametros que ponderam a importancia dos objetivos J1 e J2 na fun custo
J. Assim se    o aprendizado da mais importancia a redu dos erros enquanto que se
  , o aprendizado deve enfatizar a diminui da magnitude dos pesos da rede, produzindo
uma superfcie de decisao mais suave. O principal
problema em implementar o algoritmo de regulariza e ajustar de forma correta os valores desses
parametros.

5422

Anais do XIX Congresso Brasileiro de Automática, CBA 2012.

Segundo (Dan Foresee and Hagan, 1997), a
regra de Bayes pode ser usada para otimizar os
valores de  e . Nesse caso, tem-se que os valores otimos desses parametros, em cada itera
do treinamento, sao dados por



2J2 (w)

(3)



n
2J1 (w)

(4)

onde
1

  N  2tr (H)

(5)

onde  representa o numero efetivo de parametros, isto e, uma medida de quantos parametros
da rede foram usados na redu da fun custo,
podendo assumir valores de 0 a N , com N representando o numero total de parametros da rede.
Como a otimiza Bayesiana requer a computa da matriz Hessiana H da fun J, utiliza-se
a aproxima da matriz hessiana disponvel no
algoritmo de Levenberg-Marquardt (LM) (Hagan
and Menhaj, 1994). Os passos necessarios para
a otimiza dos parametros de regulariza sao
descritos a seguir
1. Inicialize os parametros ,  e os pesos.
2. Calcule uma itera do algoritmo LM para
minimizar a fun custo da Equa (2).
3. Computar o numero efetivo de parametros 
de acordo com a Equa (5) fazendo uso da
aproxima de Gauss-Newton presente no
LM.
4. Computar as novas estimativas para os parametros  e  encontrados a partir das Equacoes (3) e (5).
5. Repita os passos 2, 3 e 4 ate atingir a convergencia.
A partir de cada nova estimativa dos parametros da fun custo J, o vetor de pesos deve-se
mover em dire ao ponto de mnimo. Quando a
fun objetivo nao tiver mudancas significativas
a convergencia tera sido alcancada.

4.1

Defini

A teoria_dos_jogos pode ser definida como a teoria
dos modelos_matematicos que estuda a escolha de
decisoes otimas sob condi de conflito (Sartini
et al., 2004). O elemento mais basico de um jogo e
o jogador, sendo que este sempre tem um conjunto
de estrategias. Quando cada jogador escolhe sua
estrategia, entao temos uma situa no espaco de
todas as situacoes possveis. Cada jogador tem um
ganho (fun de utilidade) para cada estrategia
escolhida. Assim temos o conjunto dos jogadores G  g1 , g2 , ..., gn  e cada gi possui um conjunto finito de estrategias S  si1 , si2 , ..., sin . O
produto cartesiano destas matrizes gera a matriz
de payoff, que possui um valor de utilidade para
cada posi da matriz. Essa matriz tambem determina o espaco de estrategia pura do jogo. Para
as estrategias existe o criterio de dominancia que
pode ser verificada fixando uma estrategia para
um jogador e variando as estrategias restantes.
Diz-se que uma estrategia e dominante quando e
a melhor escolha para um jogador, quando se leva
em conta todas as escolhas possveis dos outros jogadores. Uma estrategia dominada, por sua vez,
e aquela que nunca e melhor que outra disponvel.
Quando uma estrategia e sempre pior que outra,
diz-se que e estritamente dominada. Com esse conhecimento podemos entao definir o equilbrio de
Nash Uma solu estrategica ou equilbrio Nash
de um jogo e um ponto onde cada jogador nao tem
incentivo de mudar sua estrategia se os demais jogadores nao o fizerem (Sartini et al., 2004), sendo
isto definido para jogos nao cooperativos. Existem
jogos que nao possuem equilbrio de Nash para estrategias puras. Uma alternativa para estes casos
e considerar o jogo do ponto de vista probabilstico, isto e, ao inves de escolher um perfil de estrategia pura, o jogador deve escolher uma distribui de probabilidade sobre suas estrategias puras.
Portanto uma estrategia mista si para o jogador
gi  G e uma distribui de probabilidades sobre o conjunto S de estrategias puras do jogador,
sendo pi um elemento do conjunto
mi  (x1 , ..., xmi )  <mi x1  0, ..., xmi  0
(6)
onde
mi
X

xk  1

(7)

k1

4

Teoria dos Jogos

Para constru do decisor proposto, foi utilizado
um algoritmo chamado N-Player Game (NPG)
(Chatterjee, 2009). Este algoritmo calcula o equilbrio de Nash para jogos nao-cooperativos entre
varios jogadores. Primeiramente alguns conceitos
basicos serao definidos.

ISBN 978-85-8001-069-5

O espaco de todos os perfis de estrategia mista
e o produto cartesiano de todos os conjuntos mi ,
sendo denominando o espaco de estrategias mistas. Um vetor p   e denominado um perfil
de estrategia mista. Cada perfil determina um
payoff esperado, ou seja, uma media dos payoffs
ponderada pelas distribuicoes de probabilidades.
Outro fato importante e que todos os criterios ba-

5423

Anais do XIX Congresso Brasileiro de Automática, CBA 2012.

sicos para solucoes de jogos em estrategias puras
podem ser estendidos para estrategias mistas. Assim, todos os criterios explicados de dominancia e
de equilbrio de Nash podem ser usados nas estrategias mistas. Podemos dizer que todo jogo definido por matrizes de payoffs possui um equilbrio
de Nash em estrategias mistas. O que nos leva ao
fato que qualquer problema modelado na forma de
um jogo nao cooperativo possui um equilbrio de
Nash e sua matriz de payoff probabilstica.
4.2

O algoritmo N-Player Game (NPG)

O algoritmo funciona calculando o equilbrio de
Nash de estrategias mistas para n jogadores com
m estrategias com a respectiva matriz de payoff
de estrategias puras dos jogadores. A partir
das estrategias mistas calculadas, a otimiza
para cada jogador se baseia em maximizar o seu
payoff dado que os jogadores adversarios escolhem suas estrategias baseadas no equilbrio de
Nash. Consequentemente, o jogador tenta minimizar o gap entre o payoff otimo e o payoff obtido por uma possvel combina de estrategias
mistas (Chatterjee, 2009). O algoritmo entao ira
transformar o problema de equilbrio de Nash em
um problema comum de otimiza nao linear na
forma
min

f (x)
g(x)  0
h(x)  0
xi  0, i  1, ..., m

s.t.

(8)

xi  irrestrito,
i  m + 1, m + 2..., m + n
onde,

f (x) 

X

( i  ui ())

(9)

iN
i

g(x)  u (
h(x) 

mi
X

i

, sji )

  ,j1,2,...,m

ji  1

i

equilbrio de Nash, com esse metodo busca-se encontrar a solu que fornece o melhor equilbrio
entre o erro e2 e a norma w da rede. Para constru do decisor, foi necessario modelar o conjunto PO em um problema de teoria de jogos, ou
seja, definindo os jogadores e as estrategias. Como
jogadores, foi determinado que cada ponto do pareto seria um jogador e como estrategia de cada
jogador a norma e o erro quadratico que formam
a curva de pareto. Assim cada ponto (jogador)
pode definir como sua estrategia um dos valores
que o forma (analogo a escolher par ou impar em
um jogo). Definido isto, e necessario gerar uma
equa matematica que defina uma rela entre a norma e o erro de cada ponto para o calculo
da matriz de payoff, que sera entao usada posteriormente pelo algoritmo NPG para o calculo do
equilbrio de Nash baseado em estrategias mistas.
Gerar a equa e o principal ponto do problema,
sendo que a matriz de payoff baseado nas estrategias puras deve ser bem elaborada para que o
ponto calculado seja correto e esteja dentro do
cone de solucoes otimas do pareto. Para testes
gerais uma formula simples foi gerada, sendo
proposto uma analise e estudo posterior para uma
reformula da equa adotada que gere uma
perfeita rela entre a norma e o erro. Primeiramente deve-se calcular a matriz de payoff de estrategias puras. A escolha da estrategia dado um
ponto i e definida em duas partes, tendo como parametro a probabilidade a priori das estrategias.
Como no caso de jogar uma moeda temos que a
probabilidade a priori de cara ou coroa e igual
(50 de chance). Neste caso a regra de jogo criada tem como base que o jogador escolha uma das
duas estrategias sendo esta de maior peso (75)
e a nao selecionada com menor peso (25).
A primeira se si propiciar o erro mostrada na
Equa (12), e a segunda se si ira propiciar a
norma, descrita na Equa (13).

i

i  N

,i N

P r1si  e2i

0.75
0.25
+ wi 
2
max(e )
max(w)

(12)

P r1si  e2i

0.75
0.25
+ wi 
2
max(e )
max(w)

(13)

(10)
(11)

j1

de acordo com (Chatterjee, 2009), i e o payoff
otimo do jogador i e  e o equilbrio de Nash do
jogo. Para minimizar a Equa (9) o algoritmo
usa o metodo de programa quadratica sequencial (Sequential Quadratic Programming - SQP).
Mas qualquer metodo de otimiza que resolva
problemas nao lineares poderia ser usado.

onde e2i e wi  sao o erro de treinamento e a
norma respectivamente para o jogador (solu)
i. Entao o payoff de estrategias puras de cada
jogador i, levando-se em conta que as estrategias
devem competir entre si ja que o jogo e nao cooperativo, e definido pela matriz de payoff
Si  P r1si 

n
X

P r1sj

(14)

j1

5

Decisor Proposto

O metodo proposto busca encontrar a melhor solu do conjunto Pareto-otimo (PO) atraves do

ISBN 978-85-8001-069-5

j6i

Atraves da transforma dos valores de
norma e erro em estrategias para os pontos, podese entao passar os dados para o algoritmo. Isso

5424

Anais do XIX Congresso Brasileiro de Automática, CBA 2012.

e feito da seguinte forma supondo que o numero de pontos do pareto seja igual a 3, e como
cada ponto pode assumir duas estrategias diferentes (erro ou norma), temos que o numero de
combinacoes possvel e 2n sendo n o numero de
jogadores. Logo a matriz de payoff para estrategias puras para as combinacoes de estrategias
puras (1, 1, 1), (1, 1, 2), (1, 2, 1) e assim por diante
e escrita na forma
 1o linha seria, por exemplo, (s1,s2,s3) (0.5,
0.8, 1)
 2o linha seria, por exemplo, (s1,s2,s3)
(0.2,0.3,0.9)
e assim sucessivamente ate a 8o linha. Apos a execu do algoritmo e retornado um vetor de payoff
obtido pela combina de estrategias mistas, do
qual possui um valor para cada ponto (jogador),
onde a posi com menor valor ira indicar qual
ponto do pareto sera selecionado para a classifica dos dados de testes no algoritmo Multiobjetivo. Espera-se que este ponto calculado pelo
equilbrio de Nash selecionado possua uma acuracia aceitavel ou que seja a melhor solu dentro
do pareto.

e esses dados passaram a pertencer ao conjunto de
treinamento.
Tabela 1 Resultados referentes ao algoritmo com
regulariza bayesiana.
Base
CE R. Bayesiana Maximo
30
0.78  0.05
0.89
heart
5
0.78  0.05
0.89
30
0.72  0.04
0.81
diabetes
5
0.74  0.03
0.81
30
0.96  0.02
0.99
breast
5
0.96  0.01
0.98
Pode-se observar que os metodos possuem resultados bem proximos, obtendo uma classifica
quase identica para as 3 bases. Mesmo quando o
numero de neuronios e superestimado, os algoritmos continuam a retornar resultados aceitaveis,
nao evidenciando o processo de overfitting causado pelo super dimensionamento da camada escondida.
Tabela 2 Resultados para os decisores de valida e NPG
Base

6

Resultados

Os testes foram elaborados utilizando o MATLAB
R2008a para a execu do algoritmo. As bases
de dados foram obtidas atraves do repositorio publico UCI (Frank and Asuncion, 2010). Todas as
bases utilizadas no artigo sao binarias the Stalog
heart disease (heart), the Pima Indians diabetes
(diabetes) e the Wisconsin breast cancer (breast).
Todas bases foram normalizadas com media x  0
e desvio padrao   1. Para ter validade estatstica, todos os algoritmos foram executados 30
vezes para cada teste. Sao mostrados os resultados da media e desvio padrao para a metrica da
acuracia, utilizando 30 e 5 neuronios na camada
escondida da rede (CE). No aprendizado com o
algoritmo MOBJ, o conjunto de solucoes foi composto por 10 redes MLP, onde foi utilizada a fun de transferencia tangente_hiperbolica para a
camada oculta e de sada da rede. A diferenca de
norma euclidiana (complexidade) entre as solucoes
foi de w  0.5.
A Tabela 1 mostra os resultados referentes ao
algoritmo de regulariza Bayesiana, onde foram
utilizados 80 dos dados para o treinamento e
20 para os testes. A Tabela 2 apresenta os resultados para o metodo MOBJ, alem do decisor
NPG proposto, foi utilizado o decisor de valida, onde 60 dos dados foram utilizados para
treinamento, 20 para valida e 20 representaram os testes. Ja para os testes com o decisor
NPG foram utilizados 80 dos dados para o treinamento e 20 para o teste, ja que a nova abordagem nao necessita de um conjunto de valida

ISBN 978-85-8001-069-5

heart
diabetes
breast

CE
30
5
30
5
30
5

Valida
0.83  0.04
0.82  0.03
0.76  0.03
0.76  0.03
0.97  0.02
0.96  0.01

Max
0.93
0.87
0.82
0.82
1
0.99

NPG
0.84  0.05
0.82  0.04
0.76  0.03
0.76  0.03
0.94  0.02
0.94  0.02

Max
0.91
0.89
0.82
0.82
0.98
0.98

De acordo com os resultados podemos dizer
que o decisor NPG teve uma performance bem
parecida com o decisor de valida, sendo que
para alguns casos apresenta resultados superiores.
Isto se explica porque o decisor de valida escolhe o seu ponto baseado na acuracia obtido para
a classifica do conjunto de valida, enquanto
o NPG substitui esta parte de valida e escolhe a solu baseado no equilbrio de Nash entre
a norma e o erro de cada ponto. A acuracia do
decisor NPG pode ser melhorada se a formula
das Equacoes (12) e (13) forem melhor elaboradas
como dito anteriormente, ja que esta formula
mais simples foi feita apenas para demonstrar o
uso do metodo como decisor. Outro fato importante a salientar e que quanto mais pontos no pareto, mais tempo o NPG gastara para executar,
ja que o numero de combinacoes sobe exponencialmente.
7

Conclusoes

O objetivo deste trabalho foi apresentar os resultados preliminares de um novo decisor para o metodo MOBJ. Em outras maquinas_de_aprendizado
como as SVMs e metodos de regulariza por
exemplo, a tomada_de_decisao depende do acerto
de um ou mais parametros, exigindo a experien-

5425

Anais do XIX Congresso Brasileiro de Automática, CBA 2012.

cia do usuario e muitas vezes uma busca exaustiva pelos melhores valores de parametros. O que
contrasta com o decisor proposto, ja que ele nao
necessita de parametros informados pelo usuario
e descarta a necessidade de um conjunto de valida. Baseado na teoria_dos_jogos, o decisor encontra a solu que fornece o equilbrio de Nash
em um cenario conflitante entre a norma (complexidade) e o erro da rede MLP. Resultados experimentais com benchmarks n-dimensionais mostraram que o metodo de decisao proposto, aliado
ao algoritmo MOBJ, foi eficiente e pode controlar bem os problemas de super dimensionamento
da rede, evitando que haja overfitting na classifica dos dados. Outras metodologias podem
ser estudadas no futuro, para obter um algoritmo
que forneca uma execu mais rapida e mais precisa para a classifica. Sendo que o proprio metodo proposto pode ser reformulado para apresentar uma equa mais refinada entre a norma e o
erro dos pontos do pareto.
Agradecimentos

Koerich, A. (2008). Aprendizagem Bayesiana.
Material da disciplina de Aprendizado de maquina da PUCPR, Pontifcia Universidade
Catolica do Parana - PUCPR.
MacKay, J. (1992). A practical bayesian framework for backpropagation networks, Neural computation 4(3) 448472.
Medeiros, T. H., Takahashi, H. C. R. and Braga,
A. (2009). A incorpora do conhecimento
previo na tomada_de_decisao do aprendizado
multiobjetivo, Congresso Brasileiro de Redes
Neurais - Inteligencia Computacional vol.
9 2528.
Sartini, B., Garbugio, G., Bortolossi, H., Santos,
P. and Barreto, L. (2004). Uma introdu a teoria_dos_jogos, II Bienal da SBM
Universidade Federal da Bahia .
Teixeira, R. A., Braga, A. P., Takahashi, R. H. C.
and R., S. R. (2000). Improving generalization of mlps with multi-objective optimization, Neurocomputing Vol. 35(1) 189194.

O presente trabalho foi realizado com o apoio financeiro da CAPES - Brasil
Referencias
Chatterjee, B. (2009). An optimization formulation to compute nash equilibrium in finite games, Methods and Models in Computer Science, 2009. ICM2CS 2009. Proceeding of International Conference on, IEEE, pp. 15.
Dan Foresee, F. and Hagan, M. (1997). Gaussnewton approximation to bayesian learning,
Neural Networks, 1997., International Conference on, Vol. 3, IEEE, pp. 19301935.
Frank, A. and Asuncion, A. (2010). UCI machine learning repository httparchive.
ics.uci.eduml, University of California,
Irvine, School of Information and Computer
Sciences .
Fudenberg, D. and Tirole, J. (1991). Game Theory, 1991, MIT Press.
Hagan, M. and Menhaj, M. (1994). Training feedforward networks with the marquardt algorithm, Neural Networks, IEEE Transactions
on 5(6) 989993.
Jin, Y. and Sendhoff, B. (2009). Corrections
to pareto-based multiobjective machine learning An overview and case studies, Systems, Man, and Cybernetics, Part C Applications and Reviews, IEEE Transactions on
39(3) 373373.

ISBN 978-85-8001-069-5

5426