Anais do XX Congresso Brasileiro de Automática
Belo Horizonte, MG, 20 a 24 de Setembro de 2014

ANÁLISE DE ALGORITMOS DE MAXIMIZAÇÃO DE MARGEM BASEADOS EM GRAFOS DE GABRIEL E FECHO
AFIM.
PAULO V.C SOUZA, LUIZ C.B. TORRES, ANTÔNIO P. BRAGA, FREDERICO G. F. COELHO
PROGRAMA DE PÓS-GRADUAÇÃO EM ENGENHARIA ELÉTRICA - UNIVERSIDADE FEDERAL DE MINAS GERAIS
AV. ANTÔNIO CARLOS 6627, 31270-901, BELO HORIZONTE, MG, BRASIL
E-MAILS GOLDENPAUL@INFORMATICA.ESP.UFMG.BR, LUIZLITC@GMAIL.COM, APBRAGA@UFMG.BR, FREDGFC@GMAIL.COM
Abstract The present work has the objective to evaluate two methods of pattern classification based on maximum margin of
computational geometry, also comparing them with the Support Vector Machine, which is now considered state of the art for it.
A way to reduce cyclomatic complexity based on the Gabriel graph algorithm to improve its performance is also presented. The
results will show that the methods evaluated had statistically similar accuracy to the results of SVMs, and that reducing the cyclomatic complexity of the algorithm based on Gabriel Graph reduced computational cost.
Keywords Margin maximization, Support Vector Machine, Affine Hulls, Gabriel graph, Computational Geometry, Cyclomatic Complexity.
Resumo O presente trabalho possui o objetivo de avaliar dois métodos_de_classificação_de_padrões de margem máxima baseados em geometria_computacional, comparando-os também com o Support Vector Machine, que é hoje considerado o estado da
arte para o assunto. Também é apresentada uma maneira de diminuir a complexidade_ciclomática do algoritmo baseado em grafo de Gabriel para melhorar seu desempenho. Os resultados vão mostrar que os métodos avaliados apresentam precisão estatisticamente similares aos resultados das SVMs, e que a redução da complexidade_ciclomática do algoritmo baseado no Grafo de
Gabriel reduziu seu custo_computacional.
Palavras-chave .

1

Neste trabalho é feita uma comparação estatística na análise dos resultados obtidos por estes três
métodos, onde os resultados das SVMs são considerados como benchmark. Também é proposta uma
alteração no algoritmo do classificador baseado no
grafo de Gabriel de forma a reduzir sua complexidade_ciclomática e diminuir seu custo_computacional.
O restante do artigo encontra-se organizado da
seguinte forma. A Seção 2 apresenta os conceitos
teóricos envolvidos nos algoritmos estudados. Na
Seção 3 são descritos a metodologia utilizada para
avaliação de funcionamento nos testes realizados,
bem como os experimentos utilizados para as avaliações e seus respectivos resultados. Na Seção 4, é
descrita a modificação proposta para o algoritmo
baseado no grafo de Gabriel, a metodologia e conceitos utilizados para propor as melhorias necessárias,
além de experimentos que confirmam a eficiência da
mudança. Finalmente, na Seção 5 são apresentadas as
conclusões.

Introdução

Em um problema de classificação_de_padrões os
modelos são treinados para encontrar a superfície de
separação ótima entre as classes do conjunto de dados. O paradigma mais conhecido para a solução de
problemas de classificação_de_padrões é o supervisionado, onde as instâncias do conjunto de treinamento
são rotuladas. Dentre os métodos existentes, os que
se baseiam em princípios geométricos tem um apelo
mais forte quanto  interpretação visual, o que facilita
uma maior compreensão do problema e de sua solução. No entanto eles podem ter um custo_computacional elevado em relação a outros.
A superfície de separação ótima é aquela que
maximiza a margem entre o hiperplano de separação
e as classes de dados. As Support Vector Machines
(SVM) (Vapnik, 1999) são por natureza modelos de
classificadores de margem máxima e são consideradas hoje o estado da arte neste tipo de classificador.
No entanto a compreensão de como é maximizada a
margem nas SVMs não é tão intuitiva e natural como
é nos métodos geométricos que tentam encontrar a
superfície de separação geometricamente e muitas
vezes diretamente no espaço de entrada. O método
classificador geométrico baseado no grafo de Gabriel
(Torres, Castro  Braga, 2011) e o método classificador geométrico baseado em fechos afim (Cevikalp
et al., 2010) são métodos de máxima margem que
podem ser tão precisos quanto o algoritmo das
SVMs, no entanto, sua complexidade computacional
pode impor perda de eficiência, sobretudo com relação ao tempo de execução.

2 Revisão Bibliográfica
Nesta seção são apresentadas as definições de
margem, dos algoritmos e de outros conceitos relacionados aos objetivos deste trabalho.
2.1 Margem
Seja f n um valor real de hipótese usado
para classificação. Então (Smola et al., 2000)

 f ( x, y)  yf ( x)

1246

(1)

Anais do XX Congresso Brasileiro de Automática
Belo Horizonte, MG, 20 a 24 de Setembro de 2014

Onde Xxi  i1...N representa o conjunto de
entrada e Yyi  i1...N descreve o conjunto de
saída e N é o tamanho do conjunto de dados.  é a
margem através da qual o padrão x é classificado
corretamente (de modo que um valor negativo de
 f ( x, y) corresponde a uma classificação incorre-

min
w,b

O Grafo de Gabriel é definido como sendo um
subconjunto dos pontos do Diagrama de Voronoi e
um subgrafo de uma triangulação de Delaunay
(Zhang  King, 2002).
Um grafo de Gabriel pode ser definido da seguinte forma para cada par de pontos (pi e pj) no conjunto de referência (X, ), construa uma esfera diametral
indicada por S(pi,pj), isto é, de tal modo que a esfera
(pi,, pj) forma o diâmetro S(pi, pj). Dois pontos são
ditos vizinhos se S(pi, pj) é vazio, ou seja, se há outros pontos de (X, ) diferentes de pi e pj na esfera
diametral formada entre pi e pj não existe ligação
entre os dois pontos. A Figura 2 ilustra bem a definição, pois cria uma aresta entre os pontos A e B e não
cria entre os pontos B e C devido a outro ponto dentro da esfera formada entre os pontos analisados
(Bhattacharya, Poulsen  Toussaint, 1981).

(2)

é a margem mínima ao longo de toda a amostra, sendo determinada pela pior classificação em todo o
conjunto de treinamento (x,y) (Smola et al., 2000).
É desejável obter classificadores que sejam capazes de encontrar a máxima margem, uma vez que é
esperado que o classificador seja capaz de rotular
corretamente os dados que não foram apresentados a
ele na fase de treinamento (Smola et al, 2000).
2.2 Máquina de Vetor Suporte
A Máquina de Vetor Suporte (SVM) é uma máquina de aprendizado linear que se fundamenta nos
princípios da Minimização do Risco Estrutural. A
SVM busca minimizar o erro em relação a um conjunto de treinamento e apresenta uma boa generalização (Vapnik, 1999).
Suponha que exista um hiperplano que poderia
classificar corretamente um conjunto de dados linearmente separável como mostrado na Figura 1. Isso
significa dizer que os pontos z que se encontram no
hiperplano satisfazem a equação (Zhang  King,
2002)
(3)
wT z  b  0

b

(4)

yi ( wT xi  b)  1  0 i

1 i n

w

2

2.3 Grafo de Gabriel

 f  min  f ( x, y)

norma euclidiana.

w

S .a

ta). Além disso, considere que

Onde w é um vetor normal ao hiperplano e

1
2



Figura 2. Exemplo de funcionamento do grafo de Gabriel. Disponível em
httpcgm.cs.mcgill.cagodfriedteachingprojects.pr.98sergeiproject.html.

No trabalho (Torres, Castro  Braga, 2011) foi
proposto um algoritmo de classificação de margem
larga para problemas de duas dimensões utilizando o
grafo de Gabriel. Essa solução é realizada em seis
passos listados a seguir

é a

é a distância perpendicular do

hiperplano até a origem. A Figura 1 exemplifica o
funcionamento da projeção dos pontos no hiperplano.

1- É realizado um treinamento multiobjetivo da
rede Multilayer Perceptron a partir de um
conjunto de dados Txi, yi i  1...N, obtendo-se um conjunto PO (Teixeira et al.,
2000).
2- Utilizando todos os padrões de entrada é gerado um grafo de Gabriel, formando o conjunto de vértices V e o conjunto de arestas
E.
3- Eliminação do ruído, que é quando certo valor de xi possui vértices adjacentes com rótulos diferentes de yi, sendo eliminado de V. O
passo é repetido até que não existam mais
ruídos.
4- A borda B das classes é encontrada quando
a aresta (xi,xj), com ji são formadas por
vértices de classes diferentes.

Figura 1. Projeção de dois pontos a um hiperplano separados
(Lorena  Carvalho, 2007).

As SVMs encontram este hiperplano de separação
otimizando a forma dual da seguinte função objetivo

1247

Anais do XX Congresso Brasileiro de Automática
Belo Horizonte, MG, 20 a 24 de Setembro de 2014

5- Neste passo são calculados os pontos médios. Que são os pontos medianos entre as
arestas dos pontos pertencentes a B.
6- Encontra a solução que maximiza a margem
de separação. O algoritmo escolhe a solução
wk que pertence ao conjunto de soluções do
conjunto PO (W) mais próximo dos pontos
médios encontrados no passo cinco.

tivos resumidos a um. Mais formalmente o fecho
convexo de (xi) i .... n pode ser escrito como (Cevikalp
et al., 2010)

A Figura 3 busca retratar a resolução do algoritmo de Gabriel para um grupo de dados, onde a primeira etapa 3 (a) mostra a disposição dos dados, a
segunda, 3(b) evidencia a formação do grafo de Gabriel e a terceira 3(c) é o hiperplano de separação
entre as classes

Esse conceito é utilizado na obtenção da solução
de margem máxima pela SVM. Seguindo uma aproximação encontrada pela Equação (5) em dois fechos
convexos o método SVM encontra os pontos mais
próximos dos dois fechos_convexos, ligando-os por
um segmento de reta. O plano ortogonal em relação
ao segmento de reta que corta a linha é selecionado
para ser o hiperplano separador (Cevikalp et al.,
2010). Na Figura 4 encontramos um exemplo de fecho convexo.

n
n


H convex  x   i x i   i  1   i  0.
i 1
i 1



(5)

(a)

Figura 4. Exemplo de fecho convexo.

O conceito de fecho_afim foi utilizado para formular um novo algoritmo para encontrar a margem
máxima de dados linearmente separáveis (Cevikalp et
al., 2010).
O fecho_afim de uma classe é o menor subespaço
afim que os contenham. Por ser um modelo extremamente solto e sem limites, a modelagem por fecho
afim pode ser uma melhor escolha quando comparado com o fecho convexo para grandes dimensões de
dados. Formalmente o fecho_afim é definido como
(Cevikalp et al., 2010)

(b)

(c)
Figura 3. Resolução do algoritmo Grafo de Gabriel.

n
n


H aff  x    i x i    i  1.
i 1
i 1



2.4 Fecho Convexo e Fecho Afim
Para resolver problemas com classes linearmente
separáveis, soluções baseadas em conceitos da geometria_computacional foram desenvolvidas para melhorar o desempenho de algoritmos para encontrar a
máxima margem. Os algoritmos são baseados em
conceitos de fecho convexo e fecho_afim (Cevikalp et
al., 2010).
Nesses métodos, cada classe é aproximada com
um fecho_afim construído a partir de suas amostras de
treinamento e o rótulo de uma amostra de teste é determinado com base na distância ao fecho_afim mais
próximo (Cevikalp et al., 2010).
Um fecho convexo consiste em todos os pontos
que podem ser escritos com uma combinação convexa dos pontos do conjunto original. Uma combinação
convexa de pontos é uma combinação_linear de pontos de dados onde todos os coeficientes são não nega-

(6)

Onde o fecho_afim das amostras (xi)i...n que contém
todos os pontos na forma de



n

i  1 .

i 1

n

 i xi

i 1

com

Na Figura 5 exemplificamos o fecho

afim.

Figura 5. Exemplo de fecho_afim.

1248



Anais do XX Congresso Brasileiro de Automática
Belo Horizonte, MG, 20 a 24 de Setembro de 2014

para a obtenção das respostas. Como o objetivo desse
trabalho é avaliar o desempenho dos algoritmos para
a obtenção de máxima margem com dados linearmente separáveis, utilizaremos bases de dados que possuem duas classes. Para o caso da base de dados Iris
que originalmente possui três classes, nesse teste foi
manipulada e feita a união entre as classes versicolor
e virginica para juntamente com a classe setosa formarem uma estrutura linearmente separável (Bache 
Lichman, 2013).
Nosso objeto de comparação entre os métodos
será a acurácia, que é uma medição entre a comparação de um valor de resposta do algoritmo com um
valor referência de saída dos dados. Juntamente com
a acurácia serão calculados os desvios padrões para
cada um dos testes realizados. Antes das avaliações
do algoritmo, os dados de entrada foram normalizados com média  0 e desvio padrão  1 para execução dos testes para cada uma das bases avaliadas.
Cada um dos métodos tem uma forma de avaliar a
precisão de seus resultados após a obtenção da máxima margem. No método SVM encontramos a acurácia através da comparação da resposta obtida pela
função de classificação dos dados com um vetor y
que possui valores booleanos, determinado pela
comparação dos filtros das classes (Cristianini 
Shawe-Taylor, 2000). Já no método do grafo de Gabriel são comparados os resultados do vetor y de
treinamento com um vetor obtido através da resposta
de uma função que recebe a rede formada e os dados
de entrada (Torres, Castro  Braga, 2011). No método de fecho_afim a acurácia é determinada através da
comparação do vetor Y, formado por -1 e 1 que determinam as posições dos dados no subespaço, com a
resposta da iteração entre o vetor X de treinamento. O
valor de w e b são obtidos na função que encontra o
fecho_afim que separa a base de dados utilizada no
teste. A resposta deste vetor é uma lista de valores
positivos e negativos para os referidos dados, indicando quantos dados estão em um fecho_afim e quantos estão no outro fecho_afim calculado (Cevikalp et
al., 2010). Os dados da acurácia (AC) coletada e do
desvio padrão (DV) dos testes são mostrados a seguir
em um quadro onde indicam o seu valor em percentual para cada uma das bases analisadas. A tabela 1
apresenta os resultados dos testes propostos e os valores da acurácia para cada caso estudado.

Para encontrar a máxima margem entre dados
linearmente separáveis, foi proposto um algoritmo
que utiliza o fecho_afim para resolver o problema
(Cevikalp et al., 2010).
Deve notar-se que a separabilidade linear de pontos de dados não garante necessariamente a separabilidade dos fechos afins correspondentes das classes.
Para os caso linearmente separáveis, é mais conveniente escrever um fecho_afim como (Cevikalp et al.,
2010)
x Uv  v l .

(7)

Onde  é definido como a média das amostras de
teste e o U é uma base ortogonal para as direções
ocupadas pelo subespaço afim. Numericamente U
pode ser encontrada por uma matriz gerada por SVD
(Singular Value Decomposition) de x1  , ..., xn 
. É indicado que foram retiradas somente as colunas da Matriz U que são significativamente diferentes
de zero. Após provas e deduções, é confirmado que a
máxima margem é encontrada pela expressão (Cevikalp et al., 2010)
f (x)  < w, x > + b

w  12 (x   x - )  12 (I - P)(   )

(8)
(9)

Onde P é a projeção ortogonal sobre o espaço conjunto da orientação contida nos dois subespaços e I-P
é a projeção correspondente para o complemento
ortogonal dessa amplitude (Cevikalp et al., 2010).
3 Testes realizados
Para verificar o funcionamento dos métodos foram feitos testes referentes  resolução e obtenção da
resposta de maximização_de_margem conforme os
três métodos descritos anteriormente. Para a SVM foi
utilizado nesse trabalho um algoritmo que trabalha
com a opção de validação_cruzada para encontrar os
parâmetros. Para obtermos padrões consistentes de
verificação de resultados utilizaremos bases de dados
conhecidas em estudos de aprendizado de máquina
disponível no UCI machine learning repository (Bache  Lichman, 2013). As bases escolhidas para a
realização dos testes foram a Iris, representando dados de três tipos de plantas, a base de dados Ionosphere, que representa medições de elétrons livres na
ionosfera, a Pima Indian Diabetes (PID) que traz
informações coletadas referentes  presença de sinais
de diabetes em pacientes e por fim a base Wisconsin
Diagnostic Breast Cancer (WDBC), contendo informações sobre diagnósticos de câncer de mama (Bache  Lichman, 2013). Com uma mesma base de
dados é possível comparar os resultados de cada um
dos três métodos na solução da maximização da margem, evitando problemas de informações aleatórias

Tabela 1. Resultado das Medições dos testes com os três algoritmos.

Na tabela 1 verificamos que os três algoritmos
avaliados expuseram resultados semelhantes de acurácia e baixos valores de desvio padrão na avaliação

1249

Anais do XX Congresso Brasileiro de Automática
Belo Horizonte, MG, 20 a 24 de Setembro de 2014

de cada uma das classes de teste. Na busca de encontrar a margem de separação dos dados a Máquina de
Vetor Suporte apresentou melhores resultados de
acurácia na avaliação de duas classes. O algoritmo do
grafo de Gabriel apresentou melhor desempenho nos
testes da classe íris e o fecho_afim na classe Ionosphere.
Após a coleta dos dados, verificamos que a acurácia dos dois métodos estudados é muito parecida
com o estado da arte para encontrar a maximização
da margem. Para confirmarmos os resultados coletados aplicaremos testes estatísticos para confirmar
qual método tem melhor acurácia para as bases de
dados utilizadas. O atributo de interesse do estudo
será a acurácia dos métodos na obtenção de maximização_de_margem com dados linearmente separáveis
de cada um dos três métodos em cada uma das bases
de dados estudada.

Figura 6. Plot dos dados coletados no teste de acurácia dos algoritmos.

Para verificar as premissas do resultado da análise das variâncias, foram utilizados três testes para
confirmar respectivamente a normalidade, homoscedasticidade e independência dos dados (Montgomery,
1997). Para confirmar as condições normais dos dados, utilizamos o teste de Shapiro-Wilk que resultou
na confirmação de normalidade das informações analisadas. Já no teste de homoscedasticidade que verifica a igualdade de variâncias dos resíduos (Montgomery, 1997) foi verificada a aceitação da hipótese
nula para o teste de Fligner-Killeen, que nos informa
que as variâncias dos resíduos envolvidos na análise
são iguais. Por fim o teste Durbin-Watson confirmou
que os resíduos possuem independência. A figura 7
apresenta os resultados obtidos pelas validações propostas.

As definições das hipóteses são fundamentais
para o desenvolvimento dos testes e condução das
conclusões corretas para resolvermos a questão alvo
(Montgomery, 1997). Para essa análise definiremos
nossa hipótese nula (H0) que os algoritmos possuem
valores médios de acurácias iguais e nossa hipótese
alternativa (H1) é que os algoritmos têm valores médios de acurácia diferentes na obtenção da maximização_de_margem. Os estudos serão feitos com um
intervalo de confiança de   95.
Com as informações dos algoritmos coletadas, foi
verificado como as medições das três soluções se
comportavam em um gráfico bloxplot, apresentando
diferenças visuais que nos auxiliam nas conclusões
em aceitar a hipótese nula. Porém para uma maior
confiança estatística_foi proposto realizar um teste de
análise de variâncias com os dados disponíveis (teste
aov) (Montgomery, 1997). O resultado do teste trouxe a conclusão que devemos aceitar a hipótese nula
(p-value 0.991) de que os algoritmos possuem valores médios de acurácias iguais na obtenção da maximização_de_margem para as bases de dados utilizadas. De posse do resultado do teste aov, devemos
verificar alguns fatores se as premissas de testes são
verificadas, quanto s médias dos algoritmos diferem
do algoritmo de referência (SVM) e qual dos dois
algoritmos (grafo de Gabriel ou fecho_afim) apresentam melhor acurácia em maximizar a margem. A
Figura 6 apresenta o resultado referente  acurácia
dos algoritmos estudados.

Figura 7. Gráficos de confirmação das premissas dos dados.

Como não aconteceram problemas com os testes
de verificação de premissas, podemos concluir, com
um nível de 95 de confiança, que devemos aceitar a
hipótese nula de que os algoritmos possuem valores
médios de acurácias iguais na obtenção da maximização_de_margem para as bases de dados utilizadas nos
testes.
Sabendo que os valores médios de acurácia são
bem parecidos entre os três algoritmos, vamos realizar comparações das acurácias com o algoritmo de
referência através de comparações múltiplas Post-hoc
1250

Anais do XX Congresso Brasileiro de Automática
Belo Horizonte, MG, 20 a 24 de Setembro de 2014

usando o teste de Dunnett (Montgomery, 1997). O
resultado do referido teste segue abaixo conforme o
quadro 1 de informações

mo original propõe percorrer os dados buscando
formar as ligações com pontos mais próximos. A
ligação entre os pontos é realizada utilizando uma
estrutura de laços de repetição de 1 ate n, onde n é o
tamanho dos dados utilizados, percorrendo as linhas
da matriz formada com o cálculo das distâncias euclidianas entre os pontos envolvidos na formação do
grafo. Dentro do laço de repetição anterior outro laço
é iniciado de 1 até o n dados de entrada utilizados no
exemplo, percorrendo as colunas da matriz de distâncias. Após o término dessa etapa, um novo laço de
repetição, que percorre de 1 até m, onde m é o tamanho máximo dos dados, é inicializado, obtendo-se os
índices dos dados que estão em situação de ruído
(Torres, Castro  Braga, 2011). A proposta da mudança no algoritmo é unir essas duas etapas fazendo
com que a quantidade de laços de repetição seja menor, melhorando o desempenho computacional do
algoritmo, evitando grande número de repetições
efetuadas nos laços de repetição, mantendo as características eficientes na remoção dos ruídos. Foi alterada a estrutura de busca e identificação dos ruídos
fazendo com que as ações fossem realizadas em uma
única estrutura de repetição.
A melhora da qualidade do processamento de um
algoritmo através da diminuição dos laços de repetição pode ser explicada pelo conceito de complexidade_ciclomática (McCabe, 1976).
A complexidade_ciclomática de um algoritmo é a
medida que estima a complexidade lógica de um programa desenvolvido em alguma linguagem de programação. O método que encontra a complexidade
lógica do trecho de código estudado pode ser definido através da seguinte equação (McCabe, 1976)

Tukey multiple comparisons of means
95 family-wise confidence level
Fit aov(formula  Acuracia  Algoritmo, data  algoritmos)

Algoritmo
diff
adj
GABRIEL-FECHO
0.9995568
SVM-FECHO
0.9906375
SVM-GABRIEL
0.9942468

0.000575

lwr
-0.0560378

upr

p

0.0571878

0.002650 -0.0539628 0.0592628
0.002075

-0.0545378

0.0586878

Quadro 1. Comparações post-hoc dos algoritmos estudados.

Após a execução dos testes podemos concluir que
no geral os três algoritmos realmente apresentaram
resultados parecidos para as bases de dados testados
na obtenção de máxima margem e os dois algoritmos
baseados em geometria_computacional são um pouco
inferiores ao estado de arte.
Quando comparamos as duas soluções geométricas, os resultados são muito próximos para os casos
estudados, com leve vantagem no desempenho do
algoritmo do grafo de Gabriel.
4 Proposição de melhoria ao algoritmo Grafo de
Gabriel
A utilização de estruturas geométricas para resolver problemas de separação de margem é extremamente eficiente conforme os testes realizados. Os
algoritmos apresentam acurácia bem próxima a do
SVM conforme os testes realizados, porém o tempo
de execução e o esforço computacional são maiores
para realizar a maximização_de_margem. No algoritmo do grafo de Gabriel esse fato acontece devido ao
excesso de funções que realizam as tarefas de percorrer todos os dados das classes, para encontrar as informações necessárias para obtenção de margem máxima.
A forma como os algoritmos são implementados
e as estruturas utilizadas para sua construção podem
afetar sensivelmente o desempenho das atividades
computacionais.
Por se tratar de uma estrutura eficiente para encontrar a máxima margem em problemas linearmente
separáveis, propomos melhorar o algoritmo original
para encontrar o grafo de Gabriel otimizando ações
computacionais do algoritmo, aglutinando tarefas que
podem ser executadas agregando melhor desempenho
ao processo de obtenção da maximização da margem.
No algoritmo do grafo de Gabriel após um treinamento multiobjetivo da rede, utilizam-se todos os
padrões de entrada para formar o grafo com seus
conjuntos de vértices e arestas. Então após a estrutura
formada, percorre-se todo o grafo buscando identificar e eliminar os ruídos (dados classificados incorretamente), evitando prejudicar a busca da margem
máxima (Torres, Castro  Braga, 2011). O algorit-

V(G)  e - n + p

(10)

Onde a variável e representa o número de arestas do
grafo, n é o número de nodos ou nós do grafo e p é o
número de componentes ligados (McCabe, 1976).
Esse tipo de métrica verifica a quantidade de caminhos lógicos de um código, formando nodos de um
grafo para cada uma das ações que o algoritmo possui. Para cada uma das variações utilizadas em estruturas como if, else, for, while e case são gerados desvios de fluxo no grafo (McCabe, 1976). A base fundamental de funcionamento da estrutura ciclomática é
representada na Figura 8.

1251

Anais do XX Congresso Brasileiro de Automática
Belo Horizonte, MG, 20 a 24 de Setembro de 2014

(c)
Figura 10. Comparação dos resultados finais do algoritmo original
e o algoritmo de Gabriel modificado onde os dados estão apresentados sequencialmente dados originais (10 (a)), resultado do
algoritmo original (10 (b)) e resultado do algoritmo com a melhoria proposta (10 (c)).

Figura 8. Exemplos de funcionamento da medição da complexidade_ciclomática (McCabe, 1976).

Existem alguns plug-ins que são instalados na
IDE para desenvolvimento Java, Eclipse, que fornecem a complexidade_ciclomática de todos os métodos
envolvidos em um pacote de dados quando esses
estão desenvolvidos na linguagem de programação
Java. Para testarmos a solução foi instalado o plug-in
Metrics, que faz a verificação da complexidade_ciclomática de métodos em Java. Como o algoritmo
original foi desenvolvido em linguagem Matlab e sua
relação com linguagem como o C, C++ e Java é muito grande, foi possível transcrever o trecho de código
analisado para Java, possibilitando a comparação
ciclomática da solução inicial e do trecho de código
com a melhoria (reduzindo os laços de repetição).
Como resultado obtivemos que o algoritmo inicial
tem como complexidade_ciclomática V(G)  (9) e o
algoritmo modificado possui V(G)  (8). Portanto é
comprovado que a modificação proposta de aglutinar
os passos de percorrer e simultaneamente identificar
os ruídos possui menor custo_computacional, baseado
na complexidade_ciclomática encontrada. A Figura 9
é o resultado mostrado no Eclipse para a análise ciclomática das soluções avaliadas.

Como comprovamos que a nova solução diminui
a complexidade_ciclomática do método, desejamos
provar se as modificações diminuirão o tempo de
execução para realizar as tarefas.
Para testar a solução proposta foi criado um
script de teste que deseja concluir, através da coleta
do tempo de execução de identificação do ruído, se a
melhoria além de melhorar o desempenho computacional, diminuiu o tempo de execução da solução.
Para verificar o tempo_real da execução, utilizamos
um notebook marca Qbex, processador Intel
Atom CPU N455 @1.66GHz 1.67GHz, memória
RAM de 2.00 GB, o software Matlab 2013 e alguns
cuidados foram tomados para que fatores externos
não interferissem na execução das tarefas desativar
todos os softwares concorrentes de processamento
como atualizações de antivírus, atualização de navegadores da internet, etc. No código foram inseridas
estruturas que coletam o início e o término do tempo
da execução, sendo exibida no console a duração
total da ação. O script faz a chamada das duas funções no mesmo código, pois desejamos avaliar as
condições do tempo de execução do software utilizando dados com as mesmas características. Para
evitar tendências nas medições que poderiam ser geradas pela inicialização das variáveis, o script foi
acrescentado de uma chamada da função original que
deveria ser desconsiderada para critério de coleta dos
dados.
Para verificar o tempo médio de execução dos
dois algoritmos executamos o script 12 vezes e coletamos as 12 medições de tempo de execução de cada
um dos algoritmos. Para efetuarmos as conclusões
acerca dos tempos coletados, realizaremos um teste
estatístico para verificar se o tempo de execução da
solução proposta apresenta tempo inferior  solução
atual do grafo de Gabriel.
Antes de efetuar os testes definiremos para o referido caso que nossa hipótese nula é que os algoritmos
possuem tempos iguais de execução e como hipótese
alternativa que os tempos de execução dos algoritmos
são diferentes.
De posse dos dados realizamos os mesmos testes
para a resolução da acurácia dos algoritmos Análise
de variância, Shapiro-Wilk, Fligner-Killeen e Durbin-Watson e obtivemos como respostas para nossos
testes estatísticos que devemos aceitar a hipótese

Figura 9. Visão no Eclipse do Plug-in Metrics dos resultados da
complexidade Ciclomática dos métodos originais e alterados.

Na execução dos dois algoritmos, verificamos o
resultado de saída para a mesma base de dados e
concluiu-se que a alteração no código não refletiu na
acurácia do algoritmo grafo de Gabriel. Na Figura 10
podemos verificar tal fato.

(a)

(b)

1252

Anais do XX Congresso Brasileiro de Automática
Belo Horizonte, MG, 20 a 24 de Setembro de 2014

nula de que os algoritmos possuem tempos iguais de
execução. Todas as premissas foram validadas e a
possibilidade de se aceitar a hipótese nula é aceitável.
Na Figura 11 podemos visualizar a comparação entre
os tempos de execução.

California, School of Information and Computer
Science.
Bhattacharya, B. K., Poulsen, R. S.,  Toussaint, G.
T. (1981). Application of proximity graphs to
editing nearest neighbor decision rule.
In International Symposium on Information
Theory, Santa Monica.
Cevikalp, H., Triggs, B., Yavuz, H. S., Kçk, Y.,
Kçk, M.,  Barkana, A. (2010). Large margin
classifiers
based
on
affine
hulls. Neurocomputing,73(16), 3160-3168.
Cristianini, N., and Shawe-Taylor, J. (2000). An
Introduction to Support Vector Machines and
Other Kernel-based Learning Methods, First
Edition (Cambridge Cambridge University
Press). httpwww.support-vector.net.
Lorena, A. C.,  de Carvalho, A. C. (2007). Uma
introdução s support_vector_machines. Revista
de Informática Teórica e Aplicada, 14(2), 43-67.
McCabe, T. J. (1976). A complexity measure.
Software Engineering, IEEE Transactions on,
(4), 308-320.
Montgomery, D. C.,  Montgomery, D. C.
(1997). Design and analysis of experiments.
New York Wiley., 5th ed. - Capítulo 3.
Smola, A., Bartlett, P., Schlkopf, B., 
Schuurmans, D. (2000). Introduction to large
margin classifiers.
Teixeira, R. A., Braga, A. P., Takahashi, R. H., 
Saldanha,
R.
R.
(2000).
Improving
generalization of MLPs with multi-objective
optimization. Neurocomputing, 35(1), 189-194.
Torres, L. C. B.  Castro, C.L.  Braga, A. P. .
Estratégia de Decisão Baseada em Margem para
o Aprendizado Multiobjetivo de Redes Neurais.
In X Congresso Brasileiro de Inteligência
Computacional, 2011, Fortaleza. CBIC, 2011.
Vapnik, V.N. The Nature of Statistical Learning
Theory. USA Springer, 2nd ed., 1999.
Zhang, W.,  King, I. (2002). A study of the
relationship between support_vector_machine and
Gabriel graph. In Neural Networks, 2002.
IJCNN02.
Proceedings
of
the
2002
International Joint Conference on (Vol. 1, pp.
239-244). IEEE.

Figura 11. Gráficos dos resultados de análise_estatística realizada
com os tempos de execução dos algoritmos.

Como a mudança não interferiu na eficiência do
método em resolver problemas de eliminar ruídos
manteve um tempo médio igual ao algoritmo original
e conseguimos diminuir a complexidade_ciclomática
do método, podemos concluir que a mudança melhora o desempenho computacional do algoritmo.
5 Conclusão
Verificamos que as soluções baseadas em geometria_computacional são excelentes em encontrar a
máxima margem para dados linearmente separáveis,
apresentando comportamentos parecidos com o estado da arte, o Support Vector Machine. Os métodos
que apresentam apelo geométrico, portanto são facilmente compreendidos e até mesmo visualizados.
Existem, para o grafo de Gabriel e o fecho_afim,
classificadores de margem larga para dados não separáveis ou para multiclasses apresentando bom resultado e o custo_computacional justificável.
Verificamos que para as situações mais simples
que foram estudadas nesse trabalho os métodos geométricos apresentam desempenho computacional e
tempo relativamente maior ao estado da arte. A melhoria proposta no código diminuiu a complexidade
ciclomática e manteve o tempo de execução, conforme comprovação estatística demonstrada.
Outros estudos para melhorar as técnicas dos
algoritmos baseados na geometria_computacional,
mantendo sua eficiência em encontrar a margem, mas
diminuir o tempo de execução pode ser proposto em
futuros trabalhos.
Agradecimentos
O presente trabalho foi realizado com o apoio financeiro do CNPq e da CAPES - Brasil.
Referências Bibliográficas
Bache, K.,  Lichman, M. (2013). UCI machine
learning repository. Irvine, CA University of

1253