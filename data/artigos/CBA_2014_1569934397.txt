Anais do XX Congresso Brasileiro de Automática
Belo Horizonte, MG, 20 a 24 de Setembro de 2014

SELECAO DE NEURONIOS ESCONDIDOS EM MAQUINAS DE APRENDIZADO
EXTREMO USANDO F-SCORES
David Pinto Andre P. Lemos Antonio P. Braga


Programa de Pos-Graduacao em Engenharia Eletrica
Universidade Federal de Minas Gerais
Av. Antonio Carlos 6627, 31270-901, Belo Horizonte, MG, Brasil
Email davidem1990@ufmg.br, andrepl@cpdee.ufmg.br, apbraga@cpdee.ufmg.br
Abstract This paper proposes a single step pruning approach for Extreme Learning Machines. Pruning is
performed using a predefined F-score measure threshold. This procedure allow retaining only the most discriminative neurons to the pattern classes. Network structure definition and parameter estimation are achieved in a
single step using only training data. No validation set and computationally intensive cross-validation procedures
are required. Experiments are performed using classification problems to validate the proposed approach.
Keywords

Extreme learning machines (ELMs), pruning, F-score.

Resumo Este trabalho propoe uma abordagem de poda em unico passo para Maquinas de Aprendizado
Extremo. A poda e realizada utilizando-se um limiar pre-definido para a medida de F-score. Tal procedimento
permite reter somente os neuronios mais discriminativos para as classes dos padroes. A definicao da estrutura
da rede e a estimacao dos parametros sao efetuados em um unico passo, utilizando-se dados de treinamento
apenas. Nenhum conjunto de validacao e procedimentos computacionalmente intensivos de validacao-cruzada
sao exigidos. Experimentos sao realizados utilizando-se problemas de classificacao para validar a abordagem
proposta.
Palavras-chave

.

Quadrado e da medida de informacao mutua entre
as sadas dos nos e os rotulos dos padroes. Miche et al. (Miche et al., 2010) descrevem o OPELM (Optimally Pruned ELM), aplicavel tanto a
problemas de classificacao quanto a problemas de
regressao. Tal abordagem inicia-se tambem pela
construcao de uma SLFN com uma camada escondida de dimensao alta. Em seguida, o algoritmo MRSR (Multi-response Sparse Regression)
(Simila and Tikka, 2005) e utilizado para ranquear os neuronios. Finalmente, sao eliminados
os neuronios menos significativos via validacaocruzada leave-one-out. Ambas as abordagens descritas sao baseadas em poda de neuronios. Outros metodos baseiam-se em procedimentos alternativos, como regularizacao durante a estimacao dos pesos da sada (Deng et al., 2009 Miche et al., 2011 Li and Niu, 2013) e solucoes
construtivas (Huang, Chen and Siew, 2006 Feng
et al., 2009 Lan et al., 2010).

Introducao

Os custos computacionais associados ao aprendizado de bases de dados grandes, muito comuns
na atualidade (Schadt et al., 2010), conduzem as
tecnicas classicas de inteligencia_computacional,
como as redes_neurais artificiais (Bishop, 1995) e
as maquinas de vetores suporte (SVMs) (Vapnik,
1995 Cristianini and Shawe-Taylor, 2000), a uma
convergencia de treinamento lenta, tornando-as
impraticaveis em algumas situacoes. A Maquina
de Aprendizado Extremo (Huang, Zhu and Siew,
2006) e um algoritmo de aprendizado para redes feedforward de unica camada escondida (Single Hidden Layer Feedforward Networks  SLFNs)
com baixa complexidade computacional, apto ao
treinamento com bases de dados grandes, e que
garante uma convergencia rapida e uma boa performance de generalizacao. A definicao aleatoria
dos parametros da camada escondida e a estimacao dos pesos de sada via mnimos_quadrados ordinarios permite efetuar o treinamento em uma
unica iteracao.
A formulacao original do algoritmo ELM nao
estabelece uma metodologia para definicao da estrutura da SLFN. A dimensao da camada escondida e inferida, geralmente, via tentativa e erro.
Muitas modificacoes do ELM ja foram propostas visando contornar esse problema. Rong et al.
(Rong et al., 2008) detalham o algoritmo P-ELM
(Pruned ELM), destinado a problemas de classificacao_de_padroes. Partindo de uma rede com
um numero alto de neuronios escondidos, a poda
e efetuada por meio de um teste estatstico Qui-

O presente trabalho propoe uma nova abordagem para poda de SLFNs com algoritmo ELM
de aprendizado, o F-Score Based Pruned ELM
(FSP-ELM), aplicavel a problemas de classificacao
de padroes. Basicamente, o problema de escolha
dos neuronios escondidos mais relevantes e tratado como um problema classico de selecao de caractersticas. Dessa forma, a metrica de F-scores
(Chen and Lin, 2006) e utilizada para avaliar o
poder discriminativo dos neuronios em relacao as
classes dos padroes. Partindo de uma SLFN com
uma quantidade alta de neuronios na camada escondida, todos os nos com F-score abaixo de um
limiar previamente definido sao eliminados. O me-

1285

Anais do XX Congresso Brasileiro de Automática
Belo Horizonte, MG, 20 a 24 de Setembro de 2014

todo proposto define esse limiar a partir dos dados
de treinamento, sem exigir conjuntos de validacao e procedimentos computacionalmente intensivos de validacao-cruzada. A poda e efetuada em
um unico passo, previamente ao ajuste dos pesos
da camada de sada da rede, conferindo alta velocidade ao algoritmo sem perda de performance
preditiva.
O restante do artigo esta organizado da seguinte forma. A Secao 2 apresenta uma breve
descricao do algoritmo de aprendizado ELM. Em
seguida, a Secao 3 detalha a metrica de F-scores
e apresenta o algoritmo de aprendizado proposto.
Na Secao 4 sao apresentados os experimentos numericos para problemas de classificacao e a analise
estatstica dos resultados. Finalmente, as discussoes e conclusoes sao apresentados na Secao 5.

As colunas da matriz H, definida em (2),
correspondem as sadas dos neuronios escondidos da SLFN com respeito a entrada X 
T
x1 x2    xN mN . O treinamento da rede, de
modo a minimizar o erro de aproximacao das N
observacoes de Y , e realizado da seguinte forma
1. Atribuicao aleatoria dos vetores de pesos wj
e dos vieses nodais bj , para j  1, 2, . . . , k
2. Computo da matriz H de sada da camada
escondida
3. Computo dos pesos da camada de sada, b 
(H T H)1 H T Y  H + Y , onde H + corresponde a pseudo-inversa de Moore-Penrose.
Camada
Intermediária

2

Maquinas de Aprendizado Extremo

h0  +1
x1

Uma SLFN com algoritmo ELM de aprendizado
difere-se das redes feedforward convencionais devido a dispensabilidade do ajuste dos parametros
da camada intermediaria, a qual pode ser composta por neuronios definidos aleatoriamente e independentes dos padroes de treinamento (Huang
et al., 2011).
Mais formalmente, dadas N amostras arbitrarias de treinamento com m caractersticas
e d sadas associadas, compondo pares do tipo
md
(xi , yi )N
, o modelo que representa
i1  R
uma SLFN com k nos escondidos (ver Figura 1,
onde d  1), funcoes de ativacao g() e algoritmo de aprendizado ELM, pode ser definido da
seguinte forma

yi 

k
X

j g(wj , xi , bj ), i  1, . . . , N

x2

T

T

Y  y1 y2    yN dN

b2

b0

y
Neurônio
de Saída

hk

Espaço Projetado
(Linearização do Problema)

Figura 1 SHLFN com neuronios intermediarios
sigmoidais.

3

Metodo de Poda por F-scores

A projecao dos dados efetuada pela camada escondida da SLFN pode ser interpretada como um procedimento de extracao_de_caractersticas, ja que
gera um novo conjunto de caractersticas, representadas pelas colunas da matriz H, a partir da
combinacao e transformacao das caractersticas da
matriz de entrada X (Mao and Jain, 1995). O objetivo da projecao no algoritmo ELM e mapear
espacos de entrada nao-lineares em espacos lineares, geralmente de maior dimensao, trataveis pela
camada de sada da rede. Entretanto, a presenca
de caractersticas dependentes (com alta correlacao) nesse novo espaco introduz informacao redundante sobre as classes dos padroes, podendo prejudicar a performance preditiva da SLFN. Uma
estrategia possvel para solucionar o problema, a
qual sera adotada no presente trabalho, consiste
em projetar os padroes de entrada para um espaco
de dimensao alta e depois selecionar as caractersticas mais relevantes, aquelas que contem informacao discriminativa maxima sobre as classes.
O F-score (Chen and Lin, 2006) e uma metrica simples, porem efetiva na avaliacao do poder discriminativo das variaveis do conjunto de

(1)

N k

(2)

  1 2    k mk

h2

bk
Espaço de
Entrada

onde wj e o vetor de pesos das conexoes entre
as m entradas e o j-esimo neuronio escondido, j
e o vetor de pesos das conexoes entre o j-esimo
neuronio escondido e os d neuronios da sada da
rede, e bj e o vies do j-esimo neuronio escondido.
As funcoes de ativacao sao gerais e podem representar funcoes de ativacao arbitrarias, como por
exemplo g(wj xi + bj ). O modelo definido em (1)
e equivalente a H  Y , onde


g(w1 x1 + b1 )    g(wk x1 + bk )
 g(w1 x2 + b1 )    g(wk x2 + bk ) 


H

..
..


.

.
g(wk xN + bk )

b1

xm

j1

g(w1 xN + b1 )   

h1

(3)
(4)

1286

Anais do XX Congresso Brasileiro de Automática
Belo Horizonte, MG, 20 a 24 de Setembro de 2014

caractersticas. Em (Gao et al., 2013) os autores
aplicam F-score no contexto de SHFLNs com algoritmo de aprendizado ELM, porem para fazer
uma selecao previa das variaveis de entrada da
rede. No presente trabalho esta selecao sera efetuada na sada da camada escondida, de forma a
eliminar os neuronios cujas sadas estiverem associadas a um F-score abaixo de um limiar previamente estabelecido.

T
Dado a sada hj  h1j , . . . , hn+ j , . . . , hN j
do j-esimo neuronio escondido, com numero de
instancias pertencentes a classe positiva igual a
n+ e numero total de instancias igual a N , o Fscore correspondente e definido por
(+)

Fj 

(j

()

 j )2 + (j
j2

(+)

+ j2

 j )2

()

A abordagem proposta define a estrutura da
SLFN com base apenas em informacao a priori,
usando somente os dados de treinamento. Nenhum ajuste previo de parametros e necessario.
Alem disso, nao sao exigidos dados de validacao e
procedimentos computacionalmente intensivos de
validacao-cruzada para ajustar a dimensao da camada escondida. O calculo dos F-scores e simples
e rapido, e a poda e executada em um unico passo.
4

Com o objetivo de avaliar a performance preditiva do algoritmo FSP-ELM proposto, foram realizados experimentos com 10 bases de dados de
classificacao binaria, extradas do repositorio UCI
(Bache and Lichman, 2013). A Tabela 1 sumariza
os detalhes de cada uma das bases. Todas foram
pre-processadas da mesma forma. Primeiramente,
os padroes disponveis foram permutados 30 vezes
aleatoriamente e sem reposicao. Em seguida, para
cada permutacao, 70 das instancias foram separadas para treinamento e 30 para teste. Todas
as variaveis foram normalizadas para media nula
e desvio-padrao unitario.
Em todos os experimentos utilizou-se uma
quantidade inicial de neuronios escondidos k 
100, 200. Os parametros da camada escondida
foram amostrados de uma distribuicao uniforme
U 0, 5 0, 5. Funcoes de ativacao sigmoidais do
tipo tangente_hiperbolica foram adotadas para todos os neuronios.
O algoritmo FSP-ELM foi comparado a um
ELM baseado em validacao-cruzada, o CrossValidation Based ELM (CV-ELM). Na formulacao do algoritmo CV-ELM, o numero de nos escondidos e variado de 10 a k com passo 1 e a quantidade adequada e definida via validacao-cruzada
leave-one-out e estatstica PRESS (Allen, 1974).
Os algoritmos P-ELM (Rong et al., 2008), que
seleciona os neuronios mais correlacionados com
o vetor de rotulos de resposta segundo a estatstica Qui-Quadrado e que considera a quantidade de nos que minimiza o ndice AIC, e OPELM (Miche et al., 2010), que ranqueia os neuronios utilizando o algoritmo least angle regression
(Efron et al., 2004) e escolhe a quantidade que
minimiza o ndice PRESS, tambem foram comparados ao FSP-ELM. Adicionalmente, foram considerados os algoritmos estado-da-arte SVM linear
com kernel vanilla (Vanilla Linear SVM  VANSVM), SVM nao-linear com kernel rbf (Radial
Basis Function SVM  RBF-SVM), o Real AdaBoost (RE-ADA) (Friedman et al., 2000) e o kNearest Neighbours (KNN). Todos os algoritmos
foram implementados utilizando-se o software estatstico R. Para os algoritmos VAN-SVM e RBFSVM foi aplicado o pacote kernlab (Karatzoglou
et al., 2004). Para implementacao do algoritmo
RE-ADA foi utilizado o pacote ada (Culp et al.,

(5)

n+

(+)
j2

()
j2

X
1
(+)

(hij  j )2
n+  1 i1

N
X
1
()
(hij  j )2

N  n+  1 in +1

(6)

(7)

+

(+)

Resultados Experimentais

()

onde j , j e j sao as medias das instancias totais, positivas e negativas, respectivamente,
e hij e a i-esima instancia da sada do j-esimo
neuronio escondido.
O metodo proposto consiste entao em computar a matriz H e depois calcular o F-score para
cada uma de suas colunas. Com base na estrategia adotada em (Gunes et al., 2010), as variaveis hj com F-score inferior a media de todos
os F-scores calculados serao removidas da matriz
H. Isso e operacionalmente equivalente a zerar
o peso j associado ao neuronio escondido correspondente. Dado um conjunto de treinamento
T  xi , yi xi  Rm , yi  R, i  1, . . . , N , uma
funcao de ativacao g(), e uma quantidade inicial
k de neuronios escondidos, o algoritmo FSP-ELM
envolve a execucao dos seguintes passos
1. Atribuir valores aleatorios para wj e bj , para
j  1,    , k
2. Computar a matriz H da sada da camada
escondida
3. Computar os F-scores Fj , para j  1,    , k
4. Eliminar as colunas de H com
PkF-score menor
que o F-score medio F  k1 j1 Fj 
5. Estimar os pesos da sada b  H + Y considerando somente as colunas remanescente de
H.

1287

Anais do XX Congresso Brasileiro de Automática
Belo Horizonte, MG, 20 a 24 de Setembro de 2014

Tabela 1 Bases de dados de classificacao
Base de Dados
Abreviatura
Liver Disorders
LIV
Breast Cancer Wisconsin
CAN
Australian Credit
AUS
German Credit
GER
Pima Indians Diabetes
DIA
Heart
HRT
Ionosphere
ION
Parkinson
PKS
Sonar
SON
Spam
SPA

Treinamento Teste
241
104
398
171
390
168
700
300
538
230
189
81
245
106
137
58
146
62
3221
1380

2012). Ja para o algoritmo KNN, foi utilizado o
pacote class (Venables and Ripley, 2002). Para o
algoritmo RE-ADA foram definidas 150 epocas de
treinamento, levando-se em consideracao que tal
algoritmo nao esta sujeito ao problema de overfitting, mesmo para um numero alto de epocas
de treinamento (Friedman et al., 2000). O hiperparametro de margem das SVMs, assim como o
numero de vizinhos mais proximos do algoritmo
KNN, foram escolhidos via validacao-cruzada 10fold. Tais valores foram selecionados dentro dos
intervalos 2i , com i variando de -5 a 5 com passo
1, e 1, 15 com passo 1, respectivamente. Por sua
vez, o hiper-parametro  do kernel RBF gaussiano foi definido com base na heurstica proposta
em (Caputo et al., 2002).
Com o intuito de avaliar se a definicao automatica do limiar de selecao de neuronios escondidos pelo algoritmo FSP-ELM evita ou nao o
overfitting, foi considerado ainda nos experimentos um FSP-ELM com validacao-cruzada, o CrossValidation F-Score Based Pruned ELM (CVFSPELM). Basicamente, as medidas de F-score associadas aos neuronios escondidos sao normalizadas
no intervalo 0, 1, subtraindo-se de cada valor a
medida mnima encontrada e dividindo-se o resultado pelo range das medidas (medida maxima
menos medida mnima). O limiar e entao variado de 0,1 a 0,9 com passo 0,1 e o melhor valor e
escolhido via validacao-cruzada 10-fold.
A Tabela 2 exibe os resultados medios de performance, com o respectivo desvio-padrao entre
parenteses, das 30 repeticoes de cada algoritmo.
Alem da taxa de acuracia, os classificadores foram avaliados tambem em termos da area abaixo
da curva ROC (Fawcett, 2006), a qual mede a habilidade em classificar corretamente os indivduos
de ambas as classes do problema.
Os tempos medios de execucao dos algoritmos, medidos em segundos, se encontram listados
na Tabela 3. As simulacoes foram executadas em
um Core i3-2350M, 2.30GHz com 4-GB de RAM.
Fica evidente a superioridade da eficiencia com-

 de Variaveis
6
30
14
24
8
13
33
22
60
57

putacional do algoritmo FSP-ELM proposto.
A Tabela 4 mostra a quantidade media e os
respectivos desvios-padrao de neuronios intermediarios retidos na SLFN apos a poda pelos algoritmos P-ELM, OP-ELM, FSP-ELM e CVFSPELM, assim como a quantidade media definida
pelo algoritmo CV-ELM.
4.1

Analise Estatstica dos Resultados

Para estabelecer uma comparacao estatstica entre
os classificadores avaliados, estes foram tomados
como nveis de um unico fator, algoritmo. Alem
disso, cada problema de classificacao foi considerado como um fator de bloco. Em seguida foi
aplicada a analise de variancia (ANOVA) sobre
os resultados das 30 repeticoes realizadas em cada
grupo (um grupo corresponde a combinacao de um
nvel do fator algoritmo com um fator de bloco,
por exemplo, FSP-ELM sobre a base de dados heart). Ao todo sao 90 grupos (9 algoritmos e 10
problemas) com uma amostra de tamanho n  30
para cada um. O objetivo da analise e verificar se
o fator algoritmo afeta significativamente a AUC
media de classificacao, isto e, checar se os algoritmos apresentaram desempenho medio significativamente diferente em termos de AUC.
O tamanho amostral n  30 permite, para
um nvel de significancia   0, 05 e uma potencia 1    0, 80, detectar efeitos de tamanho f  0, 121 (Cohen, 1988). Isso corresponde a
rejeitar a hipotese de igualdade entre os classificadores para diferencas na AUC media a partir de
0, 006. Tal valor pode ser considerado pequeno,
ja que para diferencas na media menores que 0, 01
seria razoavel decretar equivalencia entre os classificadores. Considerando, no entanto, todas as
30 observacoes, os resultados da ANOVA forneceram, para um nvel de significancia   0, 05,
evidencia suficiente para rejeitar a hipotese nula
de igualdade dos efeitos dos tratamentos (p-valor
< 21016 ).
No entanto, a analise de variancia nao iden-

1288

Anais do XX Congresso Brasileiro de Automática
Belo Horizonte, MG, 20 a 24 de Setembro de 2014

Tabela 2 Resultados da classificacao para o conjunto de dados de teste
VAN-SVM RBF-SVM RE-ADA
LIV
CAN
AUS
GER
DIA
HRT
ION
PKS
SON
SPA

68.9(4.9)
97.3(1.4)
82.7(2.2)
76.4(2.4)
76.8(2.6)
84.0(3.2)
86.6(2.8)
87.0(3.2)
77.4(4.9)
93.0(0.7)

68.3(4.4)
97.5(1.1)
84.0(2.4)
75.7(2.2)
77.0(2.7)
83.3(3.8)
94.4(2.5)
89.7(3.7)
87.2(5.7)
93.3(0.7)

71.9(3.6)
96.5(1.6)
84.7(1.7)
76.5(2.1)
76.1(2.4)
81.6(2.9)
93.3(2.3)
90.3(2.9)
84.6(3.9)
95.4(0.6)

VAN-SVM RBF-SVM RE-ADA
LIV
CAN
AUS
GER
DIA
HRT
ION
PKS
SON
SPA

0.67(0.05)
0.97(0.02)
0.83(0.02)
0.68(0.03)
0.71(0.03)
0.84(0.03)
0.83(0.03)
0.77(0.06)
0.77(0.05)
0.92(0.01)

0.67(0.04)
0.97(0.01)
0.84(0.03)
0.67(0.03)
0.72(0.03)
0.83(0.04)
0.93(0.03)
0.85(0.08)
0.87(0.06)
0.93(0.01)

0.70(0.04)
0.96(0.02)
0.84(0.02)
0.67(0.03)
0.72(0.03)
0.81(0.03)
0.92(0.03)
0.84(0.05)
0.84(0.04)
0.95(0.01)

KNN

Acuracia ()
CV-ELM P-ELM

62.1(4.8)
96.6(1.4)
83.4(2.5)
72.2(2.4)
74.7(2.2)
82.8(3.6)
86.1(3.4)
92.2(4.6)
85.2(5.0)
90.2(0.8)

68.7(6.0)
96.6(1.6)
82.9(3.1)
72.8(2.8)
75.5(2.6)
81.6(4.4)
87.3(3.0)
80.6(5.3)
72.2(7.4)
92.2(0.7)

62.7(5.5)
96.2(1.5)
83.6(2.6)
73.6(3.1)
75.8(2.8)
82.0(3.6)
85.2(5.0)
79.0(6.4)
71.2(5.3)
92.1(0.8)

KNN

AUC
CV-ELM

P-ELM

0.60(0.05)
0.96(0.02)
0.82(0.03)
0.59(0.02)
0.70(0.03)
0.82(0.04)
0.82(0.05)
0.91(0.06)
0.85(0.05)
0.90(0.01)

0.68(0.06)
0.97(0.02)
0.83(0.03)
0.69(0.03)
0.73(0.03)
0.81(0.04)
0.85(0.04)
0.79(0.07)
0.72(0.07)
0.92(0.01)

0.61(0.06)
0.96(0.01)
0.83(0.03)
0.69(0.03)
0.73(0.03)
0.82(0.04)
0.83(0.05)
0.77(0.09)
0.71(0.05)
0.92(0.01)

OP-ELM FSP-ELM CVFSP-ELM
68.4(5.4)
96.8(1.3)
83.4(2.7)
74.0(2.9)
75.1(2.9)
80.8(4.4)
87.8(4.2)
84.0(4.0)
75.4(4.8)
92.4(0.8)

67.9(5.3)
96.3(1.5)
83.9(2.9)
73.5(2.3)
75.1(3.4)
81.3(4.9)
87.1(3.2)
81.2(5.6)
75.6(5.6)
91.4(0.9)

68.1(5.5)
96.1(1.8)
83.8(3.3)
73.7(2.4)
75.5(3.0)
82.2(4.0)
86.0(3.4)
81.1(4.9)
72.7(6.1)
91.5(0.7)

OP-ELM FSP-ELM CVFSP-ELM
0.68(0.05)
0.96(0.02)
0.84(0.03)
0.70(0.03)
0.72(0.03)
0.81(0.04)
0.85(0.05)
0.84(0.05)
0.75(0.05)
0.92(0.01)

0.67(0.05)
0.96(0.02)
0.84(0.03)
0.70(0.03)
0.72(0.04)
0.81(0.05)
0.85(0.04)
0.80(0.07)
0.76(0.06)
0.91(0.01)

0.67(0.06)
0.96(0.02)
0.84(0.03)
0.70(0.03)
0.73(0.03)
0.82(0.04)
0.84(0.04)
0.81(0.05)
0.73(0.06)
0.91(0.01)

Tabela 3 Tempo medio, em segundos, de treinamento dos algoritmos
LIV
CAN
AUS
GER
DIA
HRT
ION
PKS
SON
SPA

VAN-SVM RBF-SVM RE-ADA KNN

CV-ELM

P-ELM

OP-ELM

FSP-ELM CVFSP-ELM

1.79(0.38)
2.38(0.13)
5.04(2.05)
26.21(1.48)
5.16(0.18)
2.12(0.28)
3.29(0.45)
1.54(0.14)
2.59(0.15)
360.5(32.8)

2.19(0.49)
2.98(0.04)
3.37(0.19)
6.78(0.24)
4.22(0.05)
1.77(0.05)
1.91(0.05)
1.43(0.08)
1.37(0.05)
380.2(36.9)

0.57(0.17)
0.61(0.01)
0.63(0.04)
0.68(0.04)
0.64(0.03)
0.52(0.03)
0.54(0.02)
0.51(0.04)
0.48(0.05)
5.4(1.4)

1.43(0.31)
2.01(0.04)
2.20(0.19)
4.97(0.14)
3.05(0.06)
1.04(0.03)
1.14(0.01)
0.88(0.04)
0.79(0.05)
349.5(38.0)

0.06(0.03)
0.07(0.02)
0.07(0.02)
0.10(0.03)
0.07(0.01)
0.05(0.01)
0.05(0.01)
0.04(0.01)
0.05(0.02)
1.0(0.1)

2.34(0.33)
4.13(0.09)
4.37(0.25)
16.45(0.29)
6.98(0.14)
1.76(0.08)
2.84(0.05)
1.44(0.07)
2.90(0.12)
285.8(18.4)

2.93(0.67)
6.38(0.08)
4.37(0.17)
8.01(0.10)
4.30(0.04)
3.12(0.11)
5.55(0.05)
3.67(0.09)
7.15(0.08)
60.9(3.9)

0.18(0.06)
0.34(0.02)
0.24(0.01)
0.64(0.04)
0.25(0.01)
0.17(0.01)
0.25(0.03)
0.18(0.03)
0.21(0.02)
19.6(2.9)

Tabela 4 Quantidade de neuronios retidos posteriormente a poda
LIV
CAN
AUS
GER
DIA
HRT
ION
PKS
SON
SPA

k

CV-ELM

P-ELM

OP-ELM

FSP-ELM CVFSP-ELM

100
100
100
100
100
100
100
100
100
200

17.0(5.5)
61.5(17.7)
29.6(9.4)
38.1(14.7)
22.4(10.9)
19.0(6.3)
51.5(17.4)
40.2(17.2)
31.5(9.2)
185.2(9.1)

3.1(2.7)
28.6(21.9)
15.1(10.1)
15.4(5.0)
13.3(10.2)
14.0(11.9)
31.1(14.9)
19.3(15.2)
7.1(4.6)
137.3(39.6)

21.8(8.5)
47.2(8.0)
29.6(13.5)
25.6(10.7)
27.1(8.4)
23.2(17.3)
46.4(9.6)
45.0(12.4)
29.5(16.7)
125.9(12.2)

31.7(2.4)
34.3(2.5)
34.1(2.0)
32.3(2.6)
37.1(3.3)
35.1(2.9)
35.8(2.8)
41.3(2.8)
32.9(2.4)
65.0(4.4)

1289

19.0(11.2)
30.3(14.5)
26.0(13.6)
28.0(12.2)
12.1(8.9)
12.0(10.9)
32.1(17.5)
49.9(15.9)
12.1(11.5)
71.9(12.3)

0.18(0.06)
0.22(0.04)
0.26(0.05)
0.34(0.05)
0.34(0.05)
0.18(0.03)
0.18(0.03)
0.23(0.05)
0.13(0.03)
3.2(0.6)

Anais do XX Congresso Brasileiro de Automática
Belo Horizonte, MG, 20 a 24 de Setembro de 2014

Intervalos Dunnett

tifica exatamente quais medias sao diferentes.
Dessa forma, foi aplicada, adicionalmente, uma
analise de comparacoes multiplas post-hoc, o teste
de Dunnet (Hsu, 1996). O teste compara as medias dos tratamentos com um grupo de controle.
Como o interesse principal e identificar quais algoritmos apresentaram desempenho melhor, pior
ou equivalente ao algoritmo proposto, o FSP-ELM
foi definido como o grupo de controle. A Figura
2 exibe os intervalos de 95 de confianca do teste
de Dunnett relativos as diferencas na media da
AUC entre o FSP-ELM e os demais algoritmos.
Basicamente, intervalos que englobam o zero indicam ausencia de evidencia suficiente para concluir
diferenca entre os algoritmos, e intervalos estritamente positivos ou estritamente negativos indicam, respectivamente, desempenho medio superior ou inferior ao FSP-ELM, para um nvel de
significancia   0, 05.
OP.ELM  FSP.ELM
P.ELM  FSP.ELM













CVFSP.ELM  FSP.ELM







CV.ELM  FSP.ELM











KNN  FSP.ELM





RE.ADA  FSP.ELM


RBF.SVM  FSP.ELM
VAN.SVM  FSP.ELM

nar a estrutura que maximiza a performance preditiva da classificacao. No entanto, como confirmado pelos resultados, por meio da escolha dos
neuronios mais discriminativos, o algoritmo FSPELM confere uma performance de classificacao tao
alta quanto os algoritmos CV-ELM e OP-ELM, e
superior a do P-ELM.
Portanto, e valido concluir que, alem de uma
solucao eficiente computacionalmente, a abordagem proposta e uma alternativa viavel para realizar a poda de SLFNs com algoritmo de aprendizado ELM. O FSP-ELM prove a poda em um
unico passo, usando somente o conjunto de dados de treinamento. Dados de validacao e ajuste
previo dos parametros da rede nao sao necessarios para especificar a dimensao da camada escondida. Tal particularidade e confirmada pela falta
de evidencia suficiente nos resultados de classificacao para rejeitar a equivalencia com o algoritmo
CVFSP-ELM, que utiliza validacao-cruzada para
definir a estrutura da SLFN.
Trabalhos futuros estao focados na generalizacao do algoritmo para problemas de multiplas
classes e na comparacao do mesmo com outras metodologias de poda em maquinas_de_aprendizado
extremo.



0.02











0.00

0.02

Agradecimentos

0.04

Diferenças na AUC

O presente trabalho foi realizado com o apoio financeiro da CAPES - Brasil.

Figura 2 Intervalos de 95 de confianca da analise post-hoc de Dunnett para as diferencas de performance media entre o metodo proposto e os demais avaliados.

Referencias
Allen, D. M. (1974). The relationship between variable selection and data agumentation and a
method for prediction, Technometrics 16(1).

A partir dos intervalos de confianca da Figura
2 conclui-se que nao houve evidencia suficiente nos
resultados para rejeitar a equivalencia entre FSPELM e OP-ELM (p-valor 0, 377). Alem disso,
ha evidencia estatstica de superioridade de desempenho do algoritmo proposto em relacao ao
P-ELM (p-valor < 0, 001). Por fim, nao ha evidencias de overfitting por parte do algoritmo FSPELM, ja que sua performance foi estatisticamente
equivalente a do algoritmo CVFSP-ELM (p-valor
0, 975). Dessa forma, confirma-se a viabilidade
da metodologia adotada para definicao automatica do limiar das medidas de F-score.
5

Bache, K. and Lichman, M. (2013). UCI machine
learning repository.
Bishop, C. M. (1995). Neural Networks for Pattern Recognition, Oxford University Press,
Inc., New York, NY, USA.
Caputo, B., Sim, K., Furesjo, F. and Smola, A.
(2002). Appearance-based object recognition
using svms Which kernel should i use?, Proc
of NIPS workshop on Statistical methods for
computational experiments in visual processing and computer_vision, Whistler 2002.

Conclusoes e Trabalhos Futuros

Chen, Y.-W. and Lin, C.-J. (2006). Combining
svms with various feature selection strategies, in I. Guyon, M. Nikravesh, S. Gunn and
L. Zadeh (eds), Feature Extraction, Vol. 207
of Studies in Fuzziness and Soft Computing,
Springer Berlin Heidelberg, pp. 315324.

A ausencia de evidencia para rejeitar a equivalencia de desempenho entre o algoritmo FSP-ELM
proposto e os algoritmos CV-ELM e OP-ELM, somada a evidencia de superioridade de desempenho
do primeiro em relacao ao algoritmo P-ELM, confirmaram a eficacia da abordagem adotada para
selecao da estrutura da SLFN. O metodo proposto, diferentemente das demais abordagens, nao
utiliza informacao da sada da rede para selecio-

Cohen, J. (1988). Statistical Power Analysis for
the Behavioral Sciences, L. Erlbaum Associates.

1290

Anais do XX Congresso Brasileiro de Automática
Belo Horizonte, MG, 20 a 24 de Setembro de 2014

Cristianini, N. and Shawe-Taylor, J. (2000). An
Introduction to Support Vector Machines and
Other Kernel-based Learning Methods, Cambridge University Press.

Huang, G.-B., Zhu, Q.-Y. and Siew, C.-K. (2006).
Extreme learning machine theory and applications, Neurocomputing 70(1) 489501.
Karatzoglou, A., Smola, A., Hornik, K. and Zeileis, A. (2004). kernlab  an S4 package for
kernel methods in R, Journal of Statistical
Software 11(9) 120.

Culp, M., Johnson, K. and Michailidis, G. (2012).
ada an R package for stochastic boosting. R
package version 2.0-3.

Lan, Y., Soh, Y. C. and Huang, G.-B. (2010).
Constructive hidden nodes selection of extreme learning machine for regression, Neurocomput. 73(16-18).

Deng, W., Zheng, Q. and Chen, L. (2009). Regularized extreme learning machine, Computational Intelligence and Data Mining, 2009.
CIDM 09. IEEE Symposium on, pp. 389
395.

Li, G. and Niu, P. (2013). An enhanced extreme
learning machine based on ridge regression
for regression, Neural Computing and Applications 22(3-4) 803810.

Efron, B., Hastie, T., Johnstone, I. and Tibshirani, R. (2004). Least angle regression, The
Annals of Statistics 32(2) 407499.

Mao, J. and Jain, A. (1995). Artificial neural
networks for feature extraction and multivariate data projection, Neural Networks, IEEE
Transactions on 6(2) 296317.

Fawcett, T. (2006). An introduction to roc analysis, Pattern Recognition Letters 27(8) 861 
874.

Miche, Y., Sorjamaa, A., Bas, P., Simula, O., Jutten, C. and Lendasse, A. (2010). Op-elm
optimally pruned extreme learning machine,
Trans. Neur. Netw. 21(1) 158162.

Feng, G., Huang, G.-B., Lin, Q. and Gay, R.
(2009). Error minimized extreme learning
machine with growth of hidden nodes and
incremental learning, Trans. Neur. Netw.
20(8) 13521357.

Miche, Y., van Heeswijk, M., Bas, P., Simula,
O. and Lendasse, A. (2011). Trop-elm
A double-regularized ELM using LARS
and tikhonov regularization, Neurocomputing
74(16) 24132421.

Friedman, J., Hastie, T. and Tibshirani, R.
(2000). Additive Logistic Regression a Statistical View of Boosting, The Annals of Statistics 38.

Rong, H.-J., Ong, Y.-S., Tan, A.-H. and Zhu, Z.
(2008). A fast pruned-extreme learning machine for classification problem., Neurocomputing 72 359366.

Gao, J., Wang, Z., Yang, Y., Zhang, W., Tao,
C., Guan, J. and Rao, N. (2013). A novel
approach for lie detection based on f-score
and extreme learning machine, PLoS ONE
8(6).

Schadt, E. E., Linderman, M. D., Sorenson, J.,
Lee, L. and Nolan, G. P. (2010). Computational solutions to large-scale data management
and analysis, Nat Rev Genet 11(9) 647657.

Gunes, S., Polat, K. and Yosunkaya, S. (2010).
Multi-class f-score feature selection approach
to classification of obstructive sleep apnea
syndrome, Expert Systems with Applications
37(2) 9981004.

Simila, T. and Tikka, J. (2005). Multiresponse
sparse regression with application to multidimensional scaling, Proceedings of the 15th
international conference on Artificial neural
networks formal models and their applications - Volume Part II, ICANN05, SpringerVerlag, Berlin, Heidelberg, pp. 97102.

Hsu, J. (1996). Multiple Comparisons Theory
and Methods, Taylor  Francis.
Huang, G.-B., Chen, L. and Siew, C.-K. (2006).
Universal approximation using incremental
constructive feedforward networks with random hidden nodes, Trans. Neur. Netw.
17(4).

Vapnik, V. N. (1995). The nature of statistical
learning theory, Springer-Verlag New York,
Inc., New York, NY, USA.
Venables, W. N. and Ripley, B. D. (2002). Modern
Applied Statistics with S, fourth edn, Springer, New York.

Huang, G.-B., Wang, D. H. and Lan, Y. (2011).
Extreme learning machines a survey, 2 107
122.

1291