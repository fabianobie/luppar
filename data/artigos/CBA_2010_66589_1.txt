XVIII Congresso Brasileiro de Automática  12 a 16 Setembro 2010, Bonito-MS.

UMA APROXIMAÇÃO GENÉTICA PARA MÁQUINAS DE VETOR DE SUPORTE EM PROBLEMAS
DE CLASSIFICAÇÃO
CARLOS A. DE A. PADILHA, NAIYAN H. C. LIMA, ADRIÃO D. D. NETO, JORGE D. DE MELO
Laboratório de Sistemas Inteligentes Depto. de Engenharia de Computação e Automação, Universidade de
Federal do Rio Grande do Norte
Caixa Postal 1524, 59072-970 Natal, RN, BRASIL
E-mails carlosalberto@dca.ufrn.br, naiyan@dca.ufrn.br, adriao@dca.ufrn.br,
jdmelo@dca.ufrn.br
AbstractAmong the various approaches to solve pattern classification problems, the Support Vector Machines (SVM) receive
great emphasis, due to its ease of use and good generalization performance. Trying to improve the SVM performance, several
tools have been developed, like the employment of committee machines. Observing that these methods usually operate in only
one level in the problem, we propose in this work one further step, applying not only committee machines, but also genetic algorithms, in two levels both in SVM kernel optimization and in SVM classification optimization.
Keywords Pattern classification, Support Vector Machine, Optimization problems, Genetic algorithm.
Resumo Dentre as várias abordagens para a solução de problemas de classificação_de_padrões, as Máquinas de Vetor de Suporte (SVM) recebem grande destaque, devido  sua facilidade de utilização e bom desempenho de generalização. Na tentativa
de melhorar o desempenho das SVM, diversas ferramentas foram desenvolvidas, como o emprego de máquinas de comitê. Observando que esses métodos costumam operar em apenas um nível dentro do problema, nós propomos neste trabalho um passo
além, aplicando, além das máquinas de comitê, mas também algoritmos_genéticos, em dois níveis a otimização do kernel das
SVM quanto na otimização da classificação obtida com essas SVM.
Palavras-chave .

1

somo. Um cromossomo é constituído por um conjunto de valores numéricos ou por uma sequência de
bits. A otimização é feita a partir de uma função
matemática chamada função de fitness, que mede a
capacidade de sobrevivência daquele indivíduo.
Então a população é sujeita a diversos níveis de mutação, seleção e cruzamento, simulando o princípio
da sobrevivência do mais apto, até que um certo
nível da função fitness seja atingido, ou até que uma
quantidade limite de gerações tenha transcorrido.
Então o cromossomo mais apto é a resposta do AG
para o problema.
Nosso trabalho propõe a união de AG e conjuntos de SVM para a obtenção de um classificador mais
forte. Os AG atuam em dois níveis, tanto na determinação da melhor composição do kernel de cada SVM
do conjunto quanto ao fim do treinamento para ponderar a influência de cada SVM. No primeiro nível, o
cromossomo é a largura da gaussiana do kernel RBF.
No segundo nível, Um AG é usado para analisar a
importância de cada SVM no conjunto, por meio de
um vetor de pesos, que é o cromossomo do AG. A
classificação final é obtida por uma simples combinação_linear. Em seguida, comparamos nossos resultados para uma SVM individual e para ensembles de
SVM.
Este artigo está organizado da seguinte forma
Seção 2 apresenta as SVM e algumas de suas características, e também tem uma breve explicação sobre
algoritmos_genéticos e seus mecanismos. Seção 3
descreve o método proposto neste trabalho, enquanto
que a Seção 4 tem os resultados experimentais e suas
análises. O artigo se encerra com as considerações
finais da seção 5.

Introdução

Problemas de classificação_de_padrões são muito
comuns dentro da área de sistemas_inteligentes. Uma
grande quantidade de ferramentas é capaz de lidar
com esse tipo de problema, todos envolvendo alguma
forma de otimização. Na otimização, há outra classe
de algoritmos, os algoritmos_evolutivos, que é comumente aplicada.
O estudo das Máquinas de Vetor de Suporte
(SVM) é uma das áreas de pesquisa de maior destaque em classificação_de_padrões. O treinamento das
SVM busca maximizar a margem de separação entre
as classes, encontrando um hiperplano ótimo. Em
problemas não-lineares é utilizado o método conhecido como função kernel, que fornece um mapeamento do espaço de entrada em um outro espaço,
sendo este de alta dimensionalidade, chamado espaço
de características. Várias técnicas foram empregadas
de modo a melhorar a classificação de SVM, quer
através do desenvolvimento de novos métodos de
treinamento em Osuna (1997) ou através da criação
de comitês de SVM em Lima (2009), mas pode-se
observar que as mesmas costumam atuar em apenas
um ponto específico do problema.
Algoritmos Evolutivos (AE) são métodos de otimização inspirados na evolução biológica. Utilizando eventos naturais como inspiração, são gerados
mecanismos de mutação e cruzamento, que fazem a
busca pela solução ótima. Algoritmos Genéticos
(AG) são uma classe de AE que emulam essa evolução a nível intracelular. Nessa metaheurística, o conjunto de soluções possíveis é chamado de população,
e cada solução do problema é chamada de cromos-

1970

XVIII Congresso Brasileiro de Automática  12 a 16 Setembro 2010, Bonito-MS.

2 Referencial Teórico
Onde
é o multiplicador de Lagrange correspondente  amostra e é o parâmetro de regularização, que fornece um limite na equação (3), garantindo um equilíbrio entre erro de treinamento e a
complexidade do modelo.

2.1 Máquinas de Vetor de Suporte
Em Vapnik (1998), foi proposta uma métrica diferente para o erro de treinamento no aprendizado
supervisionado. O princípio da Minimização do Risco Estrutural (SRM) estende a Minimização do Risco
Empírico (ERM) absorvendo outra medida de erro.
ERM, comumente usado em Redes Neurais Artificiais, lida com a minimização do erro dos dados de
treinamento, chamado erro empírico. SRM, usado em
SVM, incorporam a dimensão VC em Vapnik
(1971), uma medida da capacidade de um dado classificador para resolver um problema de classificação
binária, para calcular a taxa de erro de generalização,
chamado erro estrutural, que é limitada pela adição
da taxa de erro de treinamento e um valor que é dependente da dimensão VC. O equilíbrio das taxas de
erro evita o overfitting, a incapacidade de classificar
itens semelhantes como pertencentes  mesma classe,
garantindo boa generalização. A margem das classes
é definida por vetores de suporte, que são exemplos
marginais de uma dada classe nos dados de treinamento, exemplos de entrada mais próximos de outra
classe.

2.2 Algoritmo Genético
O Algoritmo Genético (AG) em Castro (2002) é
um método computacional de busca e otimização
inspirado no princípio Darwiniano da evolução das
espécies e na genética. O AG foi inicialmente proposto por John Holland em 1975, como parte de suas
tentativas em explicar processos ocorrendo em sistemas naturais e de construir sistemas artificiais baseados em tais processos. Opera sobre um conjunto
de soluções representado por cromossomos e seguem
a idéia de sobrevivência dos mais aptos que são as
melhores soluções a cada geração.
Conceitualmente, adota dois espaços separados
bem definidos chamados de espaço de busca e espaço de solução. O primeiro é o espaço formado pelas
soluções codificadas (genótipo) para o problema e o
segundo, o espaço das soluções reais (fenótipo), que
serão avaliadas pela função de aptidão.
O algoritmo_genético canônico (simples AG)
pode ser visto como uma junção de três componentes (a) a representação do cromossomo, comumente
uma cadeia de bits ou um conjunto de valores em
ponto_flutuante (b) a síntese de novos indivíduos,
usando operadores inspirados na biologia como o
crossover e a mutação (c) seleção dos pais da próxima geração, usando a função de aptidão, através de
uma roleta, torneio, ou outro método de seleção.
O crossover é um intercâmbio de informações
entre cromossomos pais, criando novos cromossomos. Em cadeias de bits, usualmente acontece através do intercâmbio de determinados índices de bits,
já em valores em ponto_flutuante, é uma operação
aritmética entre os cromossomos. Também ocorre
como certa probabilidade, e a escolha de quais indivíduos participarão no crossover podem também ser
feita de acordo com a função de aptidão.
A mutação é a troca no valor de um gene no
cromossomo. Como o crossover, também tem uma
probabilidade de ocorrência. A mutação pode ser
uma simples inversão de um gene na cadeia de bit,
ou uma pequena adição ou subtração do cromossomo, no caso do ponto_flutuante.
Na seleção por roleta, cada indivíduo da população é representado proporcionalmente ao seu grau de
aptidão. Logo, indivíduos com valores altos de aptidão ocupam grandes porções da roleta (maior probabilidade de ser um pai da próxima geração), enquanto
aqueles de menor aptidão ocupam uma porção relativamente menor da roleta (menor probabilidade de ser
um pai da próxima geração). Finalmente, a roleta é

A superfície de decisão gerada pela SVM é um
hiperplano, que pode ser descrito como

é o conjunto de treinamento,
Onde
é o vetor de pesos, b é o bias,
representa um
produtor escalar e
é o mapeamento não-linear.
Esse mapeamento não-linear não é necessariamente
conhecido desde que haja funções kernel que substituam o produto escalar linear na equação. Essas funções geram um espaço de características com alta
dimensionalidade, onde o hiperplano gerado pelo
SVM tem uma melhor chance de classificar corretamente. Ao escolher uma função de kernel, a forma da
superfície de decisão no espaço de entrada é conhecida, mas a específica função
é obtida pela expressão

Onde
é o kernel escolhido. Desta forma,
o problema de otimização do treinamento de SVM
pode ser representado pela minimização de

Sujeita a estas restrições

1971

XVIII Congresso Brasileiro de Automática  12 a 16 Setembro 2010, Bonito-MS.

ótima para melhorar a classificação geral. Utilizando
como fitness o erro de generalização das SVM, e
buscando minimizar esse fitness, temos que as SVM
mais precisas terão valores mais altos dentro da distribuição, que deve ser uma distribuição de probabilidade. O pseudocódigo do método é o seguinte

girada certo número de vezes, dependendo do tamanho da população, e aqueles sorteados na roleta serão
indivíduos na próxima geração.
A seleção por torneio usa sucessivas disputas para fazer a seleção. Esse método estabelece k disputas
para selecionar k indivíduos, cada disputa envolvendo n indivíduos selecionados aleatoriamente. O indivíduo com o maior valor de aptidão é selecionado na
disputa, e deve permanecer na população para a próxima geração.

Tabela 2. Método Proposto
Dado
, o conjunto de
entrada
o número de SVM no conjunto
Procedimento
Gerar de o conjunto de treinamento e o conjunto de teste
Chame M AGs para gerar os
Treine modelos de SVM fornecendo e
Avalie os modelos usando e obtenha a saída
Gere o vetor de pesos para o conjunto de SVM
Chame o AG para minimizar a função de erro

Tabela 1. Algoritmo Genético
Dado
, o número de iterações
Procedimento AG

InicializePopulação(P,t)
CalcularAptidão(P,t)
Repita até

SeleçãoDosPais(P,t)
Crossover(P,t)
Mutação(P,t)
CalcularAptidão(P,t)
Eliminação(P,t)
Fim
Saída Os indivíduos mais aptos da população

Recupere o vetor de pesos ótimo
Saída Classificação final

Geralmente, o AG usado em problemas reais,
Coelho (2003), é mais sofisticado que o canônico,
por incorporar operadores de reprodução e seleção
mais apropriados para as características e peculiaridades do problema-alvo.

4 Resultados Experimentais
Os testes foram executados com conjuntos de
dados do Machine Learning Repository (2007) da
Universidade da Califórnia, Irvine. Foram dois conjuntos utilizados Splice Junction Gene Sequences e
Waveform.
O Splice Junction Gene Sequences é um conjunto de dados de referência que é composto de um
conjunto de sequências genéticas. As junções de
splice são pontos na sequência genética em que DNA
supérfluo é removido durante o processo de criação
de proteínas. O problema em questão é reconhecer
dada uma sequência genética, os limites entre os
exons, que são as partes da sequência mantidas após
o splice e os introns, que são as porções eliminadas
pelo splice. O problema original possui três classes
região IE, região EI e nenhum dos dois.
O Waveform é outro conjunto de dados de referência gerados artificialmente. É dividido em três
classes de ondas, com maiores informações em Breiman (1984).
Em Benchmarks (2010) há outra versão desses
conjuntos de dados, processados para serem problemas de classificação binários, e divididos em 100
diferentes realizações de treinamento e teste.
Nossos testes ocorreram inicialmente determinando um valor arbitrário para o , próximos dos
valores em Benchmarks (2010) e incorporando o AG
apenas no segundo nível. Os resultados aí obtidos
foram publicados em Padilha et al. (2010). Posteriormente, agregamos a esta estrutura a determinação
via AG da largura da gaussiana, e comparamos as

3 Método Proposto
Nosso principal objetivo é aplicação de algoritmos_genéticos em diversos níveis para o aprimoramento da classificação de um conjunto de SVM. A
fim de criar esse conjunto de SVM, investigamos um
pouco dentro da teoria de ensemble.
Em Kucheva (2003) vemos que para um ensemble alcançar boa classificação, seus componentes
precisam ter algum grau de diversidade. De Lima
(2009) vemos que o kernel da SVM que permite uma
maior diversidade e um dos mais populares é o kernel da Função Base Radial (RBF), porque seu parâmetro de largura Gaussiana, , permite um ajuste
detalhado.
Aqui é onde ocorre a primeira atuação. Em cada
SVM do conjunto, é aplicado um AG para encontrar
um valor eficiente para . Devido  natureza da otimização do algoritmo_genético, valores diferentes de
são encontrados, garantindo a diversidade do conjunto.
O segundo ponto de atuação é através da definição de uma distribuição sobre o conjunto de SVM.
Esses valores são uma medida da influência de cada
classificador para a hipótese final, que é obtida através de uma combinação_linear das saídas das SVM
ponderadas pela distribuição. Essa distribuição é o
cromossomo do nosso segundo nível de AG.
A finalidade do AG é encontrar uma distribuição
1972

XVIII Congresso Brasileiro de Automática  12 a 16 Setembro 2010, Bonito-MS.

uma diminuição no erro de generalização. Os dois
testes, no entanto, conseguiram obter classificação
comparável  de uma SVM isolada. Já para o conjunto Waveform, verifica-se que a atuação do primeiro
AG atrapalha a classificação. Com isto, podemos
inferir que um refinamento maior na busca do valor
de
possa gerar resultados satisfatórios, em vista
que a tendência de queda com o aumento do conjunto
também pôde ser observada.

taxas de erro de generalização obtidas em ambos os
casos.
Em ambos os casos a população inicial do AG é
gerada aleatoriamente. Empregamos a seleção estocástica uniforme, que divide os pais em seções de
tamanho uniforme, o tamanho de cada um dos pais
sendo determinada pela função de escalonamento de
aptidão. Em cada uma dessas seções, um dos pais é
escolhido. A função de mutação é uma Gaussiana os
indivíduos na mutação são adicionados com um
número aleatório de uma distribuição Gaussiana com
média zero e variância decrescente em cada geração.
A função de crossover é o crossover disperso um
vetor binário é gerado, e os elementos desse vetor
decidem o resultado do crossover. Se o elemento é
um 1, o gene correspondente da criança virá do primeiro pai. Se é um 0, virá do segundo pai. O tamanho da população é 20, em cada geração dois indivíduos da elite são mantidos para a próxima geração, e
a fração gerada pelo crossover é 0.8. O AG funciona
por 100 gerações.
O primeiro gráfico contém a taxa de erro percentual obtida com o conjunto Splice. Foram testados
conjuntos contendo 5 a 10 SVM. O valor obtido para
uma SVM isolada foi de 13,01 de erro.

5 Conclusões
Nosso principal objetivo foi alcançado apresentamos que é possível melhor o desempenho de classificação da SVM através de um algoritmo_genético.
Isso é uma simples e bem conhecida metaheurística
que pode realmente ser aplicada para ajudar em problemas de classificação_de_padrões.
Embora os métodos ensemble sejam a maneira
mais aplicada na literatura, a maioria dos pesquisadores fora da comunidade de aprendizado de máquina
não tem muito conhecimento de ensembles, redes
modulares e, assim, é muito mais simples para que
seja aplicado um AG ou outra metaheurística similar,
e nós acreditamos que isso seja a maior contribuição
do nosso trabalho.
Agradecimentos
Gostaríamos de agradecer ao CNPq - Conselho Nacional de Desenvolvimento Científico e Tecnológico
 e ao programa REUNI - Reestruturação e Expansão
das Universidades Federais  pelo apoio financeiro e
, também, aos colegas de laboratório pelo convívio e
inspiratória dedicação aos trabalhos de pesquisa.

O segundo gráfico contêm a taxa de erro percentual obtida com o conjunto Waveform. Também
foram testados conjuntos contendo 5 a 10 SVM.
Neste caso, o valor obtido para uma SVM isolada foi
de 11,33 de erro.

Referências Bibliográficas
Asuncion, A. Newman, D.J. (2007). UCI Machine
Learning Repository. Irvine, CA University of
California, School of Information and Computer
Science.
Disponível
em
<httpwww.ics.uci.edumlearnMLRepository.
html>.
Breiman,L., Friedman,J.H. Olshen, R.A. Stone, C.J.
(1984).
Classification and Regression Trees. Wadsworth
International
Group Belmont, California. p. 43-49.
Castro, Leandro de Zuben, Fernando Von. (2002).
Algoritmos
Genéticos.
Disponível
em
<ftp.dca.fee.unicamp.brpubdocsvonzubenia70
702topico902.pdf>.
Coelho, André Luís Vasconcelos (2004). Evolução,
simbiose e hibridismo aplicados  engenharia de
sistemas_inteligentes modulares Investigações
em redes_neurais, comitês de máquinas e sistemas_multiagentes. Unicamp.
Kuncheva, Ludmila Whitaker, Christopher (2003).
Measures in diversity in classifier ensembles and

Como pudemos observar, para o conjunto Splice
houve uma leve melhora no desempenho, o que indica que os valores de encontrados pelo AG foram
melhores que os indicados arbitrariamente. Além
disso, é possível verificar que com o aumento da
quantidade de SVM componentes do conjunto, houve

1973

XVIII Congresso Brasileiro de Automática  12 a 16 Setembro 2010, Bonito-MS.

their relationship with ensemble accuracy. Machine Learning 51 (2), 181-207.
Lima, Naiyan Neto, Adrião Dória Melo, Jorge
(2009). Creating an Ensemble of Diverse Support Vector Machines Using Adaboost. Proceedings on International Joint Conference on Neural
Networks.
Osuna, Edgar Freund, Robert Girosi, Frederico
(1997). An Improved Training Algorithm for
Support Vector Machines. NNSP 97.
Vapnik, Vladimir Chervonenkis, Alexey (1971).
On the uniform convergence of relative frequencies of events to their probabilities. Theory
of Probability and its Applications.
Vapnik, Vladimir (1998). Statistical Learning
Theory. John Wiley and Sons Inc., New York.
Benchmarks.
Disponível
em
<httpida.first.fraunhofer.deprojectsbenchben
chmarks.htm>. Acesso em 10 Jan. 2010.

1974