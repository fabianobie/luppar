COMPARING GMM AND PARZEN IN AUTOMATIC SIGNATURE RECOGNITION - A STEP
BACKWARD OR FORWARD?
JUGURTA MONTALVÃO
Universidade Federal de Sergipe - UFS (Brazil), Campus de São Cristóvão - Sergipe, CEP. 49100-000 , Brazil
NESMA HOUMANI
Institut Télécom Télécom SudParis, 9 Rue Charles Fourier, 91011, Evry, Paris, France
BERNADETTE DORIZZI
Institut Télécom Télécom SudParis, 9 Rue Charles Fourier, 91011, Evry, Paris, France
E-mails jmontalvao@ufs.br,nesma.houmani@it-sudparis.eu,
bernadette.dorizzi@it-sudparis.eu
Abstract The use of Gaussian Mixture Models (GMM), adapted through the Expectation Minimization algorithm, is quite
widespread in automatic verification (Biometric) tasks. Its choice is, at a first glance, founded on the good qualities of GMM
models when aimed at approximating Probability Density Functions (PDF) of random variables. But biometric models for verification are frequently adapted from small sample sets of biometric signals, since in real applications subjects are not willing to
accept long enrollment sessions. This well known constraint raises a problem of balance between model complexity and sample
size. From this perspective, we show, through simple online signature verification experiments, that constrained GMM with fewer degrees of freedom, compared to GMM with full covariance matrices, provide better performances. Moreover, pushing this
argument even further, we also show that a Parzen model (seen here as a over-constrained GMM) can be even better than usual
GMM, in terms of Equal Error Ratio (EER).
Keywords GMM, Parzen, Degrees of freedom of probabilistic models.
Resumo O uso de Gaussian Mixture Models (GMM), adaptados através do algoritmo iterativo Expectation Minimization , é
comum em tarefas de verificação automática de indivíduos (Biometria). Sua escolha,  primeira vista, é bem fundamentada nas
boas características do GMM como ferramenta para modelar Funções Densidade de Probabilidade de variáveis aleatórias. Mas
os modelos em biometria, não raramente, são adaptadosaprendidos a partir de pequenos conjuntos de amostras, pois em aplicações reais, os indivíduos podem não aceitar longas sessões de cadastros. Esta restrição, bem conhecida em biometria, pode gerar
problemas de balaço entre complexidade de um modelo de probabilidade e a quantidade de dados disponíveis para o seu aprendizado. A partir desta perspectiva, através de experimentos simples de verificação pela assinatura online, este trabalho aponta
evidências de que modelos com menos garus de liberdade que os GMM com matrizes_de_covariância cheias, incluindo GMM regularizados, provêm melhores resultados. Mais ainda, também é mostrado que um simples modelo Parzen (visto aqui como
um GMM sobre-regularizado) pode ser melhor que os GMM usuais, em termos de Equal Error Ratio (EER).
Palavras-chave .

1

and Gaussian Mixture Models (GMM) (Richiardi et
al., 2003). This high degree of variability may induce, for instance, the study of writer quality (entropic) measures that allows for quantifying both the stability and the complexity of a writers genuine signatures (Garcia-Salicetti et al., 2009).
As it is common to most biometric systems, systems users are willing to provide, during enrollment,
just a few samples. For instance, nowadays, as established in SVC2004 (Yeung et al., 2004), typically,
only 5 reference signatures are used as enrollment
set. In this paper, we address a question usually taken
for granted, concerning Probability Density Function
(PDF) estimation in biometric applications are
GMM, usually adapted through Expectation-Maximization (EM) (Dempster et al., 1977), suitable for
PDF estimation from a few samples per subject?
On the other hand, Gaussian Mixture Models
provided by the Parzen method are intrinsically regu-

Introduction

Handwritten signature is a behavioural biometric
modality showing high variability from one instance
to another for the same writer (intra-class variability).
This high variability explains indeed that the best
verification approaches  as particularly reflected for
Online Signature in the results of the First International Signature Verification Competition SVC2004
(Yeung et al., 2004) and the Signature Evaluations
carried out in the framework of BioSecure Multimodal Evaluation Campaigns, BSEC2009 and
BMEC2007 (BioSecure, 2009)  are those tolerating
random local variations of the signature, as elastic
matching techniques (Dynamic Time Warping (Rabiner et al., 1993),(Jain et al., 2002) or statistical
models, as Hidden Markov Models (HMM) (Rabiner
et al., 1993),(Kashi et al., 1998),(Ortega-Garcia et
al., 2002),(Dolfing et al., 1998),(Ly-Van et al., 2007)

4463

straints or lower bounds on the covariance matrix of
Gaussian kernels in GMM).
(IV) Conexionist models (e.g. artificial neural
networks) can also be regularized, or partially regularized by pruning (Haykin et al., 1999), though it is
not always explicitly referred to as a regularization
procedure. This includes Radial Basis Functions
Neural Networks, which are structurally related to
GMM.
In (Ormoneit et al., 1996), for instance, two approaches to GMM regularization are compared one
based on averaging (category II), and the other based
on an explicit regularization term (category I). Both
provided improved models (if compared to the unconstrained one), with similar performances.
On the other hand, the nonparametric Parzen
method (Duda et al., 2001),(Webb, 2002) can loosely
be regarded as a mixture model based method with
strongly-constrained mixture components (category
III). The Parzen approach gives an instant PDF approximation (no iterations). In spite of its simplicity,
it is known that, under some constrains on the Parzen
window width parameter, the convergence of the estimated PDF with the actual one is guaranteed, when
the number of samples tends to infinity (Duda et al.,
2001),(Leiva-Murillo et al., 2006). In other words,
many small isotropic (radial basis) Gaussian kernels,
with identical dispersion, can virtually approximate
any PDF shape. This corresponds to a trade from
kernel complexity (elliptical kernels, for instance,
typically obtained via the EM approach) to kernel
number.
Although EM and Parzen approaches come from
different paradigms  namely, parametric and nonparametric PDF estimation, respectively  they share
a striking structural similarity, whenever the Parzen
method is based on Gaussian kernels. In both cases,
the actual PDF is approximated by a Mixture of
Gaussians. However, Gaussian Mixture Models
provided by the Parzen method are intrinsically regularized, for kernel centers cannot move (structural
regularization - category IV) and identical radial dispersions are imposed on all kernels (parametric regularization - category III).
In this paper, we propose a useful point of view
from which both kinds of PDF estimates  i.e. GMM
learned via EM and Parzen  are seen as Gaussian
Mixture Models (GMM), with different levels of regularization. More precisely, starting from GMM with
unconstrained covariance matrices (full covariance
matrices), we can obtain several levels of parametric
regularization, through the replacement of full covariance (Level 0) matrices with
Level 1 one diagonal covariance matrix for
each Gaussian in the Mixture
Level 2 one scalar covariance matrices for
each Gaussian in the Mixture
Level 3 the very same scalar covariance matrix
for all Gaussian in the Mixture

larized. Therefore, we address the model regularization issue by comparing the performances provided
by usual GMM and Parzen PDF models in signature
verification tasks, with samples from the well-known
signature database MCYT. From this comparison, we
gather preliminary evidences that conventional GMM
may not be suitable models in some Biometric scenarios, and that improvements can be obtained through
model regularization.
This paper is organized as follows in Section 2,
model regularization issues are detailed, whereas our
stochastic online signature models are presented in
Section 3. In Section 4, we present some experimental evidences which corroborate our theoretic claims.
The potential importance of these results is discussed
in Section 5.
2

PDF estimation and Model Regularization

PDF estimation from limited data sets is a classical problem in pattern recognition for which many
approximated solutions are presented even in classic
literature (Duda et al., 2001). But it is worth noting
that, though the EM is not the fastest algorithm for
GMM optimization (Ly-Van et al., 2007), it is usually simpler to apply, which can partially explain its
widespread popularity in many application fields.
However, in addition to its possibly poor convergence rate (depending on the data distribution and the
initial estimates of its parameters), it also presents the
following drawbacks (Webb, 2002)
(a) Its likelihood-based criterion presents a mul
titude of useless global maxima
(b) Convergence to parameter values associ
ated with singularities is more likely to
occur with small data sets, and when cen
ters are not well separated.
Indeed, it is well-known that likelihood is often
unrepresentative in high dimensional problems,
which can be true in some low-dimensional problems
as well (MacKay, 2003). In order to cope with these
drawbacks, model regularization is the usual solution,
where constraints on the Gaussian mixture composition increases generalization (Larsen, 1996).
Regularization strategies can be roughly split
into four categories, namely
(I) One general approach to regularization is
based on the addition of a regularization term to the
unconstrained criterion function, which expresses
constraints or desirable properties of solutions.
(II) For models obtained via clustering-like algorithms (including the EM, which can be loosely
seen as a soft clustering algorithm (Webb, 2002),
(Theodoridis et al., 1999)), a straightforward regularization approach is that of averaging estimates from
many independent initializations.
(III) For Mixture Models, regularization can be
easily obtained by imposing constraints on the mixture component parameters (e.g. by imposing con-

4464

var  xn var  y nvar p n1
where var . stands for empirical variance.
In order to formulate our problem from a probabilistic point of view, we consider each signature as
an instance of a 3D stochastic process,
S S n n1,2 ,,  , where a single instance of the n-th random variable S n ,
sn   xn  yn  p n  , n N p , can be re0, nN p
garded as a single point in the 3D trajectory, as illustrated in Figure 1, with two genuine signatures from
MCYT database.

i 
stances si , from the random process
S .
Therefore, after enrollment, we assume that a fixed
number N L of instances of each random process
is available. For example, if N L 5 then each subject provided 5 genuine signatures during enrollment.
From these N L signatures, we must be able to
learnadapt a model of the underlying random process. When GMM are deployed for this task, a
straightforward approach is to adjust one Gaussian
for each short segment of each S i  through time,
thus providing one mixture of Gaussians for each
whole random process.
However, defining which segments (temporal
windows) of S i to take is not a straightforward
matter. In this work, we cope with this difficult question by a very simple approach we just introduced a
new variable
t nK 12n1  N p1 , n 1,... , N p
to explicitly represent time, where K is a Real
constant. Note that t n is a random variable,
since N P , the length of each signature instance, is
itself a random variable. Thus, by setting
K 1.74 , we imposed a normalized variance to
t n  , whereas its mean is null by definition.
Accordingly, all probabilistic models can be adapted with 4D sets of points, given
by
stime n s n t n
or,
equivalently,
stime n xn y n p n t n . As an expected result, trajectory points from different instants of
time are clearly separated in (signal) space representation, thanks to this new space dimension introduced
by the variable t n . This very simple approach
to take into account the dynamic dimension of online
signatures contrasts with more elaborated models,
such as those based on Hidden Markov Models. Nonetheless, its simplicity highlights the experimental
evidences found in this work.
After model adaptation with sets of genuine signatures from each subject, one GMM per subject is
obtained. Figure 2 illustrates the placement of a
single Gaussian over a segment of time. Please note

Figure 1. Two instances of the same signature represented as two
trajectories (first signature from the MCYT data base). Pressure
variable was dropped to allow graphic representation

Figure 2. Illustration of the placement of a single Gaussian into a
conventional GMM, in reduced dimension (pressure dimension is
not represented).

Accordingly, we further assume that each (human) subject, i1,2 , , I , to which a personal
signature is associated, is a source of genuine in-

that, in this simplified illustration, only 2 trajectories
(i.e. signature instances) are represented, whereas this

On this third level of parametric model regularization (category III), we impose identical and isotropic Gaussian kernels throughout all the mixture. Structurally, we are very close to the Parzen model with
Gaussian kernels. In fact, the only remaining difference is that Gaussian centers cannot move during adaptationlearning of the Parzen model. Therefore,
Parzen models can be seen as the result of a fourth
level of GMM regularization, namely
Level 4 similar to Level 3, but Gaussian
centers are not allowed to move during model adaptationlearning (i.e. the Parzen model).
3 Online signature models
In this work, each online signature is originally
represented by 3 vectors of the same length, N P ,
corresponding to coordinates,  xn , yn , and
n 1,, N P ,
pressure, p n ,
regularly
sampled through a conventional tablet device. These
variables are further modified in order to ensure
centralization, rotation and normalization of each signature. More precisely, beyond the rotation of each
signature so that its principal axis lays in horizontal
direction, we also impose that
NP

 x n 
1

NP



 y n
1

NP



 p n

0

1

and





4465

N L 5 tranumber is actually N L (we use
jectories in each experiment presented in Section 4).
Once the GMM is available, simple likelihood
scores are compared to a given threshold in order to
test the authenticity of new incoming signatures. This
threshold can be further adaptednormalized to each
subject. Nonetheless, here we use the simplest approach, where a single threshold is used with all subjects. In Sec. 4, all values of Equal Error Rate
(EER)are reported with a single threshold, which optimizes the tradeoff between minimization of both
False Acceptation Rate (FAR) and False Reject Rate
(FRR), considering a set of labeled (genuineforgery)
signatures for testing.
By contrast, in the Parzen based approach proposed in this comparative work, we simply assume
that each trajectory point, from each enrollment signature, is to be taken as a Gaussian kernel center, so
that, from each genuine signature, a Parzen model of
the random process is almost instantly given by a
N P N L Gaussian kernels. Figure 3
mixture of
illustrates the placement of two Gaussian kernels of a
Parzen model, for a given (time) value t n .
Note that in Figure 3, the segment of time is purposely shorter than that in Figure 2, for it takes into
account just a single instant of the trajectory, i.e., a
single value of t n .

ering that Gaussian kernels were used, we can say
that the main difference is the number of Gaussians in
each model (much greater in Parzen models) and
the lack of freedom for center placement in Parzen
models. In other words, Parzen model is like a
frozen GMM, where the only free parameter is the
Gaussian radial dispersion.
4 Experimental results
In this Section, we present experimental results
with the freely available and widely used MCYT
database (Ortega-Garcia et al., 2003). From this database, we use a small subset of 10 subjects, with 25
genuine signatures and 25 skilled forgeries per subject (total of 500 signatures). From the genuine signatures, 5 are randomly drawn to adaptlearn individual
models. Therefore, for each subject, a probabilistic
model is adapted with only 5 genuine signatures. Afterwards, the likelihood scores associated to the 20
remaining genuine signatures, as well as the 25
skilled forgeries are computed.
Five experiments were carried out, from unconstrained GMM (Level 0 - see Section 2 for further
details) to over-constrained GMM (Level 4 - Parzen
models). Indeed, these experiments highlight the impact of constraintregularization, in an increasing
way, of GMM on performance assessment.
Moreover, each experiment was repeated 5 times,
where signatures for model adaptation are independently re-drawn in each experiment.
Concerning GMM structure, the number of
Gaussians is adjusted according to the average number of points from each enrollment set of signatures.
Thus, we set 30 sampled points per Gaussian, as proposed in (Petrovska-Delacrétaz et al., 2009), with a
minimum number of Gaussians limited to 6. Another
important implementation aspect is that Gaussian
centers in EM algorithm are initialized with 4D
points taken at random from the training set of signatures. Consequently, we keep initialization of both
GMM and Parzen models equivalent. It contrasts
with a rather popular approach where K-means algorithm is used to initialize EM.
Table 1 displays our results, in terms of EER, for
conventional GMM (with full and diagonal covariance matrices) and Parzen model. Indeed, Table 1
shows that at the average EER, Parzen model improves performance by a factor of 35 , in comparison to GMM with diagonal covariance matrix.

Figure 3. Illustration of the placement of two Gaussian Kernels
into a Parzen model, in reduced dimension (pressure dimension is
not represented).

Since kernels are all radial with identical dispersion, the only free parameter in this model is kernel
width, which is a scalar parameter clearly equivalent,
in this context, to Gaussian radial dispersion. For a
Parzen model, the choice of this kernel width (window width) plays a pivotal role. Many methods for
kernel width choice are available in the Literature,
most of them are based on heuristics, contrasting with
the likelihood based optimization of EM. Fortunately, the Parzen window width can also be optimized with likelihood, through cross-validation method
(Duin, 1976),(Leiva-Murillo et al., 2006).
In this work, we optimized the window width for
each signature through the leave-one-out cross-validation method over the N L 5 signatures available
for model adaptation, from each subject.
We highlight the striking similarity between
GMM and Parzen models used in this work. Consid-

Table1. EER from 3 models, from 5 independent runs.

4466

Run


Model 

Exp.
1

Exp.
2

Exp.
3

Exp.
4

Exp.
5

Av.
EER

GMM-full

8.6

9.3

8.4

8.0

7.7

8.4

GMMdiagonal

5.7

9.1

9.5

5.9

8.4

7.7

Parzen

4.5

5.6

5.2

4.9

4.7

5.0

Additionally, we further observe that GMM with
constraint (diagonal matrix) gives a slightly improved
result, as compared to unconstrained (full matrix)
GMM. Figure 4 shows 2 Detection Error Trade-off
(DET) curves corresponding to 2 independent runs
with GMM (constrained to use only diagonal
matrices) and the Parzen. This figure allow for a
clear comparison between performances of GMM
with diagonal matrices, which is a model frequently
used in Biometrics, and Parzen model. From these
results, we gather evidences of a superior performance of Parzen models.

that traditional GMM adapted with EM algorithms
may not be a well suited models for typical Biometric
application, where the amount of signal available for
model adaptation, after enrollment, is typically small.
Table 2. EER from all models, from 5 independent runs.




Exp.
1

Exp.
2

Exp.
3

Exp.
4

Exp.
5

GMM-full

8.6

9.3

8.4

8.0

7.7

8.4

GMMdiagonal

5.7

9.1

9.5

5.9

8.4

7.7

GMM- multi 3.8
scalar

5.6

3.1

5.9

5.8

4.84

GMM-single
scalar

4.9

3.8

3.1

5.1

5.8

4.5

Parzen

4.5

5.6

5.2

4.9

4.7

5.0

Run

Model

Av.
EER

From this table, we notice that the best result is
provided by the GMM model closer (in terms of
model regularization) to the Parzen model. However,
taking into account the confidence interval shown in
Table 3, we still cannot decide whether this superiority is due to statistical fluctuations.
Table 3. EER from each mixture model, from 5 independent runs.
Figure 4. DET curves corresponding to GMM diagonal and
Parzen models, considering 10 subjects of MCYT Database.

Av. EER

95 confidence
interval

GMM-full

8.4

7.9  8.9

GMM-diagonal

7.7

6.1  9.3

GMM-multi scalar

4.8

3.7 

6.0

GMM-single scalar

4.5

3.4 

5.9

Parzen

5.0

4.6 

5.4


Model 
Run

Observing from these first results that, apparently, increasing regularization improves performance, we add more GMM constraints, by introducing
GMM with scalar covariance matrices in our experiments. Thus, to further highlight the decreasing degree of freedom in each model, as discussed in Section 2, we explicitly present their respective number
of parameters to be adapted, per level of parametric
regularization. All models lay in a 4D space, and
N G stands for the number of Gaussian kernels.
Therefore, we have
Level 0 GMM, N G full covariance matrices
 parameters to be adapted 15 N G 
Level 1 GMM, N G diagonal covariance matrices
 parameters to be adapted 9N G 
Level 2 GMM, N G scalar covariance matrices
 parameters to be adapted 6 N G 
Level 3 GMM, single scalar covariance matrix
 parameters to be adapted 5N G 1 
Level 4 Parzen model
 parameters to be adapted 1 .
Though GMM with scalar covariance matrices
are not too current in Biometrics, in Table 2 we
present the performances provided by these unusual
models. Former results are also presented again for
reader convenience.

Though it was presented through experiments
with a reduced number of subjects, and with just one
biometric modality, we believe that this drawback
can be true for many other biometric modalities too,
for it comes from a wider and quite older discussion
on which PDF estimator best fits each practical application. Here, we compared Gaussian Mixture
Models with 5 levels of parametric and structural regularization, from GMM with full covariance matrices
to Parzen model with Gaussian kernels (seen here as
a over-constrained GMM).
First comparing these two extremes, we gave one
illustration, through simple experiments, that even if
both GMM and Parzen models are theoretically able
to converge to the true PDF to be estimated, under
training data "shortage" (as usual in Biometric enrollment), they provide remarkably different EER. What
we observe through our experiments is that, with a reduced number of instances for the model training, the
path taken by the Parzen model seems to be safer in
terms of verification performance.
Thus, what we claim here is that it is a clear
matter of model regularization the more regularized,
the better, if the number of training patterns are too

5 Conclusions and discussions
In this preliminary work, we present evidences

4467

Journal on Document Analysis and Recognition,
vol. 1, pp. 102109.
Ly Van, B., S. Garcia-Salicetti, and B. Dorizzi
(2007) On using the Viterbi path along with
HMM likelihood information for online signature
verification, IEEE Transactions on Systems,
Man, and Cybernetics, Part B, vol. 37, no. 5, pp.
12371247.
Larsen, J. (1996) Design of Neural Network Filters,
Ph.D. Thesis, Electronics Institute, Technical
University of Denmark, March 1993, Second
Edition.
Leiva-Murillo, J. M., A. Artes-Rodriguei (2006) A
Fixed-Point Algorithm for Finding the Optimal
Covariance Matrix in Kernel Density Modeling,
in ICASSP 2006 Proceedings, vol. 5, pp. V-V.
MacKay, D. J. C. (2003) Information Theory, Inference, and Learning Algorithms, Cambridge University Press, Cambrige.
Ormoneit, D., V. Tresp (1996) Improved Gaussian
Mixture Density Estimates Using Bayesian Penalty Terms and Network Averaging., Advances
in Neural Information Processing Systems, vol.
8, The MIT Press, 542-548.
Ortega-Garcia, J., J. Gonzalez-Rodriguez, D. SimonZorita, S. Cruz-Llanas (2002) From biometrics
technology to applications regarding face, voice,
signature and fingerprint recognition systems, in
Biometrics Solutions for Authentication in an EWorld, D. Zhang, Ed., pp. 289337, Kluwer
Academic Publishers, Dordrecht, The Netherlands.
Ortega-Garcia, J., J. Fierrez-Aguilar, D. Simon, et al.
(2003) MCYT baseline corpus a bimodal biometric database, IEE Proceedings Vision, Image and Signal Processing, vol. 150, no. 6, pp.
395401.
Petrovska-Delacrétaz, D., G. Chollet, B. Dorizzi
(2009) Guide to Biometric Reference Systems
and Performance Evaluation, Springer.
Rabiner, L., B. H. Juang (1993) Fundamentals of
Speech Recognition, Signal Processing Series,
Prentice-Hall, Englewood Cliffs, NJ, USA.
Richiardi, J., A. Drygajlo (2003) Gaussian mixture
models for on-line signature verification, in Proceedings of the ACM SIGMM Workshop on
Multimedia Biometrics Methods and Applications (WBMA 03), pp. 115122, Berkley, Calif,
USA.
Theodoridis, S., K. Koutroumbas (1999) Pattern Recognition, Academic Press.
Webb, A. (2002) Statistical Pattern Recognition, 2nd
Ed. ,Wiley.
Yeung, D.-Y., H. Chang, Y. Xiong, et al. (2004)
SVC2004 first international signature verification competition, in Proceedings of International Conference on Biometric Authentication
(ICBA 04), vol. 3072 of Lecture Notes in Computer Science, pp. 1622, Springer, Hong Kong.

limited. To provide even more evidences of this
claim, we finally compared all the 5 models.
Beyond our initial claim, we further found that,
apparently (to be further tested in future work), minimum EER is to be reached with a model whose level
of regularization lies somewhere between conventional GMM with diagonal covariance matrices, and
the Parzen model, but much closer to this last one.
Moreover, preliminary results, still not published, with both speaker and keystroke (typing dynamics) verification provided similar improvements
whenever GMM are replaced by Parzen models,
therefore, in the sequel of this work, we are going to
look for more evidence in favor of the use of more
regularized models, like Parzen, in Biometric application. Also, we are going to study whether the level
of model regularization should be adapted subject by
subject, and whether the particularity of Parzen based
approaches could be more efficient in such cases.
Acknowledgments
This work was partially granted by the Brazilian
Conselho Nacional de Desenvolvimento Científico e
Tecnológico (CNPq).
References
BioSecure Multimodal Evaluation Campaigns
until 2009
httpbiometrics.it-sudparis.euBMEC2007,
httpbiometrics.it-sudparis.euBSEC2009.
Dempster, A., Laird, N. and Rubin, D. (1977) Maximum likelihood estimation from incomplete data
using the EM algorithm, 39 138.
Dolfing, J. G. A., E. H. L. Aarts, and J. J. G. M. Van
Oosterhout (1998) On-line signature verification with hidden Markov models, in Proceedings of the International Conference on Pattern
Recognition, pp. 13091312, Brisbane Australia.
Duda, R. O., Hart, P. E. and Stork, D. G. (2001) Pattern Classification, 2nd edn, John Wiley  Sons,
Inc.
Duin, R.P.W. (1976) On the choice of smoothing
parameters for parzen estimators of probabiity
density functions, 25 11751179.
Garcia-Salicetti, S., N.Houmani, B. Dorizzi (2009)
A Novel Criterion for Writer Enrolment Based
on a Time-Normalized Signature Sample Entropy Measure, EURASIP Journal on Advances
in Signal Processing, Article ID 964746.
Haykin, S. (1999) Neural Networks, A Comprehensive Foundation, Prentice-Hall, Inc., New Jersey.
Jain, A. K., F. D. Griess, and S. D. Connell (2002)
On-line signature verification, Pattern Recognition, vol. 35, no. 12, pp. 29632972.
Kashi, R., J. Hu, W. L. Nelson, and W. Turin (1998)
A Hidden Markov Model approach to online
handwritten signature verification, International

4468