TREINAMENTO DE REDES SUPERVISIONADAS UTILIZANDO JANELA
DINAMICA
Marcelo Souza Fassarella, Evandro Ottoni Teatini Sales, Hans-Jorg Andreas
Schneebeli


Universidade Federal do Esprito Santo - UFES
Vitoria, ES, Brasil

Emails fassarella@ele.ufes.br, evandro@ele.ufes.br, hans@ele.ufes.br
Abstract We propose the Window method to be inserted into supervisioned neural training with noise data.
The method has an intrinsic caracteristic of regularization, because it tries to eliminate noise while the network
is beeing trained, reducing its influence of the adjustment of network weights. We implement and analize the
method in adaptive logic networks(ALN) and at multilayer perceptrons(MLP). The method was tested with
problems of function aproximation, adaptive filters and time series prediction.
Keywords

Regularization, Artificial Neural Networks, Window.

Resumo Neste trabalho, propomos o metodo da Janela a ser inserido no treinamento de redes supervisionadas com conjuntos de dados ruidosos. O metodo possui uma caracterstica intrnseca de funcao regularizadora,
ja que procura eliminar rudos enquanto a rede esta sendo treinada, reduzindo a influencia destes no ajuste dos
pesos da rede. Ele foi implementado e analisado em redes logicas adaptivas(ALN) e em redes perceptrons de
multiplas camadas(MLP). Foram feitos testes para problemas de aproximacao_de_funcoes, filtragem adaptiva e
previsao de series_temporais.
Palavras-chave

.

dados qualquer, sem necessariamente ter de assumir caractersticas gaussianas para o rudo contido
nesses dados.
O uso de redes perceptron de multiplas camadas (MLP) se deve ao fato de que elas sao aproximadores universais de funcoes Ramesh (1994).
Essas redes tem sido aplicadas com sucesso na
resolucao de diversos problemas como em Veiga
(2008). Ja as redes logicas adaptativas (ALN) sao
estruturas neurais construdas como arvore de decisao, possuindo propriedades semelhantes as das
redes MLP. Elas tambem podem ser utilizadas em
aplicacoes como em Armstrong (2000).
Neste trabalho propoem-se o uso de um metodo de regularizacao para redes supervisionadas, baseado no bloqueio do ajuste dos pesos da
rede para dados considerados inadequados, onde,
o termo inadequados refere-se aos dados cujo erro
quadratico produzido pela rede seja maior do que
um valor maximo permitido. Desta forma, ira
formar-se ao longo da superfcie neural gerada,
uma Janela ou regiao de permissao, onde o erro
de sada da rede, devido a um vetor de entrada
em particular, deve estar. O limite de permissao
da Janela e alterado a cada epoca, de forma que, a
medida que a rede se ajusta ao conjunto de dados
a Janela torna-se mais restrita.

Introducao

Ao empregar uma rede_neural artificial como ferramenta para a resolucao de um determinado
problema, estamos interessados em construir um
modelo nao-linear do fenomeno fsico responsavel
pela geracao dos exemplos de entrada - sada. Porem, como os dados sao representacoes limitadas
da superfcie que se deseja modelar e normalmente
estao contaminados com rudos, torna-se necessario assumir um compromisso adequado entre a
qualidade e a quantidade de ajuste dos pessos aos
dados usados no treinamento, de forma que a rede
consiga produzir uma boa estimacao para dados
desconhecidos. Em outras palavras, a rede_neural
deve ser capaz de generalizar. Esse e um problema
conhecido como dilema bias - variancia e sua solucao requer o uso de algoritmos capazes de evitar
que os dados de treinamento sejam decorados. As
tecnicas que evitam este problema sao conhecidas
como tecnicas de regularizacao.
Para resolver o problema de regularizacao
muitos metodos tem sido propostos, como as Tecnicas de poda e crescimento da rede Reed (1993)
Kwok (1997), as Tecnicas de parada antecipada
Dias (2003), o Treinamento com injecao de rudo
Ho (2009) e o Metodo de regularizacao bayesiana
Foresee (1997). Essas abordagens produzem resultados bons para dados contaminados com rudos cuja distribuicao e gaussiana. Porem, quando
nestes dados existem rudos com chegada aleatoria, de forma que a distribuicao assuma uma outra
forma qualquer, ha uma degradacao no resultado
final desejado. Portanto, e de interesse que exista
algum meio de treinar a rede para um conjunto de

2

O metodo da Janela

O metodo da Janela procura avaliar, durante o
treinamento, o resultado de sada gerado pela
rede, utilizando para o ajuste dos pesos somente os
dados considerados validos. A avaliacao consiste
em comparar o erro quadratico gerado na sada

5089

da rede, como resposta a um vetor de entrada em
particular, com um valor maximo esperado para
o resultado, . Sao considerados validos somente
os resultados cujo valor e menor do que o . O
efeito obtido e uma aproximacao maior das entradas mais coerentes entre si, minimizando o efeito
produzido pelos rudos existentes.
Um aspecto importante da Janela e que ela
pode ser ajustada dinamicamente, de forma que,
a medida em que o tempo passa (entenda como
tempo a execucao de uma epoca) e a rede_neural for incorporando as caractersticas medias da
funcao objetivo, o tamanho da Janela possa ser
reduzido ate um ponto de interesse.
2.1

o  e2max .

Modo 1 Uma maneira de atender a inequacao (2) e escolhendo um valor N  1, sendo N
 <. Desta forma teremos
o  N e2max .

Para estabelecermos uma relacao entre os valores, inicial e final, de abertura da Janela e as epocas, atual e final, do treinamento, podemos utilizar uma equacao que relaciona esses quatro parametros ao mesmo tempo. Uma alternativa interessante e a equacao do decaimento exponencial
Fritzke (1997), dada pela Equacao (1), cuja forma
grafica pode ser vista na Figura 1.
  o

f
o

(3)

A principal vantagem deste modo e que ele e
bem simples, necessitando apenas que determinemos o valor de N. A desvantagem e que nao ha
uma escolha direta do momento em que  igualase ao e2max inicial. Esse momento e importante
porque ele nos fornece uma expectativa de quanto
tempo a rede tera para treinar, sem ser perturbada pela Janela. Podemos manipular a Equacao
(1) para obter

Decaimento da Janela



(2)



o



f
o

epoca
 epoca

max

.

(4)

Aplicando o logaritmo neperiano em ambos os
lados da Equacao 4 teremos

epoca
 epoca

max

,

(1)

epoca
 
   epoca

max

f
ln
 ln
,
o
o
 
 

epoca
f
ln

ln
.
o
epocamax
o

onde
o  valor inicial da abertura da Janela,
f  valor final da abertura da Janela,
  valor da Janela num instante qualquer,
epoca  epoca atual,
epocamax  numero maximo de epocas permitido.

(5)
(6)

Considerando que   e2max e substituindo o
o e  na Equacao (6) obtem-se


 2

epoca
emax
f

.
ln
ln
N e2max
epocamax
N e2max

(7)

Por fim, manipulando a Equacao 7 obteremos
a relacao entre a epoca do encontro de  e e2max
inicial com a epoca maxima de treinamento, conforme

epoca  epocamax

(8)

Podemos ver que na Equacao (8) existe uma
dependencia relacionada ao e2max inicial, cujo valor e determinado pela inicializacao dos parametros da rede e pelo proprio conjunto de dados,
impedindo a determinacao antecipada de N e da
epoca desejada.
Modo 2 A segunda tecnica utilizada procura explicitar melhor o momento do encontro entre  e o e2max inicial, atraves da Equacao (1). Podemos dizer que a epoca escolhida e representada
por uma relacao ab do numero total de epocas, com
a  , b   - 0, e a < b. Manipulando a Equacao
(1) temos

Figura 1 Foma do decaimento exponencial.

2.2

ln(N )
.
ln(f )  ln(N e2max )

Determinando o valor de abertura inicial, o

Como queremos que a rede aprenda com a base de
dados, o deve ser tal que permita que todos os
pontos possam ser utilizados durante as primeiras etapas de ajuste dos pesos. Para isso, antes
do treinamento ser iniciado, devemos encontrar o
maior erro quadratico, e2max , gerado pelo conjunto
de treinamento e fazer

5090


  o

  o

f
o



f
o

 ab

,
,

a

  o

s(t)  valor de sada da rede,
X (t)  vetor de entrada,
F (w (t))  funcao relacionando todos os pesos da
rede.

a epocamax
b epocamax

(9)
(10)

Correspondentemente, o valor instantaneo da
energia total do erro,  (t), e obtido somando-se o
erro de cada amostra, conforme a equacao

!

fb

,

a

(11)

ob

1X 2
e (t) .
(17)
2
O algoritmo de retropropagacao aplica uma
correcao w(t) nos pesos, proporcional a derivada
parcial  (t)w(t) definida pela regra delta
 (t) 

colocando em termos de o



ba

o b 

a ,
fb
s

o 

ba

(12)
b
.
fa

 (t)
,
w (t)
 e(t)0 (t)s(t).

w (t)  
(13)

Considerando agora que   e2max , podemos
determinar o valor inicial de abertura da Janela
substituindo  por e2max na Equacao (13). A expressao final sera
s
o 
2.3

ba

e2max b
.
fa

(19)

sendo
  taxa de aprendizado,
0 (t)  derivada da funcao de ativacao.
A derivacao completa das equacoes acima
pode ser vista em Haykin (2004)1 .
Para aplicar o metodo da Janela, e necessario
acrescentar o parametro  na regra delta

(14)

w (t)   e (t) 0 (t) s (t) .

Determinando o f

(20)

O parametro  corresponde a situacao do
erro quadratico da amostra t em relacao a Janela
e e definido de acordo com a regra

1 se e2 (t)  ,
 
0 caso contrario.

Ja o parametro f deve ser escolhido pelo usuario
e correspondera ao maior e2max permitido como
resposta da rede a um dado de entrada na ultima epoca de treinamento. Isso pode ser feito
de acordo com algum conhecimento a priori que
o projetista tenha dos dados, ou um valor que
seja aceitavel para uma aplicacao especfica, ou
por meio de alguma heurstica conveniente para o
projetista.
2.4

(18)

E importante salientar que o  pode ser incorporado ao  , mas nao o contrario, substituindo
o valor um na regra anterior pelo  e eliminando
este da regra delta. Porem, preferiu-se manter os
dois parametros em separado, ja que o  pode ser
entendido como um elemento que ajusta o quanto
a rede deve aprender numa determinada epoca,
enquanto que o  nos diz quem tem o direito de
ajustar a rede durante a excecucao da epoca.

A Influencia da Janela no Algoritmo de Retropropagacao

O algoritmo de retropropagacao e uma generalizacao do algoritmo do mnimo quadrado medio
(LMS). Este algoritmo e dividido em duas etapas
o passo para frente e o passo para tras. No passo
para frente obtemos a resposta da rede a um vetor
de entrada e no passo para tras, os pesos sao ajustados de acordo com alguma regra relacionada ao
sinal de erro gerado pela diferenca entre a sada e
o valor desejado, conforme

2.5

Implementacao em Software

Para implementar o metodo da Janela no treino
de uma rede_neural sao necessarios alguns poucos
passos, conforme listado a seguir
 Adicionar a variavel , correspondente a
abertura da Janela.

e(t)  d(t)  s(t),
e(t)  d(t)  X(t)F (w(t)).

(15)
(16)

 Adicionar a variavel  , que ira afetar a regra
de correcao dos pesos.

onde para uma amostra t temos
e(t)  erro de sada,
d (t)  valor desejado,

 Definir o valor de f .
1 No

5091

captulo 4, da pagina 183 a pagina 194.

 Definir o valor de N, se usando o Modo 1, ou
o valor de a e b se usando o Modo 2.

3.2

Aproximador de Funcao

Neste caso utilizaremos um sistema que gera um
sinal de sada correspondente a equacao y  3x
limitado ao intervalo -5, 5 e que depois passa
por um agente contaminante ate tomar a forma da
Figura 2 e possuir estatsticas de media e variancia
conforme apresentadas pela tabela 1.

 Passar os dados pela rede sem treinar, para
estabelecer o valor de o .
 Implementar a equacao (1).
 Acrescentar a variavel  na regra delta .
 Alterar, para cada amostra, o valor de  de
acordo com o e2 (t) e .
3

Testes e Resultados

Nesta secao serao apresentados alguns dos casos
de teste utilizados durante o processo de avaliacao
do metodo da Janela. O metodo foi implementado
nas redes logicas adaptivas, ALN, e nas redes perceptrons de multiplas camadas, MLP. Ambas com
a correcao dos pesos baseado no algoritmo do gradiente_descendente e procurando minimizar a funcao de custo do erro quadratico medio. No caso
das redes MLP foram utilizadas apenas estruturas
com uma unica camada oculta.
Mostraremos os resultados referentes a duas
redes perceptrons de multiplas camadas, MLP E1 e MLP - E2, cuja diferenca esta no numero
de neuronios ocultos, e uma para a rede ALN.
Quando as redes estiverem usando o metodo da
Janela, serao acrescentados os termos JG ao final do nome, em referencia a reducao gradual da
abertura da Janela.
O treinamento de ambas as arquiteturas foi
feito em softwares especficos para cada uma delas.
Mais especificamente, o software ALNGD, implementado em Java e o software Flood, implementado em C++ como biblioteca livre.
Para comparacao com um metodo de regularizacao tradicional, foi utilizada a biblioteca neural do MatlabT M para implementacao de uma
rede com o algoritmo Levenberg-Marquadt (MLP
- LM) como em Soares (1999) e Lourakis (2005)
e implementacao do algoritmo de Regularizacao
Bayesiana (MLP - RB) como em Mackay (1991) e
Foresee (1997).
3.1

Figura 2 Grafico do sinal y  3x apos contaminacao.

Tipo de sinal
Sinal Limpo
Sinal Contaminado

Media - 
0
0,1890

Variancia -  2
75,0819
368,5605

Tabela 1 Estatsticas de media e variancia associadas ao experimento y  3x.
O objetivo e que a rede_neural possa encontrar o sinal limpo, com base na apresentacao dos
exemplos contaminados. Primeiramente, discutiremos os resultados sobre os dados de treinamento
e depois sobre o conjunto de teste. Para avaliar o
desempenho do metodo da Janela, iremos comparar as estruturas duas a duas, separadas por tamanho, caso das MLPs, ou por tipo de rede, caso
das ALNs. No caso das redes MLP, foram escolhidos para apresentacao somente dois tamanhos
diferentes um com pouca ou nenhuma oscilacao
do sinal de sada, redes com cinco neuronios, e outro com incio de polarizacao aos dados, redes com
quinze neuronios.
Para este exemplo consideramos os seguintes
parametros, obtidos apos experimentacao
o  3e2max ,
f  0,01,
Numero de epocas  5000,
Taxa de aprendizado  0,001.

Metricas

Para avaliar o desempenhos das redes em cada
problema existem varias formas ja estabelecidas
no meio academico e profissional. Dentre eles optamos por utilizar
 A visualizacao grafica,
 a raiz quadrada do erro quadratico medio normalizado - NRMSE,

A tabela 2 mostra as medicoes estatsticas feitas sobre os resultados gerados pelas redes apos
o treinamento, obtidos com os dados de treino.
Observa-se que em relacao ao sinal bruto nao ha
grandes diferencas entre os valores de NRMSE das

 o coeficiente de U-Theil, para o caso de previsao de series_temporais.

5092

Rede
ALN
ALN - JG
MLP - E1
MLP - E1JG
MLP - E2
MLP - E2JG
MLP - LM
MLP - RB

Estrutura
13 folhas
12 folhas
1 - 5- 1
1-5-1
1 - 15- 1
1 - 15 - 1
1 - 15 - 1
1 - 15 - 1

bruto
NRMSE
0,9089
0,8991
0,8916
0,8996
0,8742
0,8981
0,8619
0,8839

limpo
NRMSE
0,3045
0,0030
0,2903
0,0293
0,4507
0,0188
0,5698
0,2699

Tabela 2 Estatsticas obtidas com dados de treinamento do aproximador. A coluna bruto referese ao sinal original, usado no treinamento, enquanto que, a coluna limpo refere-se a um conjunto de dados obtido atraves da equacao y  3x,
como referencia.

Figura 3 Serie historica da concentracao de ozonio em Azuza, de 1956 a 1970.
A previsao sera feita um passo a frente, utilizando para isso um vetor de dados de entrada
contendo o dado atual mais os seus cinco elementos antecessores.
Dos resultados apresentados na Tabela 4, podemos verificar que todas as redes, exceto a MLPLM, possuem o indice U de Theil abaixo de 1, o
que indica uma previsao melhor do que a previsao
trivial. Apesar disso, ao compararmos as redes entre si, vemos que o metodo da Janela aplicado as
redes ALN, MLP-E1 e MLP-E2 proporcionou um
ganho de 3,1, 14,8 e 2,7 na previsao e na precisao (NRMSE), enquanto que, o MLP-RB produziu resultados aproximadamente 52 melhores do
que o MLP-LM.
Comparando os resultados do MLP-RB com o
MLP - E2JG, vemos que o primeiro apresenta um
ganho de 10,3, mas ainda assim e pior do que
os valores gerados pela rede MLP - E1JG, o que
nos leva a questionar se a previsao do MLP-RB
com uma estrutura menor, com apenas 5 neuronios ocultos, produzira previsoes melhores. Podemos ver que nao e isso que ocorre. O MLP-RB,
apesar de continuar melhorando o resultado do
MLP-LM, se manteve estavel em relacao a rede
com quinze neuronios ocultos, e com isso, a rede
MLP - E1JG apresentou um resultado favoravel
de 1,5 em relacao a rede de mesmo tamanho e
com regularizacao Bayesiana.

redes com e sem Janela, mas ao comparar os resultados em relacao ao sinal limpo, percebe-se que o
metodo da Janela proporciona as redes um ganho
significativo, com resultados de NRMSE proximos
de zero.
Comparando separadamente o metodo de regularizacao Bayesiana com a rede MLP - E2,
pode-se ver, pelo NRMSE relativo ao sinal limpo,
que o metodo Bayesiano melhorou o resultado,
mas se a comparacao for feita com a rede MLP
- E2JG, observa-se que a Janela aproxima melhor
da equacao desejada (y  3x).
Os resutados obtidos na etapa de teste foram
semelhantes aos da etapa de treinamento e podem
ser vistos na Tabela 3.
Rede
ALN
ALN - JG
MLP - E1
MLP - E1JG
MLP - E2
MLP - E2JG
MLP - LM
MLP - RB

Estrutura
13 folhas
12 folhas
1 - 5- 1
1-5-1
1 - 15- 1
1 - 15 - 1
1 - 15 - 1
1 - 15 - 1

bruto
NRMSE
0,8904
0,8730
0,8600
0,8719
0,8704
0,8720
0,8971
0,8673

limpo
NRMSE
0,3033
0,0030
0,2875
0,0310
0,4478
0,0197
0,7142
0,2710

Tabela 3 Estatsticas de teste do aproximador. A
coluna bruto refere-se ao sinal original, usado no
treinamento, enquanto que a coluna limpo referese a um conjunto de dados obtido atraves da equacao y  3x.

3.3

3.4

Filtragem Neural

Como aplicacao pratica de um filtro neural, utilizamos os sinais provenientes de um sensor de infravermelho passivo(PIR). O sensor PIR e capaz de
detectar mudancas no padrao de luz infravermelha
emitida pelos objetos que estao na sua vizinhaca
e, dependendo de como sao construdos, e possvel
detectar o movimento de um objeto no ambiente
e a direcao para onde ele se desloca. Para isso,
o sensor deve ser construdo com dois elementos
PIR dispostos lado a lado, conforme a Figura 4. A

Previsao de Series Temporais

A analise da influencia do metodo da Janela em
um sistema previsor sera feita usando a base de
dados de serie historica, apresentadas em Morettin
(2006), correspondente aos valores mensais de concentracao de ozonio em Azuza, California, EUA,
de Janeiro de 1956 a Dezembro de 1970, conforme
Figura 3.

5093

Rede
ALNGD
ALNGD - JG
MLP - E1
MLP - E1JG
MLP - E2
MLP - E2JG
MLP - LM
MLP - RB
MLP - LM
MLP - RB

Estrutura
40 folhas
40 folhas
1-5-1
1-5-1
1 - 15 - 1
1 - 15 - 1
1-5-1
1-5-1
1 - 15 - 1
1 - 15 - 1

NRMSE
0,5420
0,5254
0,5768
0,4913
0,5716
0,5563
0,5951
0,4988
1,0571
0,4988

U de Theil
0,7061
0,6845
0,7514
0,6401
0,7446
0,7247
0,7753
0,6498
1,3772
0,6498

Tabela 4 Resultados da previsao dos nveis de
concentracao da camada de ozonio sobre os dados
de teste.
sada resultante e uma forma de onda que depende
do sentido no qual o objeto passa. Podemos ver
na Figura 5 como esse sinal e gerado e sua forma
caracterstica.
Figura 5 Utilizacao do sensor PIR para deteccao
de direcao do movimento da fonte de calor e forma
de onda tpica.

Figura 4 Sensor PIR construdo para deteccao de
direcao de movimento.
Entretanto, conforme mostrado na Figura 6,
os sinais provenientes destes sensores nao esta
isento de rudos provocados por variacoes termicas
no ambiente, nos componentes do sistema sensor e
interferencias de radio frequencia (RF) e essa contaminacao produz diversas sequencias de variacao
do sinal da derivada, o que dificulta a analise dos
resultados.
O tratamento dos sinais rudosos e feito atraves do uso de filtros, cujo objetivo e reduzir o
efeito das impurezas contida nos dados. No caso
do sensoriamento com o PIR, o uso de um filtro
adaptativo e de extremo interesse ja que a sada
deste sensor e sensvel a variacoes estatsticas do
ambiente.
Porem, para construcao de um filtro neural
e necessario um sinal de referencia. No caso do
PIR, este sinal nao estava disponvel diretamente.
Por isso, foi proposto o processamento dos dados
em duas etapas, conforme Figura 7. Na primeira
etapa deve-se contruir um aproximador de funcao
para gerar o sinal de referencia ou sinal limpo. Na
segunda etapa o filtro neural e implementado da
maneira tradicional, porem utilizando o sinal de
sada do aproximador como sinal desejado.
Na primeira etapa optamos pelo uso de uma

Figura 6 Sinal capturado correspondente a passagem de uma pessoa da esquerda para a direita,
sob o ponto de vista do sensor.

5094

Figura 9 Resultado de sada do filtro neural para
o PIR.
Figura 7 Metodologia utilizada para implementacao do filtro neural para o sensor PIR.

determinstico para o mesmo.
Pode-se notar pela Figura 9 que o nvel de
rudo, durante os momentos sem o sinal de pessoa
passando, foi reduzido, mas nao completamente
eliminado. A Tabela 6 mostra o quanto o filtro
conseguiu aproximar do sinal desejado. O valor
do NRMSE e baixo o suficiente para garantirmos
que o filtro esta bem dimensionado para o conjunto de dados em questao. Porem, um dos atributos de uma rede_neural e a capacidade de generalizacao mediante a apresentacao de dados desconhecidos. Por isso, passaremos por este filtro
neural um conjunto de dados cujo sinal contem a
informacao do deslocamento de duas pessoas que,
inicialmente passam pelo sensor da esquerda para
a direta, uma apos a outra, e logo em seguida retornam, ou seja, o segundo movimento ocorre da
direita para a esquerda.
Rede
ALNGD-JG

Figura 8 Resultado grafico do aproximador ALN
para o sinal do PIR.

Estrutura
40 folhas

NRMSE
0,2395

Tabela 6 Resultados do filtro neural para o sinal
do PIR com deslocamento no sentido da esquerda
para a direita, medido em relacao ao sinal desejado.

rede ALN regularizada pelo metodo da Janela. O
resultado final da rede treinada pode ser visto na
tabela 5. Ja a Figura 8 mostra o sinal de sada da
rede apos a passagem das amostras.
Rede
ALNGD-JG

Estrutura
40 folhas

O resultado desse conjunto de dados apos a
passagem pelo filtro pode ser visto na Figura 10.
Podemos notar que as principais caractersticas do
sinal, que sao os picos e as transicoes, se mantiveram e houve, conforme ocorreu durante o treinamento, uma reducao no nvel do rudo durante os
momentos sem a informacao de pessoas passando.

NRMSE
0,2395

Tabela 5 Resultado do aproximador neural ALN
para o sinal do PIR.
Agora existe um interesse em incorporar informacoes temporais na rede_neural. Para isso, serao
utilizados como sinais de entrada o sinal atual e
um conjunto de sinais atrasados no tempo. Foi
escolhido para o desenvolvimento deste filtro um
conjunto com 5 elementos atraso unitario. A escolha desta quantidade de atrasadores foi meramente opcional e nao envolveu nenhum algoritmo

4

Conclusoes

O uso de elementos regularizadores procura garantir uma boa relacao entre o ajuste e a capacidade de generalizacao de uma rede_neural para
um problema em particular, atraves de um ajuste
dos parametros livres a tendencia dos dados ao

5095

Dias, F. Antunes, A. M. A. (2003). Regularization versus early stopping A case study with
a real system, 2nd IFAC Conference Control
Systems Design .
Foresee, F. D. Hagan, M. T. (1997). Gaussnewton approximation to bayesian regularization, pp. 19301935.
Fritzke, B. (1997). Some competitive learning
methods, Institute For Neural Computation
.
Haykin, S. (2004). Redes Neurais Princpios e
Pratica, Bookman.
Ho, K. Leung, C. S. J. (2009). On weightnoise-injection training, Advances in NeuroInformation Processing pp. 324331.

Figura 10 Resultado de teste do filtro neural. O
sinal corresponde a informacao do deslocamento
de duas pessoas no sentido da esquerda para a
direita e da direita para a esquerda.

Kwok, T. Yeung, D. (1997). Constructive algorithms for structure learning in feedforward
neural networks for regression problems,
IEEE Transactions on Neural Networks
pp. 630645.

inves de aos dados. Neste trabalho foi proposto
um novo metodo para regularizar o treinamento
de redes_neurais supervisionadas. Esse metodo
nao assume que o rudo contido nos dados e puramente Guassiano. Como vantagens adicionais
do metodo podemos citar o conjunto de dados
de treinamento nao e reduzido, nao e necessario
interromper o processo de ajuste para avaliar o
comportamento da rede, o baixo_custo_computacional e pode ser utilizado no treinamento em que
os pesos sao ajustados a cada novo vetor de entrada apresentado.
O metodo da Janela baseia-se na medicao do
erro quadratico gerado por cada vetor de entrada,
cujo valor nao pode ultrapassar um limite maximo estabelecido. Desta forma, o erro devera estar contido dentro de uma regiao permitida, caso
contrario, a amostra nao e utilizada para o ajuste
dos pesos. Esse limite maximo e reduzido a cada
epoca ate um valor final pre-determinado.
Para validacao do metodo, foi analisada a influencia da Janela em aplicacoes de aproximacao
de funcoes, previsao de serie temporais e filtragem
de rudos. Pode-se ver, pela analise dos resultados,
que a Janela fornece resultados compatveis com
o metodo de regularizacao Bayesiana tradicional.

Lourakis, M. I. A. (2005). A brief description
of the levenberg-marquardt algorithm implemented by levmar.
Mackay, D. (1991). Bayesian interpolation, Neural
Computation 4 415447.
Morettin, P. A. Toloi, C. M. (2006). Analise de
series_temporais, Edgard Blucher .
Ramesh, J. Cheni, D. (1994). A robust back propagation learning algorithm for function approximation, IEEE Transactions on Neural
Networks 5.
Reed, R. (1993).
Pruning algorithms-a survey, IEEE Transactions on Neural Networks
pp. 740747.
Soares, P. P.  Nadal, J. (1999). Aplicacao de
uma rede_neural feedforward com algoritmo
de levenberg-marquardt para classificacao de
alteracoes do segmento st do eletrocardiograma, IV Congresso Brasileiro de Redes
Neurais pp. 888999.
Veiga, N. e. a. (2008). Classificacao de dados
botanicos e geomorfogicos, utilizando redes
neurais artificiais, aplicados a analise ecoepidemiologica da doenca de chagas em abaetetuba, barcarena e braganca, no estado do
para no perodo de 2000 a 2006, XXVIII Congresso da Sociedade Brasileira de Computacao .

Agradecimentos
Este trabalho contou com o apoio do Programa de
Pos-Graduacao em Engenharia Eletrica da UFES.
Referencias
Armstrong, W. Gorodnich, D. (2000). Breaking
hyperplanes to fit data with applications to
3d world modeling and oil sand data analysis,
Proc. ICSC Symposium on Neural Computation .

5096