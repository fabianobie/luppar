Anais do XX Congresso Brasileiro de Automática
Belo Horizonte, MG, 20 a 24 de Setembro de 2014

ANHNA UMA NOVA ABORDAGEM DE SELECAO DE MODELOS PARA A
MAQUINA DE APRENDIZADO EXTREMO
Ananda Freire, Guilherme Barreto


Universidade Federal do Ceara, Departamento de Engenharia de Teleinformatica (DETI)
Av. Mister Hull, SN - Centro de Tecnologia, Campus do Pici, Fortaleza, Ceara, Brazil
Emails anandalf@gmail.com, gbarreto@ufc.br

Abstract Setting an optimal architecture for the Extreme Learning Machine (ELM) for a specific problem
is still an open problem in Machine Learning. Several metaheuristic based methods have been proposed to find
the best set of synaptic hidden weights, but they fail in providing a priori the optimal number of hidden neurons. Within this context, we propose a novel approach for architecture selection and hidden neurons excitability
improvement for the ELM. Named Adaptive Number of Hidden Neurons Approach (ANHNA), the proposed approach relies on a new general encoding scheme of the solution vector that automatically estimates the number
of hidden neurons and adjust their activation function parameters (slopes and biases). Comprehensive computer experiments were carried out using Differential Evolution (DE) and Particle Swarm Optimization (PSO)
metaheuristics, with promising results being achieved by the proposed method in regression problems.
Keywords Extreme Learning Machine, Metaheuristics, Differential Evolution, Particle Swarm Optimization, Model Selection, Intrinsic Plasticity.
Resumo Determinar a melhor arquitetura, em um problema especfico, para uma rede do tipo Maquina de
Aprendizado Extremo e um desafio corrente na area de Aprendizado de Maquinas. Diversos metodos baseados
em metaheursticas se propoem a encontrar o melhor conjunto de pesos sinapticos da camada oculta, no entanto,
exigem que o numero desses neuronios seja conhecido a priori. Nesse contexto, propomos uma nova abordagem
para selecao_de_modelos aliada ao ajuste da excitabilidade dos neuronios ocultos para uma Maquina de Aprendizado Extremo. Denominada de Abordagem Adaptativa para o Numero de Neuronios Ocultos (ANHNA), ela
tem como base um esquema de codificacao geral para vetores-solucao que automaticamente estima o numero
de neuronios ocultos, bem como ajusta os parametros das funcoes de ativacao (inclinacoes e limiares). Experimentos computacionais, utilizando Evolucao Diferencial e Otimizacao por Enxame de Partculas, apresentaram
resultados promissores em problemas de regressao.
Palavras-chave .

1

baseado em metaheursticas vem sendo usados
como metodos de busca global para encontrar
os valores desses pesos sinapticos. Por exemplo
a rede ELM Evolucionaria (Evolutionary ELM )
(Zhu et al., 2005) utiliza o algoritmo Evolucao
Diferencial (Differential Evolution - DE) (Storn
and Price, 1997) para encontrar os pesos de entrada. Os pesos de sada sao estimados como na
ELM tradicional. O algoritmo ELM Evolucionaria Auto Adaptativa (Self Adaptive Evolutionary
ELM ) (Cao et al., 2012) e similar a E-ELM, contudo, permite uma selecao automatica de diferentes estrategias para geracao de vetores-diferenca e
parametros de controle. Em (Han et al., 2013), a
rede ELM e combinada com uma Otimizacao por
Enxame de Partculas Melhorada (Bratton and
Kennedy, 2007) para otimizar os pesos de entrada
e seus limiares. Outro metodo e o Group Search
Optimization ELM (Silva et al., 2011) que tambem busca a otimizacao dos pesos ocultos e limiares.
Nenhum dos metodos citados acima sao projetados para determinar automaticamente o numero
otimo de neuronios ocultos. Em outras palavras,
esse numero e determinado de antemao mediante
longos testes empricos. Ademais, eles nao exploram a possibilidade de adaptar os parametros de
excitabilidade dos neuronios, que sao a inclinacao

Introducao

Dos diversos tipos de redes_neurais existentes, as
redes feedforward sao as mais populares (Huang
et al., 2011), sendo as do tipo de unica camada
oculta especialmente conhecidas por funcionarem
como aproximadores universais (Cao et al., 2012).
Dentre elas, a Maquina de Aprendizado Extremo
(Extreme Learning Machine - ELM) se destaca devido ao seu rapido perodo de treinamento e sua
essencia esta no uso de pesos sinapticos aleatorios
na camada oculta. Essa configuracao resulta em
um modelo_linear para os pesos sinapticos da camada de sada da rede (pesos de sada), os quais
sao analiticamente calculados por meio de uma solucao de mnimos_quadrados (Huang et al., 2011).
Ainda que esta rede ofereca uma boa capacidade
de generalizacao, a escolha aleatoria dos pesos
sinapticos da camada oculta (pesos de entrada)
pode gerar um conjunto nao-otimo de pesos e limiares, ocasionando o efeito de sobreajuste (Cao
et al., 2012). Outro problema enfrentado e a escolha do numero de neuronios ocultos e a possibilidade de diminu-lo sem afetar a eficacia do
aprendizado, acarretando em diversos testes por
tentativa e erro (Yang et al., 2012 Ye et al., 2010).
No intuito de contornar esses desafios, durante os ultimos anos, algoritmos de otimizacao

2500

Anais do XX Congresso Brasileiro de Automática
Belo Horizonte, MG, 20 a 24 de Setembro de 2014

xi  wiT U e coletada, onde U  (u(1), ..., u(N )).
As sadas desejadas aleatorias para a camada
oculta, pertencentes a distribuicao fdes , tfdes 
(t1 , ..., tN )T , e os estmulos coletados sao ambos colocados em ordem crescente. Por fim, o
modelo (xi )  (xTi , (1, ...1)T ) e construdo de
forma a calcularmos (ai , bi )T  ((xi )T (xi ) +
I)1 (xi )T f 1 (tfdes ), em que f 1 e a inversa
da funcao logstica adotada,  > 0 e o parametro
de regularizacao e I  Rqq e uma matriz identidade (Neumann and Steil, 2011).

e o limiar de suas funcoes de ativacao. O ajuste
desses parametros esta relacionado a plasticidade
intrnseca de uma celula neuronal (Triesch, 2005)
e pode resultar em uma melhor capacidade de generalizacao (Neumann and Steil, 2011 Neumann
and Steil, 2013).
Com base nas informacoes acima, nos apresentamos um novo metodo de codificacao para
vetores-solucao de um algoritmo_de_otimizacao baseado em metaheurstica, promovendo uma estimacao automatica do numero de neuronios ocultos e dos valores dos parametros de suas funcoes de
ativacao. Este novo esquema, denominado Abordagem Adaptativa para o Numero de Neuronios
Ocultos (Adaptive Number of Hidden Neurons Approach - ANHNA), e generalizado para o uso em
qualquer algoritmo para otimizacao contnua baseado em metaheurstica. Nos experimentos realizados foram utilizados Evolucao Diferencial e
Otimizacao por Enxame de Partculas, os quais
mostraram a viabilidade da abordagem proposta.
2
2.1

2.3

A Evolucao Diferencial (Differential Evolution)
tem como objetivo evoluir uma populacao de N P
cromossomos (Ci  Rd , i  1, ..., N P ) em direcao
ao otimo global, sendo a populacao inicial escolhida aleatoriamente (Ci (0)  U (Cmin , Cmax )).
O primeiro passo e a mutacao, produzindo um
vetor-diferenca, Vi , para cada cromossomo da gesima populacao.

Algoritmos

Vi (g)  Ci1 (g) + (Ci2 (g)  Ci3 (g)),

Maquina de Aprendizado Extremo - ELM

A Maquina de Aprendizado Extremo (Extreme
Learning Machine) e uma rede_neural de duas camadas com pesos sinapticos fixos e aleatorios na
camada oculta,W  Rpq , em que p e o numero
de unidades de entrada e q o numero de neuronios
ocultos (Huang et al., 2011). A sada da camada

oculta e dada por h(k + 1)   WT (k)u(k) ,
onde u(k)  Rp e o atual vetor de entrada e ,
uma funcao de ativacao
A j-esima sada
Plogstica.
out
da rede e yj (k + 1)  qi1 wji
(k)hi (k).
Durante o treino, todas as entradas da sequencia de treinamento ((u(k), d(k)), k  1...N ) sao
apresentadas a rede, e os estados correspondentes (h(k), d(k)) sao coletados em suas respectivas
 onde d(k) representa a sada dematrizes H e D,
sejada. O calculo da matriz de pesos da camada de
sada, Wout , e realizada por meio de uma regres em que H e a inversa
sao linear Wout  H D,
generalizada de Moore-Penrose da matriz das sadas da camada oculta H.
2.2

Evolucao Diferencial - DE

(1)

em que   (0, ) e o fator de escala que controla a amplificacao da variacao diferencial (Storn
and Price, 1997). Das mais diversas abordagens
que podem ser utilizadas para mutacao, nos adotamos a seguinte os cromossomos Ci2 (g) e Ci3 (g)
sao aleatoriamente escolhidos com i1 6 i2 6 i3 , e
Ci1 (g) e o indivduo com o melhor fitness.
Em seguida, e realizada uma recombinacao
discreta entre Vi (g) and Ci (g) para produzir os
descendentes Ci (g). Para cada j-esimo componente

Vi,j (g), se U (0, 1) < CR

Ci,j (g) 
(2)
Ci,j (g), caso contrario.
Como sugerido em (Das et al., 2009), a taxa de
recombinacao CR adotada neste trabalho decrescera linearmente com as geracoes de CRmax  1
ate CRmin  0.5, seguindo a regra CR
 
g
CRmin + (CRmax  CRmin ) MAXGEN
, em
MAXGEN
que M AXGEN e o numero maximo de geracoes
definido pelo usuario. Tal ajuste do CR permite
explorar o espaco de busca exaustivamente no comeco e, a medida que as geracoes vao avancando,
o conjunto de solucoes sofre um ajuste fino. Assim, a populacao explorara o interior de um espaco
relativamente pequeno, em que esteja o possvel
otimo global (Das et al., 2009). Finalmente, a selecao dos cromossomos para a proxima geracao

Plasticidade Intrnseca em Batelada - BIP

A Plasticidade Intrnseca em Batelada (Batch Instrinsic Plasticity) trata-se de uma regra de aprendizado_online nao-supervisionada para ajuste do
limiar (bi ) e da inclinacao (ai ) das funcoes de ativacao dos neuronios ocultos, ajustando-os para regimes mais adequados, maximizando a transmissao de informacao e atuando como um regularizador de atributos (Neumann and Steil, 2011).
E executado de maneira que a ativacao de um
neuronio hi (k)  (1 + exp(ai xi (k)  bi ))1 adquira uma distribuicao exponencial desejada fdes .
Para cada neuronio oculto, toda soma sinaptica

Ci (g), se f (Ci (g)) < f (Ci (g))
Ci (g), caso contrario
(3)
em que f (.) e a funcao objetivo a ser minimizada.
Ci (g + 1) 

2501



Anais do XX Congresso Brasileiro de Automática
Belo Horizonte, MG, 20 a 24 de Setembro de 2014

Figura 1 Representacao do i-esimo cromossomo na g-esima geracao no metodo ANHNA.
2.4

Otimizacao por Enxame de Partculas - PSO

Os primeiros Qmax elementos variam entre 0,1 e
sao responsaveis pelo controle da ativacao ou desativacao do conjunto (ai,j , bi,j ) correspondente. Os
Qmax elementos seguintes sao os valores da inclinacao da funcao de ativacao (ai,j ) e os Qmax restantes sao os valores dos limiares (bi,j ). Na Figura
1 segue a representacao do i-esimo cromossomo.
Os limiares de ativacao Ti,j se comportam
como genes de controle, selecionando o numero de
neuronios ocultos ativos, q, de acordo com esta regra se Ti,j  0.5, entao o j-esimo neuronio oculto
(ai,j ,bi,j ) esta ativo, caso contrario, este neuronio
nao existe para a rede. Ademais, algumas precaucoes devem ser tomadas no comeco de cada
populacao (pais e descendentes, no caso do DE).
Primeiramente, deve-se checar se em cada cromossomo ha, pelo menos, o numero mnimo Qmin de
neuronios ativos. Em cada cromossomo que nao
atinge esse valor, Qmin limiares de ativacao sao
escolhidos aleatoriamente e tem seus valores modificados Ti (g)  U (0.5, 1). A ultima regra e
aplicada apos a atualizacao de cada cromossomo,
onde os valores da inclinacao ai sao substitudos
por seus valores absolutos. O Algoritmo 1 apresenta o pseudo-codigo do metodo ANHNA.

A Otimizacao por Enxame de Partculas (Particle Swarm Optimization) e um algoritmo de busca
baseado em populacoes que simulam o comportamento social de passaros em um bando. Os indivduos dessa populacao, que aqui tambem serao chamados de cromossomos Ci  Rd , sao estabelecidos em posicoes e velocidades aleatorias
(Ci (0)  U (Cmin , Cmax )) e vi respectivamente.
Cada cromossomo e definido dentro do contexto
de uma vizinhanca k que compreende ele mesmo
e alguns outros indivduos da populacao. No contexto do PSO global, essa vizinhanca abrange todo
o enxame e no contexto do PSO local, e adotada
uma topologia social que estabelecera vizinhancas
menores. Neste trabalho, para o caso do PSO local, escolhemos a estrutura em anel, onde cada
indivduo se comunica com os seus vizinhos imediatos.
Para cada geracao, vetores armazenam a melhor posicao historica de cada indivduo, pi  Rd ,
e a melhor posicao historica dentro da vizinhanca
k, pki  Rd . A diferenca entre pki e a i-esima
posicao atual de um indivduo e estocasticamente
adicionada em sua atual velocidade, ocasionando
uma oscilacao na trajetoria em torno da melhor
posicao. A equacao de atualizacao da velocidade
e mostrada abaixo

3

Os
desempenhos
do
ANHNADE,
ANHNAglobal-PSO, ANHNAlocal-PSO, a
ELM tradicional e a ELM com BIP sao comparados em cinco conjuntos de dados reais em
problemas de regressao. Estes foram adquiridos
atraves dos bancos de dados UCI1 (Bache
and Lichman, 2013) e StatLib2 . O conjunto
Autompg refere-se ao ciclo cidade de consumo de
combustvel em milhas por galao. O conjunto
Boston housing pertence a tarefa de predicao dos
valores imobiliarios de areas na cidade de Boston.
O conjunto CPU refere-se a predicao baseada no
desempenho relativo de uma CPU. O conjunto
Bodyfat refere-se a determinacao do percentual de
gordura corporal a partir da pesagem submersa e
de varias medidas de circunferencia corporal. Por
fim, o conjunto iCub (Freire et al., 2012), cujo
objetivo e estimar os angulos das juntas do braco
do robo humanoide iCub, usando como entrada
as coordenadas em pixels de uma bola, fornecidas
pelas suas duas cameras, em uma tarefa de
apontar para um objeto fora de seu alcance.

vij (g + 1)  vij (g) + 1 1 (pij (g)  Cij (g))
+2 2 (pkij (g)  Cij (g))
(4)
em que  e o fator de constricao, 1 e 2 sao
constantes positivas chamadas de coeficiente de
aceleracao e, 1 e 2 sao numeros aleatorios e independentes unicamente gerados a cada atualizacao
para cada elemento de um indivduo (Bratton and
Kennedy, 2007). A atualizacao do i-esimo cromossomo e
Ci (g + 1)  Ci (g) + vi (g + 1).
2.5

Resultados

(5)

Abordagem Adaptativa para o Numero de
Neuronios Ocultos - ANHNA

O metodo ANHNA (Adaptive Number of Hidden
Neurons Approach) tem como base a codificacao
de cromossomos que otimizam, simultaneamente,
o numero de neuronios ocultos e os parametros de
suas funcoes de ativacao. Inspirado por Das et al.
(2009), o cromossomo e um vetor de numeros reais de dimensao (Qmax + Qmax + Qmax ), em que
Qmax e o numero maximo de neuronios ocultos.

1 httparchive.ics.uci.edumlindex.html
2 httplib.stat.cmu.eduindex.php

2502

Anais do XX Congresso Brasileiro de Automática
Belo Horizonte, MG, 20 a 24 de Setembro de 2014
AutoMPG

Algoritmo 1 Pseudo-codigo ANHNA

RMSE Treino
RMSE Teste

ELMBIP


input Qmin , Qmax 
Inicialize os parametros do DE ou PSO
Inicialize a populacao C(0), em que T(0) e a(0) 
U (0, 1) e b(0)  U (1, 1)
Cheque os limiares de ativacao
while g < M AXGEN do
for cada cromossomo, Ci (g) do
Treine e Teste uma ELM
 com qi
neuronios ocultos ativos e seus (ai ,bi )
Avalie f (Ci )  funcao fitness RMSE do
teste
if DE escolhido then
Mutacao (Eq. 1)
Cheque os limiares de ativacao e ai 
Crossover (Eq. 2) e Selecao (Eq. 3) 
Ci (g + 1)
end if
end for
if PSO escolhido then
for cada cromossomo, Ci (g) do
Defina pi and pki   melhores posicoes
individual e na vizinhanca
Atualize a velocidade (Eq. 4) e posicao
(Eq. 5)
Cheque os limiares de ativacao e ai 
end for
end if
end while
return ELM com o menor RMSE ao final da ultima
geracao.

ELM

ANHNAPSOL

ANHNAPSO

0.
0
0. 44
0
0. 46
04
0. 8
0
0. 5
0
0. 52
0
0. 54
0
0. 56
05
0. 8
0
0. 6
0
0. 62
06
0. 4
0
0. 66
06
0. 8
0
0. 7
0
0. 72
0
0. 74
0
0. 76
07
0. 8
0
0. 8
0
0. 82
08
4

ANHNADE

Bodyfat
RMSE Treino
RMSE Teste

ELMBIP

ELM

ANHNAPSOL

ANHNAPSO

0.
0
0. 1
01
0. 25
0
0. 15
01
7
0. 5
0
0. 2
02
0. 25
0
0. 25
02
7
0. 5
0
0. 3
03
0. 25
0
0. 35
03
7
0. 5
0
0. 4
04
0. 25
0
0. 45
04
7
0. 5
0
0. 5
05
0. 25
0
0. 55
05
7
0. 5
06

ANHNADE

Boston housing
RMSE Treino
RMSE Teste

ELMBIP

ELM

Os dados de entrada foram normalizados entre
0.9, 0.9 e os dados de sada dos quatro primeiros conjuntos entre 0.1, 0.9. Os dados de sada
referentes ao iCub sao transformados em radianos
e escalonados por 0.1. Cada conjunto foi dividido
em subconjuntos independentes de treinamento
e teste, com as seguintes respectivas quantidades
de amostras Autompg (273118), Boston housing
(354152), CPU (14363), Bodyfat (17676) e
iCub (391100).
Os parametros do metodo ANHNA foram
configurados em Qmax  100 e Qmin  2.
Para o algoritmo DE,   0.5, N P  100 e
M AXIGEN  300. Para o PSO, como sugerido em (Bratton and Kennedy, 2007), 1 
2  2.05 e   0.72984, bem como N P  100
M AXIGEN  300. Sobre as redes ELM, todas
as funcoes de ativacao sao tangentes hiperbolicas,
e para aquelas que nao tiveram a sua excitabilidade alterada, o limiar  1. Todas as implementacoes foram executadas com MATLAB em 50 rodadas independentes. Do melhor resultado encontrado apos essas 50 rodadas de todos os ANHNAs,
o numero de neuronios ocultos obtido e aplicado
na ELM tradicional e ELM com BIP.
As Figuras 2 e 3 apresentam a raiz quadrada
do erro quadratico medio (RMSE) da solucao
dada por cada ANHNA e pelas redes ELM tradi-

ANHNAPSOL

ANHNAPSO

0.
0
0. 25
02
7
0. 5
0
0. 3
03
0. 25
0
0. 35
03
7
0. 5
0
0. 4
04
0. 25
0
0. 45
04
7
0. 5
0
0. 5
05
0. 25
0
0. 55
05
7
0. 5
0
0. 6
06
0. 25
0
0. 65
06
7
0. 5
07

ANHNADE

CPU
ELMBIP

RMSE Treino
RMSE Teste

ELM

ANHNAPSOL

ANHNAPSO

0
0.
05
0.
1
0.
15
0.
2
0.
25
0.
3
0.
35
0.
4
0.
45
0.
5
0.
55
0.
6
0.
65
0.
7
0.
75
0.
8
0.
85
0.
9

ANHNADE

Figura 2 RMSE do treino e teste nas 50 repeticoes dos algoritmos apresentados. O numero de
neuronios usados para a rede ELM tradicional e
ELMBIP para cada conjunto e Auto-MPG (56),
Bodyfat (50), Boston (73) e CPU (47).

2503

Anais do XX Congresso Brasileiro de Automática
Belo Horizonte, MG, 20 a 24 de Setembro de 2014

Tabela 1 Os valores medios do RMSE e do numero de neuronios ocultos q com suas respectivas variancias
para cada algoritmo testado.
Auto-MPG
CPU
Treino

Teste

q

Treino

Teste

0.02240.0007 43.682.79
0.02240.0007 43.823.77
0.02260.0008 42.503.44

ANHNADE

0.04940.0012

0.04660.0004 53.663.83

0.01430.0005

ANHNAg-PSO

0.04900.0011

0.04670.0004

54.743.51

0.01420.0004

0.04660.0004 54.483.53

0.01430.0003

ANHNAl-PSO

0.04910.0011

ELM

0.04820.0009 0.05570.0048

ELMBIP

0.04910.0012

0.05530.003

56
56

0.01370.0004 0.23830.1744

47

0.01370.0004 0.23830.1744

47

Bodyfat
Treino

Teste

q

Boston Housing
q

Treino

Teste

q

0.03520.0025

0.03910.0013

58.84.82

ANHNADE

0.01300.001 0.01190.0004 49.324.59

ANHNAg-PSO

0.01350.001

0.01220.0003

50.243.77

0.03390.0025 0.03880.0016

ANHNAl-PSO

0.01370.001

0.01220.0003

504.65

0.03410.0026

ELM

0.01490.0014

0.01960.0022

0.03800.0025

ELMBIP

0.03030.0034

0.03940.0056

73
50

0.03850.0012 60.524.49
0.05580.0045 47

0.03610.0029

0.05270.0046

59.564.62

73

iCub
Treino

Teste

ANHNADE

0.02320.0006

0.02260.0006 44.045.36

ANHNAg-PSO

0.02230.0001

0.02340.0001

38.52.73

ANHNAl-PSO

0.02270.0004

0.02320.0004

34.985.84

ELM

0.02180.0001

0.02630.0009

ELMBIP

0.02170.0001 0.02720.0008

48
48

q

iCub
ELMBIP

na determinacao de um numero otimo de neuronios ocultos. Alem disso, os resultados referentes ao perodo de teste do ANHNA mostram valores mais proximos ao do treinamento, reduzindo
o efeito de sobreajuste.

RMSE Treino
RMSE Teste

ELM

ANHNAPSOL

Na Tabela 1 sao apresentadas as medias do
RMSE (treino e teste) e do numero de neuronios
ocultos encontrados, bem como suas respectivas
variancias. Na Tabela 2 sao apresentados os melhores resultados de cada algoritmo obtidos apos
as 50 repeticoes e os seus numeros de neuronios
ocultos correspondentes. E possvel observar que
todos os ANHNAs convergem para valores bem
proximos, como corroborado pelas Figuras 4 e 5,
que mostram a evolucao do RMSE na melhor execucao ao longo das 300 iteracoes. Ademais, a variancia dos resultados e bem menor em comparacao ao ELM e ELMBIP, como e possvel verificar
tanto pela Tabela 1 como nas Figuras 2 e 3.

ANHNAPSO

0.
0
0. 21
02
0. 15
0
0. 22
02
0. 25
0
0. 23
02
0. 35
0
0. 24
02
0. 45
0
0. 25
02
0. 55
0
0. 26
02
0. 65
0
0. 27
02
0. 75
0
0. 28
02
0. 85
0
0. 29
02
9
0. 5
03

ANHNADE

Figura 3 RMSE do treino e teste apos as 50 repeticoes de cada um dos algoritmos com o conjunto
iCub. O numero de neuronios usados para a rede
ELM tradicional e ELMBIP para o conjunto e
48.
cional ELMBIP nas 50 repeticoes repeticoes realizadas. Ainda que estas ultimas facam uso de
informacao privilegiada, uma vez que sao executadas apos o algoritmo ANHNA determinar um
numero otimo de neuronios ocultos, a diferenca no
desempenho e claramente visvel. Isto demonstra
a aptidao do metodo em otimizar a capacidade de
generalizacao da rede ao mesmo tempo que define
sua arquitetura, evitando longas buscas em grid

Por fim, as Figuras 6 e 7 apresentam a variacao do numero otimo de neuronios encontrados ao
final das 50 repeticoes de cada ANHNA testado.
Como um todo, ainda que o espaco de busca tenha sido compreendido entre 2 e 100 neuronios,
todas as solucoes obtidas giram em torno de 30 a
70 neuronios.

2504

Anais do XX Congresso Brasileiro de Automática
Belo Horizonte, MG, 20 a 24 de Setembro de 2014

Tabela 2 Os melhores valores RMSE de cada algoritmo testado e seus respectivos numeros de neuronios
ocultos q.
Auto-MPG
CPU
ANHNADE
ANHNAg-PSO
ANHNAl-PSO
ELM
ELMBIP

Treino
0.0453
0.0467
0.0467
0.0467
0.0454

ANHNADE
ANHNAg-PSO
ANHNAl-PSO
ELM
ELMBIP

Treino
0.0110
0.0110
0.0109
0.0123
0.0236

Teste
0.0455
0.0455
0.0454
0.0486
0.0509

q
51
53
56
56
56

Treino
0.0134
0.0133
0.0136
0.0127
0.0124

q
50
51
40
50
50

Treino
0.0302
0.0282
0.0271
0.0334
0.0316

Bodyfat
Teste
0.0107
0.0111
0.0113
0.0144
0.0306

Teste
0.0207
0.0210
0.0204
0.0583
0.0302

q
40
48
47
47
47
iCub

Boston Housing
Teste
0.0363
0.0334
0.0356
0.0454
0.0431

Treino
0.0220
0.0221
0.0222
0.0216
0.0214

q
69
73
67
73
73

Teste
0.0220
0.0232
0.0219
0.0245
0.0252

q
48
39
48
48
48

iCub

AutoMPG
0.025

0.05
ANHNADE
ANHNAPSO
ANHNAPSOL

0.0495

ANHNADE
ANHNAPSO
ANHNAPSOL

0.0245

0.049
0.024

0.0485

RMSE

RMSE

0.048
0.0475
0.047
0.0465

0.0235

0.023

0.0225

0.046
0.022
0.0455
0.045
0

50

100

150
Iteracoes

200

250

0.0215
0

300

50

100

150
Iteracoes

200

250

300

Bodyfat
0.0145

Figura 5 A evolucao do RMSE ao longo das iteracoes no melhor teste com o conjunto iCub.

ANHNADE
ANHNAPSO
ANHNAPSOL

0.014
0.0135

RMSE

0.013
0.0125

AutoMPG numero de neuronios ocultos
0.012
0.0115

ANHNAPSOL
0.011
0.0105
0

50

100

150
Iteracoes

200

250

300

ANHNAPSO
Boston
0.048
ANHNADE
ANHNAPSO
ANHNAPSOL

0.046

ANHNADE

0.044

RMSE

0.042

44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64

0.04
0.038

Bodyfat numero de neuronios ocultos
0.036
0.034
0.032
0

ANHNAPSOL
50

100

150
Iteracoes

200

250

300

ANHNAPSO
CPU
0.045
ANHNADE
ANHNAPSO
ANHNAPSOL

ANHNADE

0.04

36 38 40 42 44 46 48 50 52 54 56 58 60 62 64 66
RMSE

0.035

0.03

Figura 6 O numero de neuronios ocultos encontrados apos as 50 repeticoes dos ANHNAs para os
conjuntos Auto-MPG e Bodyfat.

0.025

0.02
0

50

100

150
Iteracoes

200

250

300

Figura 4 A evolucao do RMSE ao longo das iteracoes no melhor teste.

2505

Anais do XX Congresso Brasileiro de Automática
Belo Horizonte, MG, 20 a 24 de Setembro de 2014
Boston numero de neuronios ocultos

Agradecimentos

ANHNAPSOL

Os autores agradecem a CAPES e NUTEC (Fundacao Nucleo de Tecnologia Industrial do Ceara)
pelo apoio financeiro.

ANHNAPSO

Referencias
ANHNADE

48

50

52

54

56

58

60

62

64

66

68

70

72

Bache, K. and Lichman, M. (2013). UCI machine learning repository, httparchive.
ics.uci.eduml.

74

CPU numero de neuronios ocultos

Bratton, D. and Kennedy, J. (2007). Defining
a standard for particle_swarm_optimization,
Swarm Intelligence Symposium, 2007. SIS
2007. IEEE, pp. 120  127.

ANHNAPSOL

ANHNAPSO

Cao, J., Lin, Z. and Huang, G.-B. (2012). Selfadaptive evolutionary extreme learning machine, Neural Processing Letters 36(3) 285 
305.

ANHNADE

35

37

39

41

43

45

47

49

51

53

55

iCub numero de neuronios ocultos

Das, S., Abraham, A. and Konar, A. (2009). Metaheuristic Clustering, Studies in Computational Intelligence, Vol. 178, Springer.

ANHNAPSOL

ANHNAPSO

Freire, A., Lemme, A., Steil, J. and Barreto, G.
(2012). Aprendizado de coordenacao visomotora no comportamento de apontar em um espaco 3d, Anais do XIX Congresso Brasileiro
de Automatica (CBA 2012), pp. 48814886.

ANHNADE

22 24 26 28 30 32 34 36 38 40 42 44 46 48 50 52 54 56 58 60

Figura 7 O numero de neuronios ocultos encontrados apos as 50 repeticoes dos ANHNAs para os
conjuntos Boston Housing, CPU e iCub.
4

Han, F., Yao, H.-F. and Ling, Q.-H. (2013). An
improved evolutionary extreme learning machine based on particle_swarm_optimization,
Neurocomputing 116 87  93.

Conclusoes

Huang, G.-B., Wang, D. and Lan, Y. (2011). Extreme learning machines a survey, International Journal of Machine Learning and Cybernetics 2(2) 107  122.

Este trabalho apresentou um metodo hbrido baseado em metaheursticas, que otimiza a rede
ELM por meio da adaptacao, em um unico processo evolucionario, do numero de neuronios ocultos e suas respectivas excitabilidades. E baseado
em uma nova codificacao de cromossomos para algoritmos evolucionarios, tais como DE e PSO. Estes sofrem pequenas melhorias bem como o acrescimo de algumas novas regras, no entanto, fazem
uso de valores de parametros comuns na literatura.
Com esta configuracao basica, nos conseguimos gerar redes ELM com melhor capacidade de
generalizacao, em problemas reais de regressao,
bem como foi demonstrado que e possvel melhorar a performance dessa rede sem servir-se da otimizacao dos valores dos pesos de entrada e seus
vieses, cuja aleatoriedade e a principal caracterstica.
Como trabalhos futuros, avaliaremos a atuacao do ANHNA na presenca de outliers, por meio
da adicao de ferramentas automaticas de estatstica robusta diretamente no vetor de solucoes.
Tambem sera realizado o estudo do impacto da
escolha das estrategias e parametros de controle
nas metaheursticas utilizadas.

Neumann, K. and Steil, J. (2011). Batch intrinsic plasticity for extreme learning machines,
Artificial Neural Networks and Machine Learning - ICANN, pp. 339346.
Neumann, K. and Steil, J. (2013). Optimizing extreme learning machines via ridge regression
and batch intrinsic plasticity, Neurocomputing 102 23  30. Advances in Extreme Learning Machines (ELM 2011).
Silva, D. N. G., Pacifico, L. D. S. and Ludermir,
T. (2011). An evolutionary extreme learning
machine based on group search optimization,
2011 IEEE Congress on Evolutionary Computation (CEC), pp. 574  580.
Storn, R. and Price, K. (1997). Differential evolution - a simple and efficient heuristic for
global optimization over continuous spaces,
Journal of Global Optimization 11(4) 341 
359.

2506

Anais do XX Congresso Brasileiro de Automática
Belo Horizonte, MG, 20 a 24 de Setembro de 2014

Triesch, J. (2005). A gradient rule for the
plasticity of a neuronas intrinsic excitability, Int. Conf. on Artificial Neural Networks,
p. 65a79.
Yang, Y., Wang, Y. and Yuan, X. (2012). Bidirectional extreme learning machine for regression problem and its learning effectiveness, Neural Networks and Learning Systems,
IEEE Transactions on 23(9) 1498  1505.
Ye, Y., Squartini, S. and Piazza, F. (2010).
Incremental-based extreme learning machine
algorithms for time-variant neural networks,
in D.-S. Huang, Z. Zhao, V. Bevilacqua and
J. Figueroa (eds), Advanced Intelligent Computing Theories and Applications, Vol. 6215
of Lecture Notes in Computer Science, Springer Berlin Heidelberg, pp. 916.
Zhu, Q.-Y., Qin, A., Suganthan, P. and Huang,
G.-B. (2005). Evolutionary extreme learning
machine, Pattern Recognition 38(10) 1759 
1763.

2507