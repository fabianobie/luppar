XXI Congresso Brasileiro de Automática - CBA2016
UFES, Vitória - ES, 3 a 7 de outubro

MODELO HIBRIDO DE APRENDIZADO POR REFORCO E LOGICA FUZZY
APLICADO AO PROBLEMA DO CAIXEIRO VIAJANTE COM
REABASTECIMENTO
Andre Luiz Carvalho Ottoni, Erivelton Geraldo Nepomuceno, Marcos Santos de
Oliveira, Leonardo Bonato Felix


Programa de Pos-Graduacao em Engenharia Eletrica (UFSJCEFET-MG)
Universidade Federal de Sao Joao del-Rei
Sao Joao del-Rei, MG, Brasil

Emails andreottoni@ymail.com, nepomuceno@ufsj.edu.br, mso@ufsj.edu.br,
leobonato@ufv.br
Abstract This work has as main objective to develop a hybrid model of Reinforcement Learning and Fuzzy
Logic for application in the Traveling Salesman Problem with Refueling. For this, were implemented four RL
methods, with two of those with a fuzzy reinforcement function. The methods with fuzzy reward, Q-Fuzzy and
S-Fuzzy, showed the best results.
Keywords

Reinforcement Learning, Fuzzy Logic, Traveling Salesman Problem.

Resumo Este trabalho tem como objetivo principal desenvolver um modelo hbrido de Aprendizado por
Reforco e Logica Fuzzy para aplicacao no Problema do Caixeiro Viajante com Reabastecimento. Dessa forma,
foram implementados quatro metodos de AR, sendo que dois desses com uma funcao de recompensa fuzzy. Os
metodos com recompensa fuzzy, Q-Fuzzy e S-Fuzzy, apresentaram os melhores resultados.
Palavras-chave

.

Introducao

A administracao nos custos no transporte e
fundamental nos dias atuais. No Brasil, o custo
com combustvel e alto, e representa uma parcela significativa nos gastos de uma transportadora (Rodrigues Junior, 2011). Nesse aspecto,
vale ressaltar que existe uma variacao importante
entre os precos dos combustveis em uma mesma
regiao rodoviaria (Rodrigues Junior, 2011). Esse
fato pode ser verificado com pesquisas de precos
de combustveis no site da Agencia Nacional de
Petroleo (ANP)1 .
Os modelos de reabastecimento buscam otimizar o gasto com combustveis. Segundo Khuller et al. (2007), existem quatro importantes tipos
de problemas de reabastecimento 1) Reabastecimento com rota fixa, 2) Reabastecimento com rota
variavel, 3) Problema do Caixeiro Viajante (PCV)
considerando custo uniforme em cada ponto, 4)
PCV com o custo do combustvel variando em
cada ponto.
Na ultima decada, alguns trabalhos abordaram e desenvolveram metodos para problema de
reabastecimento de veculos. O artigo de Lin et al.
(2007) desenvolveu um algoritmo de tempo de execucao linear para encontrar a poltica otima de reabastecimento para um problema de rota fixa. Ja
o trabalho de Suzuki (2008), apresenta um modelo que leva em consideracao os custos operacionais do veculo ao longo da trajetoria. Em Suzuki
(2012), por sua vez, aborda o PCV com janela de
1 httpanp.gov.brpreco

ISSN 2525-8311

tempo e reabastecimento. Ja a pesquisa de Rodrigues Junior e Cruz (2013), utiliza uma modelagem
com rotas fixas para otimizar um estudo de caso
real de uma transportadora. Os autores de Silva
e Cruz (2014) apresentam um estudo referencial
teorico sobre alguns modelos de otimizacao para
o problema de reabastecimento de veculos.
Um metodo abordado para a resolucao_de_problemas de roteamento, como o Problema do Caixeiro Viajante, e o Aprendizado por Reforco (AR)
(Lima Junior et al., 2010 Ottoni et al., 2015). O
Aprendizado por Reforco e uma tecnica de aprendizado_de_maquina modelada por Processos de Decisao de Markov (Watkins e Dayan, 1992 Sutton e
Barto, 1998). No AR, o aprendizado se baseia nos
resultados de sucesso e fracasso de um agente interagindo em um ambiente. O agente aprende a tomar a decisao nos estados do ambiente, recebendo
recompensas por acoes corretas e penalidades por
acoes erradas.
Uma das vertentes dos estudos de aprendizado
por reforco sao os modelos hbridos. Nesses modelos, outras tecnicas inteligentes sao aplicadas em
conjunto com AR na tentativa de melhorar o desempenho do sistema (Bianchi, 2004). Nessa perspectiva algumas pesquisas adotam algoritmos de
aprendizado_por_reforco em conjunto com logica
fuzzy. O trabalho Zhou e Meng (2003) apresenta
um metodo fuzzy reinforcement learning (FRL)
para o controle de um robo bpede. Em Faria e
Romero (2002) por sua vez, propoe o metodo Rlearning, que incorpora logica_fuzzy ao algoritmo
R-learning. Ja os trabalhos de Bonarini et al.
(2009) e Derhami et al. (2010), levantam aspectos

1

XXI Congresso Brasileiro de Automática - CBA2016
UFES, Vitória - ES, 3 a 7 de outubro

da aplicacao fuzzy em conjunto com os algoritmos
Q-learning e SARSA, respectivamente.
Baseando-se nisso, este trabalho tem como
objetivo principal desenvolver um modelo hbrido
de Aprendizado por Reforco e Logica Fuzzy para
aplicacao no Problema do Caixeiro Viajante com
Reabastecimento. Dessa forma, foram implementados quatro metodos de AR, sendo que dois desses com uma funcao de recompensa fuzzy.
Este artigo esta organizado em secoes. Na
secao 2, sao definidos conceitos teoricos do aprendizado_por_reforco. Na secao 3, o problema do
caixeiro viajante com reabastecimento e descrito.
Ja na secao 4, e apresentada a modelagem do sistema hbrido. Uma descricao dos experimentos
realizados e apresentada na secao 5. A analise
dos resultados obtidos e apresentada na secao 6.
Finalmente, na secao 7 sao apresentadas as conclusoes.
2

Definir os parametros ,  e 
Para cada s,a inicialize Q(s,a)0
Observe o estado s
repita
Selecione a acao a usando a poltica
-gulosa
Execute a acao a
Receba a recompensa imediata r(s,a)
Observe o novo estado s
Atualize Q(s,a) com a Eq. (1)
s  s
ate o criterio_de_parada ser satisfeito
Algoritmo 1 Q-learning.
Definir os parametros ,  e 
Para cada s,a inicialize Q(s,a)0
Observe o estado s
Selecione a acao a usando a poltica
-gulosa
repita
Execute a acao a
Receba a recompensa imediata r(s,a)
Observe o novo estado s
Selecione a nova acao a usando a
poltica -gulosa
Atualize Q(s,a) com a Eq. (2)
s  s
a  a
ate o criterio_de_parada ser satisfeito

Aprendizado por Reforco

Alguns algoritmos de AR sao frequentemente adotados na literatura, como o Q-learning (Watkins e
Dayan, 1992) e o SARSA (Sutton e Barto, 1998),
apresentados em seguida.
O Q-learning (Watkins e Dayan, 1992) e um
dos algoritmos de Aprendizado por Reforco mais
conhecidos. O metodo (Algoritmo 1) se baseia na
atualizacao da matriz de aprendizado Q, a partir
da Equacao 1.

Algoritmo 2 SARSA.
Qt+1 

Qt (s, a) + r(s, a) + maxa Q(s )  Qt (s, a),

(1)

em que Q e a matriz de aprendizado, r(s, a) e o
reforco para a execucao da acao a no estado s, 
e a taxa de aprendizado e  e o fator de desconto.
A matriz Q possui dimensoes ns  na , em que
ns e o numero de estados do modelo e na e o numero de acoes que o agente pode executar. Assim,
para cada tomada_de_decisao (acao) em um determinado estado do ambiente, e retornado ao agente
um sinal r(s, a). Dessa forma, em cada instante
de tempo t, a matriz e atualizada em unico par
estado  acao.
Para a atualizacao de Q tambem e necessario
identificar o novo estado do ambiente (s ou st+1 ).
Ja que, o termo maxa Q(s ) e o maximo valor na
linha relativa ao novo estado s na matriz.
Ja o algoritmo SARSA (Sutton e Barto, 1998)
e uma modificacao do Q-learning. O SARSA (Algoritmo 2) nao adota a maximizacao das acoes do
Q-learning, assim a matriz de aprendizado e atualizada como na Equacao 2
Qt+1  Qt (s, a) + r(s, a) + Qt (s , a )  Qt (s, a), (2)

em que s  st+1 e a  at+1 .
Nos Algoritmos 1 e 2, a poltica denominada
  gulosa (ou, quase-guloso) e responsavel pelo
controle entre gula e aleatoriedade na selecao das
acoes (Sutton e Barto, 1998).

ISSN 2525-8311

3

Problema do Caixeiro Viajante com
Reabastecimento

O Problema do Caixeiro Viajante (PCV) visa encontrar o menor caminho entre um conjunto de
localidades, passando por cada cidade uma unica
vez. Ja o Problema do Caixeiro Viajante com Reabastecimento (PCVR) e uma variacao do PCV, e
tem por objetivo encontrar a rota para minimizar
o custo com combustvel (Khuller et al., 2007).
3.1

Instancias Minas24 e Minas30

Neste trabalho, sao propostas duas novas instancias para o PCVR Minas24 e Minas30. Cada uma
delas envolve um conjunto de cidades do Estado
de Minas Gerais (BRA). As instancias propostas
levam em consideracao as distancias entre cada localidade e o custo medio de combustvel em cada
uma delas. As distancias entre cada cidade foram calculadas a partir das posicoes em latitude
e longitude. Ja os precos dos combustveis foram
pesquisados em Novembro2015 no site da Agencia Nacional do Petroleo. Em seguida, as cidades
que compoem cada uma das instancias
 Minas24 Araguari, Belo Horizonte, Betim,

2

XXI Congresso Brasileiro de Automática - CBA2016
UFES, Vitória - ES, 3 a 7 de outubro

Campo Belo, Contagem, Formiga, Governador Valadares, Guaxupe, Itabira, Ituiutaba,
Juiz de Fora, Monte Carmelo, Montes Claros,
Oliveira, Patos de Minas, Pocos de Caldas,
Pouso Alegre, Sete Lagoas, Teofilo Otoni,
Tres Coracoes, Uberaba, Uberlandia, Una e
Varginha.
 Minas30 Localidades de Minas24 mais
Araxa, Barbacena, Divinopolis, Ipatinga, Lavras e Passos.
3.2

Caractersticas do Veculo

O veculo fictcio considerado neste trabalho busca
representar um automovel de carga de pequeno
porte, estilo caminhonete (Ex. S10), com consumo
de gasolina. Foi cosiderado uma taxa de consumo
constante de 10 kmlitro e capacidade maxima do
tanque de 80 litros.

 Preco (p) e o preco do combustvel na
localidade de chegada B.

Tabela 1 Valores dos reforcos.
Dist.
(d)
d  250
d  250
d  250
d  250
d  250
d  250
250<d 850
d > 850

4.1

Modelagem do Sistema Hbrido

Modelagem do Sistema de Aprendizado por
Reforco

A metodologia adotada e baseada em Ottoni et al.
(2015) e Lima Junior et al. (2010)
1. Definicao dos estados do ambiente Os estados foram definidos como todas as localidades em que o caixeiro viajante (agente) deve
acessar (Ottoni et al., 2015).
2. Definicao do conjunto finito de acoes que
o agente pode realizar Cada acao foi definida como sendo intencao de ir para outra localidade (estado) do problema (Ottoni
et al., 2015).
3. Definicao dos valores dos reforcos. Neste trabalho, os reforcos visam oferecer um retorno
ao caixeiro viajante de acordo com a distancia entre as localidades, o nvel de combustvel e o valor de combustvel nas cidades de
reabastecimento. Assim, os reforcos sao maiores para cidades mais proximas (d  250),
e mais negativos para cidades mais distantes
(250 < d  850 e d > 850). Alem disso, para
as cidades mais proximas, os reforcos variam
de acordo com o nvel do combustvel na cidade de partida (A) e o preco do combustvel
na cidade de chegada (B).
Foram definidos dois metodos de reforco sem
fuzzy e com fuzzy. O primeiro metodo de
reforco e apresentado na Tabela 1, em que
 Dist. (d) e a distancia entre duas localidades A e B. Sabendo que o caixeiro
vai de A a B.
 Nvel (n) e o nvel de combustvel no
tanque do automovel, na cidade de partida A.

ISSN 2525-8311

Preco
(p)
3,40  p < 4, 00
3,40  p < 3, 51
3,51  p < 3, 63
3,63  p < 3, 76
3,76  p < 3, 90
3,90  p < 4, 00
3,40  p < 4, 00
3,40  p < 4, 00

R
1300
2000
1300
650
0
-650
-1300
-2000

Ja a proxima secao descreve o metodo de modelagem da funcao recompensa com logica_fuzzy.
4.2

4

Nvel
(n)
n  0, 5
n < 0,5
n < 0,5
n < 0,5
n < 0,5
n < 0,5
0 n  1
0 n  1

Modelagem da Funcao de Recompensa com
Logica Fuzzy

Nesta secao, sera apresentada a metodologia adotada para a modelagem da funcao de recompensa
com logica_fuzzy.
Em seguida, sao apresentadas as oito regras
definidas no modelo
1. Se a (Distancia e Perto) e o (Nvel do Tanque
e Normal) entao o (Reforco e Alto)
2. Se a (Distancia e Perto) e o (Nvel do Tanque
e Baixo) e o (Preco do Combustvel e Muito
Baixo) entao o (Reforco e Muito Alto)
3. Se a (Distancia e Perto) e o (Nvel do Tanque
e Baixo) e o (Preco do Combustvel e Baixo)
entao o (Reforco e Alto)
4. Se a (Distancia e Perto) e o (Nvel do Tanque
e Baixo) e o (Preco do Combustvel e Medio)
entao o (Reforco e Meio Alto)
5. Se a (Distancia e Perto) e o (Nvel do Tanque
e Baixo) e o (Preco do Combustvel e Alto)
entao o (Reforco e Medio)
6. Se a (Distancia e Perto) e o (Nvel do Tanque
e Baixo) e o (Preco do Combustvel e Muito
Alto) entao o (Reforco e Meio Baixo)
7. Se a (Distancia e Media) entao o (Reforco e
Baixo)
8. Se a (Distancia e Grande) entao o (Reforco e
Muito Baixo).
Em seguida, foram definidas as funcoes de
pertinencia para as tres variaveis de entrada (ver
Figuras 1, 2 e 3) e para a variavel de sada (ver
Figura 4) do modelo. Ja a Figura 5 apresenta os
resultados da modelagem_fuzzy para a funcao de
recompensa.

3

XXI Congresso Brasileiro de Automática - CBA2016
UFES, Vitória - ES, 3 a 7 de outubro

Média

Grande
Muito Baixo Baixo
1

Meio Baixo

Médio

Meio Alto

Alto

Muito Alto

0.8
0.8
Grau de Pertinência

Grau de Pertinência

Perto
1

0.6
0.4
0.2

0.6

0.4

0.2

0
0

200

400

600
Distância

800

1000

0
-2000

-1500

-1000

-500

Figura 1 Funcoes de pertinencia da variavel de
entrada Distancia.

0
Reforço

500

1000

1500

2000

Figura 4 Funcoes de pertinencia da variavel de
sada Reforco.
Baixo

1

Normal

0.6
1000
500

0.4

Reforço

Grau de Pertinência

0.8

0.2

0
-500
-1000
-1500

0

1

0

0

0.2

0.4
0.6
Nível do Tanque

0.8

1

0.5

500
1000

0

Nível do Tanque

Distância

Figura 2 Funcoes de pertinencia da variavel de
entrada Nvel do Tanque.

Muito Baixo
1

Baixo

Médio

Alto

Muito Alto

 Q-Fuzzy Q-learning com funcao de recompensa fuzzy

0.8
Grau de Pertinência

Figura 5 Superfcie 3D que retrata o Reforco em
funcao da Distancia (km) e o Preco do Combustvel (R).

0.6

 SARSA com o primeiro metodo de reforco
0.4

 S-Fuzzy SARSA com a funcao de recompensa fuzzy.

0.2

0
3.4

3.5

3.6
3.7
3.8
Preço do Combustível

3.9

4

Figura 3 Funcoes de pertinencia da variavel de
entrada Preco do Combustvel.

Os parametros do aprendizado_por_reforco foram definidos de acordo com os experimentos e
analises de Ottoni et al. (2015)
 taxa de aprendizado decaindo n (s, a) 
1
1+visitasn (s,a) 
 fator de desconto ()  0,01

5

Experimentos Realizados

Para a solucao das instancias Minas24 e Minas30
foram analisados quatro metodos
 Q-learning com o primeiro metodo de reforco

ISSN 2525-8311

 poltica   greedy  0,01
Vale ressaltar que, cada simulacao foi padronizada em tres epocas (repeticoes) com 5000 episodios. Sendo que, cada episodio teve como resposta a distancia total percorrida pelo agente na

4

XXI Congresso Brasileiro de Automática - CBA2016
UFES, Vitória - ES, 3 a 7 de outubro

rota da instancia e o gasto em reais em reabastecimento na rota. Os experimentos foram realizados
R
no software M AT LAB 
.

4500
SARSA
S-Fuzzy

6

Analise dos Resultados

Em seguida, a analise de desempenho do sistema
de aprendizado_por_reforco para cada uma das
duas instancias estudadas Minas24 e Minas30.

Gasto (Reais)

4000

3500

3000

2500

6.1

Resultados com 24 localidades - Minas24
2000

Tabela 2 Melhores resultados para a instancia
Minas24.
Algoritmo Gasto Distancia
(R)
(km)
Q-learning
1592,5
4819,6
Q-Fuzzy
1250,2
3951,4
SARSA
1465,0
4488,0
S-Fuzzy
1085,2
3583,7

6.2

Resultados com 30 localidades - Minas30

Para a instancia Minas30, os melhores resultados
calculados foram R1265,30 e 3935,6 km. Esses
valores tambem foram encontrados quando adotado o metodo S-Fuzzy. A Tabela 3 apresenta os
resultados mnimos encontrados de acordo com o
algoritmo.
Tabela 3 Melhores resultados para a instancia
Minas30.
Algoritmo Gasto Distancia
(R)
(km)
Q-learning
1917,6
5748,2
Q-Fuzzy
1445,4
4349,0
SARSA
1897,8
5639,4
S-Fuzzy
1265,3
3935,6
Apesar dos resultados dos metodos SARSA e
S-Fuzzy serem semelhantes no incio do aprendizado (Figura 6), a Figura 7 revela que ao longo de
uma sequencia maior de episodios, o metodo que
adotou a funcao de recompensa fuzzy alcancou desempenho superior. Ja as Figuras 8 e 9, mostram
o comportamento do AR, a partir dos caminhos
encontrados para a instancia Minas30, para a melhor solucao com o algoritmo SARSA (Figura 8),
e melhor solucao para o metodo S-Fuzzy (Figura
9).

ISSN 2525-8311

0

20

40

60

80

100

Episódios

Figura 6 Media de Gasto em Reais versus o episodio de aprendizado, para a instancia Minas30 e
algoritmos algoritmos SARSA e S-Fuzzy. Visualizacao dos 100 episodios iniciais.

4500
SARSA
S-Fuzzy
4000

3500
Gasto (Reais)

Para a instancia Minas24, os melhores resultados
calculados foram R1085,2 e 3583,7 km. Esses
valores foram encontrados quando adotado o metodo S-Fuzzy. A Tabela 2 apresenta os resultados
mnimos encontrados de acordo com o algoritmo.

3000

2500

2000

1500

1000

0

1000

2000
3000
Episódios

4000

5000

Figura 7 Media de Gasto em Reais versus o episodio de aprendizado, para a instancia Minas30 e
algoritmos SARSA e S-Fuzzy.

7

Conclusao

Este trabalho teve como objetivo principal desenvolver um modelo hbrido de Aprendizado por Reforco e Logica Fuzzy para aplicacao no Problema
do Caixeiro Viajante com Reabastecimento.
Os metodos com recompensa fuzzy, Q-Fuzzy e
S-Fuzzy, apresentaram os melhores desempenhos.
Alem disso, vale ressaltar a superioridade dos algoritmos SARSA sobre o Q-learning nos resultados das duas instancias.
Em trabalhos futuros, espera-se analisar instancias maiores para esse problema, aumentando
o nvel de dificuldade de resolucao. Alem disso, serao analisados outras formas de recompensa para
o PCVR.

5

XXI Congresso Brasileiro de Automática - CBA2016
UFES, Vitória - ES, 3 a 7 de outubro

Khuller, S., Malekian, A. e Mestre, J. (2007).
Algorithms  ESA 2007 15th Annual European Symposium, Eilat, Israel, October 810, 2007. Proceedings, Springer Berlin Heidelberg, Berlin, Heidelberg, chapter To Fill
or Not to Fill The Gas Station Problem,
pp. 534545.

-16

-17

Latitude

-18

-19

-20

-21

-22

-23
-50

-49

-48

-47

-46
-45
Longitude

-44

-43

-42

-41

Figura 8 Caminho definido pelo algoritmo
SARSA para a instancia Minas30. Distancia percorrida igual a 5639,4 km e gasto de R1897,8.

Lima Junior, F. C., Neto, A. D. D. e Melo, J. D.
(2010). Hybrid Metaheuristics Using Reinforcement Learning Applied to Salesman Traveling Problem, Traveling Salesman Problem,
Theory and Applications, Prof. Donald Davendra (Ed.), InTech.
Lin, S. H., Gertsch, N. e Russell, J. (2007).
A linear-time algorithm for finding optimal
vehicle refueling policies., Operations Research Letters 35(3) 290296.

-16

Ottoni, A. L. C., Nepomuceno, E. G., Cordeiro,
L. T., Lamperti, R. D. e Oliveira, M. S.
(2015). Analise do desempenho do aprendizado_por_reforco na solucao do problema do
caixeiro viajante, XII SBAI - Simposio Brasileiro de Automacao Inteligente. pp. 4348.

-17

Latitude

-18

-19

-20

-21

-22

-23
-50

-49

-48

-47

-46
-45
Longitude

-44

-43

-42

-41

Rodrigues Junior, A. D. (2011). Um modelo de otimizacao da poltica de reabastecimento para
transportadores rodoviarios de carga, Masters thesis, Programa de Pos-Graduacao em
Engenharia Civil da UFES.

Figura 9 Caminho definido pelo algoritmo SFuzzy para a instancia Minas30. Distancia percorrida igual a 3935,6 km e gasto de R1265,3.

Rodrigues Junior, A. D. e Cruz, M. M. C. (2013).
A generic decision model of refueling policies a case study of a brazilian motor carrier,
Journal of Transport Literature 7(4) 822.

Agradecimentos

Silva, H. L. F. e Cruz, M. M. C. (2014). Uma
revisao de literatura sobre problemas de reabastecimento de veculos transportadores de
cargas, XXVIII Congresso de Pesquisa e Ensino em Transportes. pp. 19.

Agradecemos a CAPES, CNPq, FAPEMIG e
UFSJ pelo apoio.
Referencias
Bianchi, R. A. C. (2004). Uso de Heurstica para a
aceleracao do aprendizado_por_reforco., PhD
thesis, Escola Politecnica da Universidade de
Sao Paulo.
Bonarini, A., Lazaric, A., Montrone, F. e Restelli, M. (2009). Reinforcement distribution
in fuzzy q-learning, Fuzzy Sets and Systems
160(10) 1420  1443.
Derhami, V., Majd, V. J. e Ahmadabadi, M. N.
(2010). Exploration and exploitation balance
management in fuzzy reinforcement learning,
Fuzzy Sets and Systems 161(4) 578  595.
Faria, G. e Romero, R. A. F. (2002). Navegacao de robos_moveis utilizando aprendizado
por reforco e logica_fuzzy, Revista Controle
 Automacao 13(3) 219230.

ISSN 2525-8311

Sutton, R. e Barto, A. (1998). Reinforcement
Learning An Introduction, 1st edn, Cambridge, MA MIT Press.
Suzuki, Y. (2008). A generic model of motorcarrier fuel optimization, Naval Research Logistics 55(8) 737746.
Suzuki, Y. (2012). A decision support system of
vehicle routing and refueling for motor carriers with time-sensitive demands, Decision
Support Systems 54(1) 758767.
Watkins, C. J. e Dayan, P. (1992). Technical note
q-learning, Machine Learning .
Zhou, C. e Meng, Q. (2003). Dynamic balance of a biped robot using fuzzy reinforcement learning agents, Fuzzy Sets and Systems
134(1) 169  187.

6