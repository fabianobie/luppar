Anais do XX Congresso Brasileiro de Automática
Belo Horizonte, MG, 20 a 24 de Setembro de 2014

SELECAO DE NUMERO DE NEURONIOS DE ELMS BASEADA EM
DECOMPOSICAO DE VALORES SINGULARES TRUNCADO
L. D. Tavares, R. R. Saldanha, D. A. G. Vieira


Programa de Pos-Graduacao em Engenharia Eletrica - Universidade Federal de Minas Gerais
Av. Antonio Carlos 6627, 31270-901,
Belo Horizonte, MG, Brasil


Rua Professor Jose Vieira de Mendonca, 770
Parque Tecnologico de Belo Horizonte
Belo Horizonte, MG, 31310-260, Brasil

Emails tavares@dcc.ufmg.br, rodney@cpdee.ufmg.br, douglas.vieira@enacom.com.br
Abstract The Extreme Learning Machine (ELM) is a recent training method for feedforward neural networks. Its main advantage is a faster and simpler training procedure when it is compared with traditional global
search optimization methods. It is achieved by using the least square solution for the output layer and random
initialization for hidden layer. In this way, only one solution is attained. Anyway, selecting the number of hidden
neurons is still an open problem, and, in most cases, the choice is made empirically. This paper presents a simple
technique based on singular value decomposition (SVD) which is able to indicate the number of neurons in the
hidden layer that favors low training error and the low complexity of the machine.
Keywords

Extreme Learning Machine, Singular Value Decomposition, Hidden layer neurons

Resumo A Maquina de Aprendizagem Extrema (Extreme Learning Machine - ELM) e uma tecnica recente
de treinamento para redes_neurais de unica camada. Sua principal vantagem e seu rapido e simples processo de
treinamento quando comparado aos tradicionais metodos de otimizacao. Isto e obtido utilizando solucoes por
quadrados mnimos para os pesos da camada de sada e inicializando de maneira aleatoria os pesos dos neuronios
da . De qualquer forma, a selecao do numero de neuronios da camada escondida continua sendo um
problema em aberto, sendo que, na maior parte dos casos, a escolha se da de forma emprica. O presente trabalho
apresenta uma tecnica simples baseada em decomposicao em valores singulares (SVD) que e capaz de indicar o
numero de neuronios da camada escondida que favoreca o baixo erro de treinamento e a baixa complexidade da
maquina.
Palavras-chave


1

Maquina de Aprendizagem Extrema, Decomposicao em Valores Singulares, Neuronios na

Introducao

Dentre as principais vantagens da ELM,
destacam-se o treinamento e extremamente rapido (quando comparado com os metodos tradicionais de aprendizagem), requer um numero
menor de parametros para ser ajustado e o resultado apresenta uma boa generalizacao (Huang
et al., 2012). A ELM tem sido utilizada em diversas aplicacoes, como predicao de series_temporais (Singh and Balasundaram, 2007), classificacao de texto (Liu et al., 2008), reconhecimento
de padroes (Liang et al., 2006), selecao de variaveis e caractersticas (Mateo and Lendasse, 2008),
entre outras. Uma lista de aplicacoes e o estado
da arte pode ser encontrado em (Rajesh and Parkash, 2011).

Desde o trabalho pioneiro de Huang et al. (Huang
et al., 2004) (Huang et al., 2006) e (Huang
et al., 2011), a Maquina de Aprendizagem Extrema (Extreme Learning Machine - ELM) tem
sido amplamente discutida e utilizada em diversas areas. A ELM e um metodo de aprendizagem_de_maquina que simplifica as etapas de inicializacao e treinamento para redes_neurais de
unica camada (Single Layer Feedforward Neural
Network - SLFN). A ELM se diferencia dos metodos tradicionais de aprendizagem por selecionar de forma aleatoria os pesos dos neuronios da
 e, posteriormente, determinar de
forma analtica, utilizando o metodo de quadrados mnimos, os pesos da camada de sada. Alem
disso, nao ha a necessidade de algoritmos baseados
em gradiente_descendente, como, por exemplo, o
algoritmo de retropropagacao ou qualquer outro
metodo de busca global. A capacidade de uma
SLFN ser um aproximador universal de funcoes
foi provada em 1989 por Cybenco(Cybenko, 1989)
and Funahashi(Funahashi, 1989). Recentemente,
Huang and Babri (Huang and Babri, 1998) demonstraram a sua capacidade maxima de aprendizagem.

O treinamento da ELM utiliza a abordagem
de quadrados mnimos, que requer apenas simples
manipulacoes algebricas em matrizes, no entanto,
ainda e necessario calcular no mnimo uma inversa
(ou pseudo-inversa). Em (Horata et al., 2011)
quatro metodos sao comparados em termos de
complexidade computacional, tempo de processamento e precisao, sao eles (i) equacao normal, (ii)
decomposicao em valores singulares (SVD), (iii)
pseudo-inversa Moore-Penrose e (iv) decomposicao QR. E importante ressaltar que o trabalho

1019

Anais do XX Congresso Brasileiro de Automática
Belo Horizonte, MG, 20 a 24 de Setembro de 2014

nao apresenta conclusao alguma relacionada a influencia do metodo de inversao com a capacidade
de generalizacao ou abordagem multiobjetivo.
Apesar de simplificar o processo de aprendizagem, a escolha do numero de neuronios da
camada escondida ainda continua sendo um problema aberto. Em (Lan et al., 2010) e apresentado
um metodo que seleciona o subconjunto de neuronios que possuam maior correlacao com o mapeamento entrada-sada apresentado durante a fase
de treinamento. Os neuronios com menor correlacao sao descartados. Ja em (Martnez-Martnez
et al., 2011) o numero de neuronios e determinado
a partir da regularizacao l 1 e l 2 (conhecida como
regularizacao Elastic-net). Nesse caso, dois parametros de regularizacao sao necessarios, sendo o
primeiro para o termo linear (l 1) e o segundo para
o nao linear (l 2), atraves da norma euclidiana. A
regularizacao do termo linear e obtida atraves do
metodo_de_pontos_interiores.
Em ambos os trabalhos, e possvel se observar
o enorme esforco computacional necessario para
se obter o numero de neuronios otimo. O presente
trabalho demonstra que e possvel se encontrar o
numero de neuronios adequado para uma SLFNN
de forma que seja factvel controlar a complexidade da maquina, medida atraves da norma dos
pesos de sada, e o risco emprico, medido atraves
da raiz quadrada do erro quadratico medio (RootMean-Square Error - RMSE).
Dessa forma, o objetivo do presente trabalho
e propor um metodo de determinacao de numero
de neuronios da  de uma ELM de
forma a controlar, ao mesmo tempo, a complexidade e erro. Para isto sera utilizada a Decomposicao em Valores Singulares Truncada (Truncated Singular Value Decomposition - TSVD). Conforme discutido em (Vieira et al., 2008), a aprendizagem_de_maquina e um problema multiobjetivo
onde deve existir um balanco entre o risco emprico e a complexidade. Dessa forma, um conjunto
de solucoes, chamado fronteira Pareto Otimo, e
desejado.
O restante do trabalho esta organizado como
se segue a Secao 2 apresenta os principais elementos relacionados a ELM e sua relacao com a
SVD. Posteriormente, na Secao 3 e apresentado
o metodo proposto para determinacao do numero
otimo de neuronios da  para uma
ELM. O desempenho do metodo proposto e avaliado na Secao 4. Finalmente, na Secao 5 sao apresentadas as discussoes e trabalhos futuros.
2

fica a transposicao do vetor ou matriz, e ti 
ti1 , ti2 , . . . , tim 0  Rm sao as respostas reais (ou
desejadas) do sistema ou ambiente que se deseja
aprender. Considere ainda h como o numero de
neuronios na  e f () como a funcao de ativacao, uma rede_neural de camada unica
(SLFN) e modelada conforme

oi 

h
X

j f (x0i wj + bj ), i  1, 2, . . . , N

(1)

j1

onde oi  oi1 , oi2 , . . . , oim 0  Rm sao
as respostas encontradas pela SLFN, wj 
wj1 , wj2 , . . . , wjh 0  Rh e o vetor de pesos que
conecta a entrada e os neuronios da camada escondida, j  j1 , j2 , . . . , jm 0  Rm sao os pesos que conectam a camada escondida com a sada
da SLFN, e bj e o limiar do j-esimo neuronio da
camada escondida.
E possvel aproximar oi P
de ti para todos os
N
N exemplos, de forma que i1 kti  oi k  0,
assumindo que existem parametros w o suficiente,
de forma que

ti 

h
X

j f (x0i wj + bj ), i  1, 2, . . . , N

(2)

j1

com erro medio zero.
Na forma matricial compacta temos
H  T

(3)

onde


f (x01 w1 + b1 ) . . . f (x0N wh + bh )


..
..
N h
H
R
.
...
.
f (x01 w1 + b1 ) . . . f (x0N wh + bh )
(4)
  1 , . . . , h 0 e T  t1 , . . . , tN 0

(5)

Conforme mencionado, a ELM inicia aleatoriamente os valores de w e b. Dessa forma, o objetivo e avaliar o valor de  de forma que
min kT  Hkp


(6)

onde p indica o tipo de norma a ser utilizada (1, 2,
, . . . ). Para p  2 o valor de  pode ser encontrado utilizando a solucao por quadrados mnimos,
conforme

Maquina de aprendizagem extrema ELM

  (T  H)0 (T  H)
 T0 T  T0 H  H0  0 T + H0  0 H

Considere um conjunto de N exemplos distintos
na forma (xi , ti ) onde xi  xi1 , xi2 , . . . , xin 0 
Rn sao as entradas de um sistema real, com
i  1, 2, ...N , o smbolo apostrofo (0 ) signi-

Resolvendo

1020




 0 obtem-se

(7)

Anais do XX Congresso Brasileiro de Automática
Belo Horizonte, MG, 20 a 24 de Setembro de 2014

3

 (T0 H)0  (H0 T) + (H0 H + H0 H)

(8a)
H0 H  H0 T

O metodo TSVD pode ser visto como uma tecnica de reducao de dimensao equivalente a analise
de componentes_principais (Principal Component
Analysis - PCA) (Jackson, 2003). Dessa forma,
e possvel analisar o espaco gerado pela camada
oculta, de forma que estejam presentes apenas as
componentes_principais, de forma a favorecer os
criterios de complexidade e erro desejados.
O metodo supoe que o espaco gerado pela  seja inicialmente de altssima dimensao, no entanto que algumas delas nao sao necessarias, podendo assim serem dispensadas. Apesar
de ser um metodo de reducao do espaco oculto, o
mesmo princpio pode ser aplicado, sem perda de
generalizacao, na forma incremento.
Em sua forma incremental, o espaco gerado
pela  e de baixa dimensao, e, a cada
iteracao, sao acrescentados neuronios ocultos ate
que os criterios de complexidade e erro estejam
equilibrados. Esta segunda abordagem, no entanto, nao esta contemplada no presente trabalho.
A secao a seguir ira detalhar o metodo proposto.

(8b)

+

H T

(8c)

onde a equacao (8b) e chamada de equacao normal, H+ e a pseudoinversa de H.
E importante observar que a inversa de (H0 H)
pode nao existir, uma vez que a matriz pode nao
ter rank completo ou alto numero de condicionamento. Nesses casos, e possvel realizar uma aproximacao da inversa de H, sendo um dos metodos
a SVD. A proxima secao explicara, brevemente, o
metodo para se obter a pseudoinversa.
2.1

Pseudoinversa usando SVD

Considere a matriz H  RN h com rank H  r.
Entao H pode ser fatorada como (Golub and Van
Loan, 1996)
H  UV0

(9)

N N

onde U  R
e uma matriz quadrada ortonormal chamada vetor singular esquerdo composta
pelos autovetores de (HH0 ), Vhh e uma matriz quadrada ortonormal chamada vetor singular direito composto pelos autovetores de (H0 H)
e   diag(1 , . . . , r )  RN h onde i sao os
valores singulares composta pelos autovalores de
(H0 H) e (HH0 ), com 1  2  . . . r  0. A 
e pseudo-diagonal uma vez que pode nao ser quadrada. Existem h valores singulares caso N > h e
existem N valores singulares no caso contrario.
Atraves de uma simples manipulacao algebrica, e possvel se obter a pseudoinversa de H
conforme
H+  (UV0 )+  V1 U0

3.1

1. UV0   svd(H)
2. Para cada i  h, h  1, ..., 2, 1
(a) i  0
(b)   V1 U0 y
(c) ye  UV0 
(d) ri  rmse(t, ye )

(10)

(e) ni  kk2
onde ye e a resposta encontrada pela ELM,
rmse(, ) e a funcao que calcula a raiz quadrada
do erro quadratico medio entre a sada desejada t
e a sada calculada ye , e kk2 e a norma euclidiana.
E possvel observar que o metodo proposto
gera diversas solucoes. O conjunto de solucoes
formado e chamado de solucoes Pareto. No caso
nao e necessario observar as solucoes dominadas e
nao dominadas, uma vez que a solucao desejada e
aquela que esta mais proxima da origem.
Ao final do processo, deve ser gerada uma matriz na forma P  r0 , n0 . Como necessariamente
os vetores r e n sao formados por valores positivos, a matriz P tambem sera. Considerando cada
linha de P como o par de solucao erro e norma,
para selecionar a solucao que equilibre os criterios, basta encontrar o ndice da linha Pi do qual
possua a menor norma euclidiana, na forma

(11)

onde


se i > 
i  1, 2, ..., r
caso contrario
(12)
onde  > 0 e um limiar escolhido pelo usuario,
1
 e a inversa de  baseado no parametro .
Este tipo de operacao e chamada de decomposicao
em valores singulares truncado (Truncated Singular Value Decomposition - TSVD), uma vez que o
processo elimina os valores pequenos e mantem os
bons.
1
 

Detalhamento do metodo

Considerando o caso N > h, onde existem h valores singulares, o metodo pode ser detalhado da
seguinte forma

onde 1  diag(11 , . . . , 1r ).
Conforme mencionado, a pseudoinversa de H
pode nao existir caso H seja singular ou malcondicionada. Nesse caso, a pseudoinversa pode
ser aproximada eliminando os valores singulares
muito pequenos, como
0
H+  V1
 U

Metodo proposto

1i
0

1021

Anais do XX Congresso Brasileiro de Automática
Belo Horizonte, MG, 20 a 24 de Setembro de 2014

h  minh (P)

solucao encontrada pelo metodo proposto (ponto
inferior esquerdo) e a solucao onde ha apenas 1
neuronio na camada escondida (ponto inferior direito). Vale a pena ressaltar que foram mapeados
150 solucoes e o tempo gasto para a geracao de
todas as solucoes foi de apenas 9.03 segundos, ou
seja, aproximadamente 0.06 segundos por solucao.

(13)

onde minh () e a funcao que retorna o ndice da
linha que possui a menor norma euclidiana da matriz P, e h e o numero de neuronios que favorece
o equilbrio entre complexidade e erro.
E possvel perceber tambem que o metodo
proposto calcula as matrizes U e V0 apenas uma
vez. Com isso e possvel mapear todas as solucoes
pareto, baseadas simplesmente nos valores sigulares do espaco oculto.
O metodo proposto, por utilizar a abordagem TSVD sera chamado, daqui em diante, de
Truncated-Extreme Learning Machine (T-ELM).
4

Experimentos realizados

A presente Secao avalia o desempenho do metodo proposto.
Todas as simulacoes foram
realizadas no ambiente MATLAB versao 7.12
(R2011a), executados em um Intel(R) Core(TM)
i3, 2.40GHz CPU, sob o sistema operacional
Ubuntu 13.10. Em todos os casos, foi utilizada
a funcao de ativacao, para os neuronios ocultos, do tipo logstica sigmoide na forma g(x) 
1(1 + exp(x)). Um caso sintetico e 10 benchmarks foram escolhidos para os experimentos. Os benchmarks foram selecionados da base
UCI Machine Learning Repository(Bache and Lichman, 2014), para a tarefa de regressao de dados. Todas as bases dos benchmarks tiveram sua
entrada normalizada no intervalo 0, 1 (mesmo
para caractersticas discretas), e suas sadas normalizadas no intervalo 1, 1.
Cada experimento foi executado 50 vezes, e
o resultado apresentado representa a media e o
desvio padrao.
4.1

Figura 1 Solucoes Pareto geradas. O ponto superior, em destaque, representa a solucao encontrada
para a ELM que possui 150 neuronios na camada
escondida, o ponto inferior esquerdo, em destaque,
representa a solucao encontrada para a ELM que
possui 38 neuronios na camada escondida, e, finalmente, o ponto inferior direito, a solucao encontrada para a ELM que possui 1 neuronio apenas
na camada escondida.
A figura 2 apresenta a solucao para a ELM
que possui os 150 neuronios na camada escondida.
E possvel perceber, neste caso, que o resultado
encontrado esta sobre ajustado, isto e, alem do
modelo o rudo tambem e aprendido, o que nao e
um efeito desejado.

Base artificial aproximacao da funcao Sinc

Neste experimento, o metodo proposto sera utilizado para aproximar a funcao Sinc, amplamente
utilizada na literatura para proposito de regressao,
na forma

ti 

sin(xi )
xi

1

xi 
6 0
xi  0

(14)

As bases de treinamento e teste possuem 500
pontos cada uma, criados de forma aleatoria em
uma distribuicao uniforme no intervalo 10, 10.
Um rudo com distribuicao normal, com media 0
e variancia0.1 foi adicionado em todos os pontos
de sada. Para este experimento, a ELM possui,
inicialmente 150 neuronios na , escolhido arbitrariamente.
As figuras de 1 a 4 apresentam o resultado do
experimento para 1 realizacao. A figura 1 apresenta as solucoes Pareto encontradas. Em destaque estao a solucao inicial (ponto superior), a

Figura 2 Resultado da regressao da funcao sinc,
onde a ELM possui os 150 neuronios na camada
escondida.
A figura 3 apresenta a solucao para a ELM

1022

Anais do XX Congresso Brasileiro de Automática
Belo Horizonte, MG, 20 a 24 de Setembro de 2014

que possui os 38 neuronios na camada escondida
(resultado encontrado pelo metodo proposto). E
possvel perceber, neste caso, que o resultado encontrado se ajusta de forma coerente com modelo,
eliminando a parte que possui rudo.

h  150
h  38
h  1

RMSE
Treinamento
Teste
0.0948(0.0272)
0.0998(0.0236)
0.0729(0.0054)
0.0781(0.0054)
0.1987(0.0114)
0.2006(0.0089)

kk2
7.64  1013
2.69  1013
0.0848

Tabela 1 Media e desvio padrao (entre parenteses) para o experimento com o caso sintetico.
4.2

Base de dados benchmarks

Foram escolhidos 10 conjuntos de dados benchmarks bem conhecidos na literatura, selecionados
da base de dados UCI Machine Learning Repository (Bache and Lichman, 2014), sendo todas
para o proposito de regressao. O numero de observacoes e atributos de cada conjunto de dados
sao apresentados na tabela 2.
A fim de verificar a eficacia do metodo proposto, o resultado encontrado sera comparado
com os trabalhos (Martnez-Martnez et al., 2011)
e (Lan et al., 2010). Ambos possuem o proposito de encontrar o numero otimo de neuronios na
camada escondida.
A tabela 3 apresenta a comparacao entre TELM e Martinez et. al. (Martnez-Martnez
et al., 2011) para a etapa de validacao. No trabalho (Martnez-Martnez et al., 2011) sao apresentados resultados para tres criterios de regularizacao (i) norma-l 1, (i) norma-l 2 e Elastic-net).
A comparacao sera realizada contra o melhor dos
tres resultados. Infelizmente os autores nao apresentam resultados para todas as 10 bases de dados
testadas.
A tabela 4 apresenta a comparacao entre TELM e Yuan et. al. (Lan et al., 2010) para a etapa
de validacao. Infelizmente, novamente, os autores
nao apresentam resultados para todas as 10 bases
de dados testadas.
E possvel perceber que em ambos os resultados (tabelas 3 e 4) que o metodo proposto T-ELM
foi capaz de gerar melhores resultados em termos
de RMSE e para, alguns casos, a estrutura obtida
possui menos neuronios na camada escondida. Os
autores, em ambos os casos, nao apresentaram os
dados da norma dos pesos da camada de sada, o
que impossibilitou uma analise desse criterio.

Figura 3 Resultado da regressao da funcao sinc,
onde a ELM possui os 38 neuronios na camada
escondida.
Por fim, a figura 4 apresenta a solucao para
a ELM que possui apenas 1 neuronio na camada
escondida. E possvel perceber, neste caso, que a
ELM nao e capaz de aprender nada sobre o modelo, uma vez que nao ha regressores no espaco
oculto o suficiente.

5
Figura 4 Resultado da regressao da funcao sinc,
onde a ELM possui os 1 neuronio na camada escondida.

Conclusoes e trabalho futuros

O presente trabalho apresentou uma tecnica simples e eficiente de selecao do numero de neuronios
na  para redes_neurais de unica camada. A tecnica e baseada em decomposicao em
valores singulares truncado, isto e, os valores singulares do espaco gerado pela  que
sao considerados pequenos sao removidos.
A tecnica e equivalente a analise_de_componentes_principais onde apenas as componentes de
maior importancia sao mantidas no modelo. Para
o experimento foram utilizadas uma funcao sintetica e dez funcoes de benchmarks, obtidas da

A tabela 1 apresenta o resultado para as etapas de treinamento e teste para a base sintetica.
E possvel perceber que o resultado encontrado
pelo metodo proposto e capaz de gerar uma solucao media que, tanto para a etapa de treinamento
quanto para a etapa de teste, possui um RMSE e
kk2 menores que a solucao com maior numero de
neuronios.

1023

Anais do XX Congresso Brasileiro de Automática
Belo Horizonte, MG, 20 a 24 de Setembro de 2014
Base de dados
Abalone
Auto price
Bank
California housing
Census (house 8L)
Computer activity
Delta ailerons
Delta elevators
Servo
Triazines

 Obserservacoes
Treinamento
Testing
2, 000
2, 177
80
79
4, 500
3, 692
8, 000
12, 460
10, 000
12, 784
4, 000
4, 192
3, 000
4, 19
4, 000
5, 517
80
87
100
86

 Atributos
Contnuos
Discretos
7
1
14
1
8
0
8
0
8
0
8
0
6
0
6
0
0
4
60
0

Tabela 2 Especificacao das bases de benchmark.
Base de dados
Abalone
Auto price
Bank
California housing
Census (house 8L)
Computer activity
Delta ailerons
Delta elevators
Servo
Triazines

h
99
78
58
93
70
98
103
109
70
98

T-ELM
RMSE
0.0746(0.0001)
0.0065(0.0001)
0.0415(0.0006)
0.1183(0.0004)
0.0103(0.0026)
0.0062()
0.0339()
0.0156()
0.0082(0.0031)
0.0381()

Martinez et. al. 2011
h
RMSE
21
0.6499(0.003)
21
0.3572(0.036)
400
0.5071(0.004)
202
0.6048(0.003)
253
0.1905(0.004)
40
0.5289(0.003)
100
0.6036(0.002)
2
1.0281(0.023)

Tabela 3 Comparacao do RMSE entre T-ELM and Martinez et. al. Valores de desvio padrao inferiores
a 0.0001 foram desprezados.
Base de dados
Abalone
Auto price
Bank
California housing
Census (house 8L)
Computer activity
Delta ailerons
Delta elevators
Servo
Triazines

h
99
78
58
93
70
98
103
109
70
98

T-ELM
RMSE
0.0746(0.0001)
0.0065(0.0001)
0.0415(0.0006)
0.1183(0.0004)
0.0103(0.0026)
0.0062()
0.0339()
0.0156()
0.0082(0.0031)
0.0381()

Yuan et. al. 2010
h
RMSE
20
0.0771(0.0014)
85
0.0442(0.0009)
38
0.1337(0.0019)
89
0.0685(0.0031)
13
0.0394(0.0007)
11
0.0537(0.0005)
16
0.1214(0.0177)
-

Tabela 4 Comparacao do RMSE entre T-ELM and Yuan et. al. Valores de desvio padrao inferiores a
0.0001 foram desprezados.
UCI Machine Learning Repository (Bache and Lichman, 2014), sendo todas para o proposito de regressao.
Durante os experimentos foi possvel observar
que o metodo proposto foi capaz de selecionar o
numero de neuronios h de forma que tanto o criterio de risco emprico RMSE quanto o criterio de
complexidade, medida pela norma dos pesos da
camada de sada kk2 foram mantidos sob controle.
Pode-se perceber que o metodo T-ELM e um
metodo de baixa complexidade computacional capaz de atender de forma multiobjetivo o problema
de treinamento de uma rede SLFNN.
Como trabalho futuro pretendido, destaca-se
a demonstracao formal que o metodo T-ELM e
capaz de gerar o espaco de mnima norma e erro
em conjunto, isto e, que a solucao encontrada pelo
metodo e Pareto otimo dominante.

Cybenko, G. (1989). Approximation by superpositions of a sigmoidal function, Mathematics of Control, Signals, and Systems (MCSS)
2(4) 303314.

Agradecimentos

Huang, G.-B. and Babri, H. A. (1998). Upper
bounds on the number of hidden neurons in
feedforward networks with arbitrary bounded
nonlinear activation functions., IEEE Transactions on Neural Networks 9(1) 224229.

Funahashi, K. (1989). On the Approximate Realization of Continuous Mappings by Neural
Networks, Neural Network 2(3) 183192.
Golub, G. and Van Loan, C. (1996). Matrix
Computations, Johns Hopkins Studies in the
Mathematical Sciences, Johns Hopkins University Press.
Horata, P., Chiewchanwattana, S. and Sunat, K.
(2011). A comparative study of pseudoinverse computing for the extreme learning
machine classifier, 3rd International Conference on Data Mining and Intelligent Information Technology Applications (ICMiA),
pp. 40  45.

Os autores gostariam de agradecer a CAPES,
CNPq e FAPEMIG pela ajuda financeira.
Referencias

Huang, G.-B., Wang, D. and Lan, Y. (2011). Extreme learning machines a survey, International Journal of Machine Learning and Cybernetics 2(2) 107122.

Bache, K. and Lichman, M. (2014). UCI Machine Learning Repository. httparchive.
ics.uci.eduml.

1024

Anais do XX Congresso Brasileiro de Automática
Belo Horizonte, MG, 20 a 24 de Setembro de 2014

Huang, G.-B., Zhou, H., Ding, X. and Zhang, R.
(2012). Extreme Learning Machine for Regression and Multiclass Classification., IEEE
Transactions on Systems, Man, and Cybernetics, Part B 42(2) 513529.

Approach to the Machine Learning Structural Risk Minimization Problem, IEEE Transactions on Neural Networks 19(8) 1415
1430.

Huang, G.-B., Zhu, Q.-Y. and Siew, C.-K. (2004).
Extreme learning machine a new learning
scheme of feedforward neural networks, Neural Networks, 2004. Proceedings. 2004 IEEE
International Joint Conference on, Vol. 2,
pp. 985990.
Huang, G.-B., Zhu, Q.-Y. and Siew, C.-K. (2006).
Extreme learning machine Theory and applications, Neurocomputing 70(1-3) 489501.
Jackson, J. (2003). A Users Guide to Principal
Components, Wiley Series in Probability and
Statistics, Wiley.
Lan, Y., Soh, Y. C. and Huang, G.-B. (2010).
Constructive Hidden Nodes Selection of Extreme Learning Machine for Regression, Neurocomput. 73(16-18) 31913199.
Liang, N., Saratchandran, P., Huang, G. and Sundararajan, N. (2006). Classification of mental tasks from EEG signals using extreme learning machine, INTERNATIONAL JOURNAL OF NEURAL SYSTEMS 16(1) 2938.
Liu, D., Zhang, H. and Hu, S. (2008). Neural
networks Algorithms and applications, Neurocomputing 71 471473.
Martnez-Martnez, J. M., Escandell-Montero, P.,
Soria-Olivas, E., Martn-Guerrero, J. D.,
Magdalena-Benedito, R. and Gomez-Sanchis,
J. (2011). Regularized extreme learning machine for regression problems, Neurocomputing 74(17) 37163721.
Mateo, F. and Lendasse, A. (2008). A variable
selection approach based on the Delta Test
for Extreme Learning Machine models, European Symposium on Time Series Prediction.
Rajesh, R. and Parkash, J. S. (2011). Extreme learning machine - A review and State-of-art,
Internationa Journal of Wisdom Based Computing 1(1) 3549.
Singh, R. and Balasundaram, S. (2007). Application of Extreme Learning Machine Method
for Time Series Analysis, International Journal of Intelligente Technology 2(4) 256262.
Vieira, D. A. G., Takahashi, R. H. C., Palade,
V., Vasconcelos, J. A. and Caminhas, W. M.
(2008). The Q-Norm Complexity Measure
and the Minimum Gradient Method A Novel

1025