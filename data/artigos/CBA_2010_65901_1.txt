IMPROVED SIGNAL REPRESENTATION FOR EVENT DETECTION IN REMOTE HEALTH
CARE THROUGH PSYCHOANALYTICAL MASKING

JUGURTA MONTALVÃO
Universidade Federal de Sergipe - UFS (Brazil), Campus de São Cristóvão - Sergipe, CEP. 49100-000 , Brazil
DAN ISTRATE
ESIGETEL, Rue du Port de Valvins, 77210 Fontainebleau-Avon, France
JEROME BOUDY
Institut Télécom Télécom SudParis, 9 Rue Charles Fourier, 91011, Evry, Paris, France
E-mails jmontalvao@ufs.br, dan.istrate@esigetel.fr, Jerome.Boudy@itsudparis.eu
Abstract Sound analysis through Mel Frequency Cepstral Coefficient (MFCC), along with Gaussian Mixture Models
(GMM), are not rarely deployed for detection of relevant events for remote health care. However, in spite of good recognition ratios obtained with MFCC, it is known that Cepstral Coefficients are aimed at rough spectral shape estimation  thus highlighting
resonance effects useful in speech recognition. Thus a relevant question may be raised is MFCC analysis also good for most signals of interest in remote health care? In this work, we address this point by comparing recognition performances with real signals recorded in remote health care scenarios and two kinds of short-time analysis one based on MFCC and another adapted
from the Ensemble Interval Histogram (EIH) model, which takes into account psychoanalytical masking effects. Experimental
results show that in the new signal projection space, between-class separation is improved, in average.
Keywords Environmental Sound Analysis, Automatic sound classification, Psychoanalytical Masking.
Resumo Análises de sons através dos Mel Frequency Cepstral Coefficients (MFCC), juntamente com Gaussian Mixture
Models (GMM), não são raras na detecção de eventos relevantes em acompanhamento médico remoto. No entanto , apesar dos
bons desempenhos relatados no uso dos MFCC, sabe-se também que esses coeficientes fornecem uma representação espectral
dos sons que evidencia ressonâncias como as notadas na voz humana - o que explica seu sucesso no reconhecimento_de_fala. Assim, uma questão relevante é saber se os MFCC também fornecem um espaço convenientes  projeção para os sinais típicos do
monitoramento de pacientes em domicílio. Neste trabalho, nós comparamos os desempenhos em termos de taxas de reconhecimentos corretos a partir de uma base de sinais gravados em cenários de acompanhamento médico, e usando dois tipos de análises de sinais em janelas curtas de tempo uma baseada nos MFCC e outra adaptada dos Ensemble Interval Histogram (EIH) models, que levam em conta o efeito psicoanalítico de mascaramento. Resultados experimentais aportam evidências de que nesse
novo espaço de projeção de sinais, em média, as classes de sons se separam mais claramente umas das outras.
Palavras-chave .

1

al., 2006) aimed at providing affordable personalized
cognitive assistance.
In recent works, it has been shown that automatic
detection of relevant events for remote health care
can be done in a rather conventional way, through the
analysis of short segments (less than 50 ms, typically)
of digitalized signals from microphones strategically
placed into rooms where the subject to be monitored
lives (e.g. places in the house of a elderly person under medical care). These short segments of sounds
are then processed and features are extracted, much
like what is done in speech or speaker recognition.
Indeed, features such as MFCC, along with Gaussian
Mixture Models (GMM), are not rarely deployed for
this kind of task. However, in spite of good recognition ratios obtained with MFCC, it is known that
Cepstral Coefficients are aimed at rough spectral
shape estimation  thus highlighting resonance ef-

I.Introduction

Acoustic Event Detection and Classification is a
recent sub-area of computational auditory scene analysis (Valenzise et al., 2007) where particular attention has been paid to automatic surveillance systems
(Wang et al., 2006), (Zieger et al., 2008), (Feil-Seifer
et al., 2005). In particular, the use of audio sensors in
surveillance and monitoring applications has proven
to be particularly useful for the detection of distress
situation events, chiefly when the person suffers from
cognitive illness. The recent research work in medicine has concluded that some persons with mild cognitive impairment will develop Alzheimer in the future (De Rotrou et al., 2005). The efficient detection
and recognition of the distress situation is one part of
the socially assistive robotics technology (Rouas et

1981

as close as possible to the MFCC. This puts into
evidence only the gain we have when masking effect
is considered. Therefore, for this work, we use an original method for Short-Time EIH analysis, or Fast
EIH (FEIH).
This paper is organized as follows in Section II,
we fist present the algorithm used for sound segmentation. In Section III, the new algorithm for shorttime analysis with masking is detailed, whereas the
simple pattern classification strategy is presented in
Section IV. Results are briefly presented in Section
V and discussed amidst the main conclusions in Section VI.

fects useful in speech recognition  , and a relevant
question may be raised do MFCCs correspond to
good signal projection spaces for most signals of interest in remote healthcare? In this work, we address
this point by comparing recognition performances using real signals recorded in remote health care scenarios and two different types of features.
In (Istrate et al., 2008), a very extensive work
was presented, in which a full prototype was implemented for real time application, from signal detection to final sound classification. There, the sound
identification module was evaluated for 1577 sound
files using a leave-one-out method (designed for
low-sized data base) to adapt Gaussian Mixture Models (GMM) of the probabilistic behaviors of acoustical parameters. In that case, 24 MFCC (Mel Frequency Cepstral Coefficients) coupled with Centroid,
ZCR and Roll-off Point were used to represent shorttime segments of signals. As it was reported by the
authors, among the three tasks (detection,
soundspeech
segmentation,
and
sound
identification), the last one, sound classification,
presented the lowest performances. Here, in this
punctual contribution, our main goal is to address this
task by comparing two different sound representations. This is motivated by the a priori belief that
psychoanalytical phenomena, such as spectral masking, may play an important role in good representation of sounds.
Moreover, we also believe that good signal
projections are those through which different sounds
(from different sources) are projected in well separated feature space regions. If it is true, then even a
over simplified pattern classifiers may provide good
performances, and probabilistic source models can be
obtained with fewer training examples (because they
form well-separated clusters in the feature space).
By assuming that all that is true, two important
restrictions are imposed on this experimental work
(a) simpler non-training models are used, instead of
GMMs, and (b) simple Euclidean distances are used
instead of likelihood to compare signals.
To include psychoanalytical phenomena into a
signal projection, we first consider, in this initial effort, the Ensemble Interval Histogram (EIH) model
(Ghitza, 1994), which produces dominant periodic
temporal structures by analyzing zero-crossing intervals in frequency narrow-filtered bands of signal. In
other words, it uses an array of level-crossing detectors attached to the outputs of many band-pass filters
to generate an (inter-crossing) interval histogram.
Though the EIH is a model aimed at the mimicking
of animal hearing systems, its main aspects are frequently adapted into more computationally efficient
algorithms, with even better performances, in some
cases. For instance, the ZCPA method (Kim et al.,
1999), which is an improvement of the EIH model,
uses peak rather than level-crossings to measure intensity of each zero-crossing interval. In this comparative work, we want to use an EIH inspired strategy

2 Sound event segmentation
Raw signals, are represented by samples,
s n  , where n 1,2 , ... , N  and N 
In this work, samples are regularly taken at 16KHz.
We assume that each raw signal, corresponding to
each recorded file in our database (see Section Experimental Results), is, at least, one relevant event
corresponding to one of those sound sources (arbitrarily limited here to just 6)
1) Glass Breaking
2) Screaming
3) Door Clapping
4) Step Sound
5) Cough
6) Metal object fall
Therefore, we first use an algorithm to detect a
single event and crop the corresponding subset of
samples s s  k  , where k k b , ... , k e  and

k b , ... , k e 1,2 , ... , N  . This segmentation

task is done here by a very simple algorithm, based
on power measurement, which can be summarized as
follows (note this algorithm was applied additionally
to re-segment sounds into files which, by themselves,
contain segmented signals) 
1) s n  s ns (  s stands for the estimated standard deviation of s n  )
2

2) p n   s n  (noisy instantaneous power
estimation)
3) p f 1  2 ( p f stands for the lowpassfiltered power estimation)
4) for k 2  N

if  p k  p f  k 1
p f  k 0.005 p  k 0.995 p f  k 1
else

p f  k 0.001 p  k 0.999 p f  k 1
end

1982

end

Fig. 2. Cropping of segmented signal from original sound  note
the end of the detected power contour, in spite of a non-negligible
residual signal after this point.

5) k a takes the value of k for which p f  k 
first crosses level 2.1 (2.0 is the expected steady
level for white noise).
6) k b takes the value of k for which p f  k 
crosses min  p f 0.1 .

Realtime experiments with this same database were
done by Dan Istrate and fellows, in (Istrate et al.,
2008), without this re-segmentation algorithm, and
with a fast wavelet based front-end processing .
In spite of its simplicity, this algorithm is able to
satisfactorily segment targeted sounds even at very
low Signal to Noise Ratio (SNR), given that noise
has an almost stationary power through time.

In other words, this algorithm just filters the
noisy instantaneous power estimation through a nonlinear low-pass IIR filter, which is more reactive to
power raises (in order to detect fast attacks of our target noises). Then, it uses two level-crossing detectors
to segment the signal to be analyzed. Figure 1 illustrates this segmentation procedure for a signal containing three step sounds, whereas in Figure 2, we
can see how a segmented signal is cropped from the
original sound. It is worth noting that, in both illustrations, only the first part of the sound is segmented
(please read discussions on this concern, in the last
Section). Unfortunately, step (1) restrains the use of
this algorithm to off-line applications, were at least a
few seconds of signal are available for variance estimation. But we believe that step (1) can be conveniently replaced by a realtime Automatic Gain
Control (AGC), with a time reaction as slow as a few
seconds, just to keep signal variance around unity, in
average.

3 Short-time sound analysis
Segmented intervals of sound, from each sound
file in the database, are short-time analyzed. That is
to say that windows of 500 consecutive samples (approx. 31 ms at 16KHz) are taken as signal vectors to
be projected in a new space of reduced dimension.
For MFCC based analysis, each 500-dimensional
vector is mapped into a 24-dimensional space, each
dimension corresponding to one Mel-Cepstral Coefficient. On the other hand, for the EIH analysis, a
frame-by-frame version of the original algorithm
(Ghitza, 1994) is proposed in this work. This new algorithm runs much faster than the filter-bank based
original EIH, while keeping its frequency analysis
into a nonlinear scale, illustrated in Figure 4, and the
sound masking effect corresponding to the dominance of the strongest spectral peaks in each band.
Since it can be regarded as a faster and simplified version of EIH, in this work it is referred to as
the Fast EIH algorithm - FEIH. This FEIH analyzes
each sound frame, much like in the MFCC based analysis, but with the nonlinear scale proposed in
(Ghitza, 1994), instead of the Mel scale used in
MFCC. Moreover, masking effect is approximated by
a sliding weighting function, whose center is slid
throughout all channels. In each position of this sliding function, the spectral power profile is weighted
and the channel number (corresponding to a frequency center) with highest weighted power is selected as a winner, or masker for that spectral region,
and a histogram vector is incremented in the bin corresponding to the winner position.
At the end of this process, spectral peaks dominating wide spectral bands are represented by peaks of
relative frequency in the histogram, whereas small
spectral peaks shadowed by bigger ones are unlikely
to be winners, and are masked, thus being represented by very low values, or even nulls, in the histogram.
This histogram plays the role of a power spectrum, and it is inverse-transformed, back to time,
through
Inverse Discrete Co-sinus Transform
(IDCT). Only the 30 first coefficients (out of 140) are
taken, for they compactly represent a smoothed spectral contour. Figure 5 illustrates the Fast EIH projection of 500 sound samples into 30 coefficients.

Fig. 1. Signal segmentation through power contour  note that
only the first relevant part of the signal is taken.

1983

feature vector is compared (through Euclidean distance) to each column of every P i , i1,... ,6 .
The minimum distance found gives the label, l m 
c m  . For instance, if a sound is split into
of 
M 140 windows, thus providing 140 feature vecc m  , and 
c 1  is closer to a column of P 3
tors 
, then l 1 3 . Following this path, we end up with
140 labels, in this example.
Every new classification of an unknown sound is
obtained as a result of a label vote counting. Indeed,
a score can be obtained as a simple proportion of
votes for each class.
In our experiments, with 6 classes, we obtain
four scores, h 1 , ..., h 6 , and as far as we do
not accept a no-classification result (reject class), we
just pick-up the index of the highest h as a pointer
to the class from which the analyzed sound is more
likely to come from.
It is a very simple approach for both class-model learning and new sound classification. Nevertheless, it has at least one remarkable advantage over
GMM only a few sound examples are enough to
provide one model per class. Moreover, since we are
focusing on the comparison of different short-sound
projections, we expect that this rough modeling approach will be not able to compensate for inadequacy
of some projections (if it is the case). Indeed, GMM
are sophisticated models which, given enough
amount of training samples, may be able to provide
fine probabilistic models that may compensate for a
not compact dispersion of patterns in the feature
space.

4 Pattern Recognition Strategy
In order to get preliminary insights concerning
the goodness of EIH inspired sound projections, we
used subsets of the sound database gathered in the
framework of the (European) CompanionAble Project (httpwww.companionable.net ), by D. Istrate
and fellows. This subset contains
Subset 1 83 files with glass breaking
sounds
Subset 2 63 files with door clapping
sounds
Subset 3 68 files with screaming sounds
(males and females)
Subset 4 17 files with step sounds
Subset 5 41 files with cough sounds
Subset 6 12 files with sounds of metal
object falls
All files were recorded at a sampling rate of
16KHz and only a single channel (monaural sound)
of each record is used in this work. Signals were not
acquired during acoustically controlled sessions, at
recording studios, but at conventional rooms, with
usual office or home background noise. Nevertheless,
some care was taken concerning noise during recordings, so that the averaged signal to noise ratio is high,
typically between 50 and 60 dB.
Only 4 files from each subset (class) are arbitrarily drawn to represent the classes (instant training).
They are then processed (MFCC or Fast EIH based
projection of short-time overlapping windows), as illustrated in Figure 3, and all vectors of features are
appended as columns of features matrices. Thus, for
each class of sound, 6 matrices of features are obtained. These matrices are gathered (by simple concatenation) into a single bigger matrix per class, represented by P 1 , P 2 , P 3 , P 4 , P 5 and P 6 ,
respectively. Thus, the learning process is accomplished and we highlight that there is no parameter
estimationadaptation, as in GMM based strategies.

5 Experimental results
We assume, for simplicity of this brief experimental investigation, that each sound subset corresponds to a sound source class (although we easily
identify a strong diversity of timbres and temporal
structures of sounds inside each subset).

Fig. 3. Short-time based analysis  two parallel approaches.

Since matrices P i , i1,... ,6 , are available, and we assume that their columns fairly represent acoustical patterns of sounds from each class,
new unlabeled sounds can be classified as follows
Unknown sounds are analyzed through the projection of M short-time overlapping time-windows
c m  , m1,... , M . Each
into feature vectors, 

Fig. 4. Nonlinear frequency scale used in EIH model.

1984

Table 2. Confusion matrix with FEIH
Assigned.

Glass

Door

Scream

Step

Cough

Metal


Actual



Glass

6279

379

079

479

979

179

Door

059

3659

059

859

1559

059

Scream

264

164

5564

064

364

364

Step

113

1113

013

113

013

013

Cough

037

437

037

237

3137

037

Metal

08

08

08

08

18

78

For completeness sake, we also provide the total
duration of sounds, per class, used for test, along with
the total amount of sound available before segmentation







Fig. 5. Illustration of main steps of the FEIH.

Our preliminary results are summarized in two
confusion matrices, one for MFCC based analysis 
Table 1  , and another for FEIH  Table 2. Since
the number of filessounds tested is still limited, we
explicitly indicate the number of files assigned to
each class (table columns). As the files are originally
labeled according to the rows, the correct classifications are found in the table diagonal.

6 Conclusions and discussions
In terms of averaged correct classifications, the
FEIH, with approx. 74 , largely outperforms the
MFCC, with 48. These results, obtained with a Pattern Classification strategy which is simple to the
point of using Euclidean metrics instead of likelihood, is a strong evidence that, with FEIH, the
classes of sounds are more well separated in the signal projection space (in terms of Euclidean distance)
than they are with MFCC. In other words, classes
correspond more to clusters in FEIH space.
Moreover, in both confusion matrices, we easily observe a strong confusion between Door Clapping and
Step sounds. It is rather expected, given the wide
variety of timbres in both classes (different kinds of
doorsfloors and very non-uniform strengths during
door closingwalking actions, respectively). Indeed,
since a single step is taken during sound segmentation (see Figure 1, for illustration), these short sounds
are confusable even for an attentive human listener.
That is to say that, throughout the listening of all
sounds, in spite of an a priori 6 classes division, we
are tempted to propose only 5 classes, where door

Table 1. Confusion matrix with MFCC
Assigned.

Glass

Door

Scream

Step

Cough

Metal


Actual



Glass

4279

3779

079

079

079

079

Door

059

5559

059

0459

059

059

Scream

364

4364

1264

064

064

664

Step

013

513

013

813

013

013

Cough

237

2937

037

437

237

037

Metal

08

28

08

08

08

68

Glass Breaking 45.9 s (segmented, actually
used) out of 69.2 s (available total before
segmentation)
Door Clapping 26.2 s out of 39.8 s
Scream 54.6 s out of 76.5 s
Step 7.6 s out of 20.4 s
Cough 23.0 s out of 44.2 s
Metal fall 3.9 s out of 6.4 s

1985

clapping and steps are merged, because even human
listeners are unable to resolve the actual source of
most segmented door clapping and step sounds. For a
human listener, a clear separation between door clapping and step sounds only appears when the rhythmical repetition of steps is considered.
It further suggests that this short-time based analysis is incomplete, and a much better distinction
between classes of sounds may be reached, for instance, if its structure throughout time is somehow
taken into account. This analysis, along with a deeper
study with more than 6 classes is going to be considered in the sequel of this work.
Acknowledgments
This work was partially granted by the Brazilian
Conselho Nacional de Desenvolvimento Científico e
Tecnológico (CNPq).
References
De Rotrou, J., Wenisch E, Chausson C, Dray F,
Faucounau V, Rigaud A.-S. (2005) Accidental
MCI in healthy subjects a prospective longitudinal study. Eur J Neurol 12(11)879-85.
Feil-Seifer, D., M.J. Matari (2005) Defining socially assistive robotics, in Proc. IEEE International Conference on Rehabilitation Robotics
(ICORR05), Chicago, Il, USA, pp. 465468.
Ghitza, O. (1994) Auditory models and human performance in tasks related to speech coding and
speech recognition, IEEE Trans. Speech Audio
Process., vol.2, no.1, pp.115132.
Istrate, D., M. Binet, and S. Cheng (2008) Real time
sound analysis for medical remote monitoring,
in IEEE EMBC, Vancouver, Canada, pp. 4046.
Kim, D.S., S.Y. Lee, and R.M. Kil (1999) Auditory
processing of speech signals for robust speech
recognition in real-world noisy environments,
IEEE Trans. Speech Audio Process., vol.7, no.1,
pp.5569.
Rouas, J.L., J. Louradour, S. Ambellouis (2006)
Audio Events Detection in Public Transport
Vehicle. Proc. of the 9th International, IEEEConference on Intelligent Transportation System
2006, pp.733  738
Valenzise, G., L. Gerosa, M. Tagliasacchi,. F. Antonacci, A. Sarti (2007) Advanced Video and
Signal Based Surveillance, 2007. AVSS 2007.
Volume 2, Issue, 5-7, Page(s)21 - 26
Wang, D., G. Brown (2006) Computational Auditory Scene Analysis Principles, Algorithms and
Application, Wiley-IEEE Press.
Zieger, C., M. Omologo (2008) Acoustic event classification using a distributed microphone network with a GMMSVM combined algorithm,
Interspeech, Brisbane, pp. 115-118.

1986