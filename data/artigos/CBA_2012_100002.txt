Anais do XIX Congresso Brasileiro de Automática, CBA 2012.

METODO DE INICIALIZACAO DE PESOS PARA MELHORIA DE DESEMPENHO
DE CLASSIFICACAO DE REDES NEURAIS MLP UTILIZANDO TECNICAS DE
PODA
M. R. Silvestre, S. M. Oikawa, F. H. T. Vieira, L. L. Ling, A. A. Cardoso


Endereco de Silvestre e Oikawa
Departamento de Estatstica da Faculdade de Ciencias e Tecnologia da Universidade Estadual Paulista
(UNESP)
19060-900 Presidente Prudente, Sao Paulo, Brasil


Endereco de Viera e Cardoso
Escola de Engenharia Eletrica e Computacao da Universidade Federal de Goias (UFG)
74605-010, Goiania, Goias, Brasil

Endereco de Ling
Departamento de Comunicacoes da Faculdade de Engenharia Eletrica e Computacao da Universidade
Estadual de Campinas (UNICAMP)
13083-970 Campinas, Sao Paulo, Brasil

Emails miriam@fct.unesp.br, smoikawa@fct.unesp.br, flavio@eee.ufg.br,
lee@decom.fee.unicamp.br, alsnac@gmail.com
Abstract In this paper we propose a new method for weight initialization of a multilayer perceptron neural
network through cluster analysis and by using the cluster cen as initial weights to initialize the network. We
propose to apply a correction taking into account the square error minimization between the network output and
the desired output as well as the distance between the cen to the input data. The classification performance
of the neural network was evaluated when using the proposed method of weight initialization in conjunction with
iterative pruning methods. Four variables were observed in the study to evaluate the computational effort of the
six methods of pruning considered the apparent error rate of classification of the test dataset (REA), number
of intermediate neurons obtained by applying the method of pruning,number of times trainingretraining and
computational time consumed.
Keywords

synaptic weights, neural network multilayer perceptron, iterative pruning method, clus.

Resumo Neste artigo, e proposto um novo metodo para a inicializacao dos pesos sinapticos de uma rede
neural perceptron_multicamadas atraves de analise de agrupamentos e utilizando os centros dos agrupamentos
(clus) gerados como pesos iniciais para inicializar a rede. Propomos aplicar uma correcao aos pesos iniciais
da rede levando em consideracao minimizacao dos erro quadratico entre a resposta da rede e a resposta desejada
e as distancias entre os centros e os dados de entrada. O desempenho de classificacao da rede_neural foi avaliado
utilizando-se o metodo proposto de inicializacao de pesos aplicado em conjunto a diferentes metodos de poda
iterativos. Quatro variaveis foram observadas no estudo para avaliar o esforco computacional dos seis metodos
de poda considerados razao de erro aparente de classificacao do conjunto de teste (REA) numero de neuronios
intermediarios obtido apos a aplicacao do metodo de poda numero de epocas de treinamentore-treinamento e
tempo computacional gasto.
Palavras-chave
.

1

pesos sinapticos, rede_neural de perceptron_multicamadas, metodo de poda iterativo, clus-

Introducao

Em (Reed, 1993) e feito um levantamento dos
principais metodos de poda, dentre os quais existe
um grupo de metodos que calcula a medida de sensibilidade de cada unidade de poda (peso sinaptico
ou neuronio) e retira as unidades que apresentarem valores baixos para esta medida (Karnin,
1990), (Lecun et al., 1990), etc.). Posteriormente,
surgiram outros metodos na area de redes_neurais artificiais (Mao et al., 1994),(Park, 1996),
(Murase et al., 1991), (Castellano et al., 1997) e
(Silvestre and Ling, 1998). O objetivo principal
dos metodos de poda e reduzir a arquitetura da
rede de forma a realizar a classificacao com rapidez e economia. Neste artigo, propomos aplicar os
metodos de poda para melhorar a fronteira de de-

ISBN 978-85-8001-069-5

cisao pratica, atraves da exclusao dos hiperplanos
que nao contribuem para a sua formacao. Quando
os metodos de poda sao construdos para remover
os pesos sinapticos, o resultado final e a producao
de uma rede grande e esparsamente conectada, ou
seja, com varios pesos nulos. Entretanto, fisicamente estes pesos continuam na rede. Para que
isto nao ocorra, os seis metodos de poda apresentados na secao 2 estao direcionados teoricamente
para realizar poda de neuronios, e serao capazes
de produzir redes finais completamente conectadas, pois os neuronios que contribuem pouco ou
nada para a classificacao serao fisicamente excludos da rede. Desta maneira, a rede final sera bem
menor e a classificacao de um novo padrao sera
bem mais rapida.
Em (Silvestre and Ling, 2002), foi realizado

3460

Anais do XIX Congresso Brasileiro de Automática, CBA 2012.

um experimento comparativo envolvendo o metodo chamado CONTROLE, rede inicial na qual
nao e aplicada nenhuma tecnica de poda, e varios
metodos de poda
 Iterativos, os quais retiram um neuronio em
cada passo Forca Bruta Iterativo (FBI,
(Silvestre and Ling, 2002))), Razao de Erro
Aparente de Validacao (REAV, (Silvestre
and Ling, 2002))), Razao de Erro Aparente de Treinamento (REAT, (Silvestre and
Ling, 2002)), Poda Baseado na Fronteira de
Decisao Efetiva de Bayes (PEB, (Silvestre
and Ling, 2002))), Saliencias (SAL, (Mao
et al., 1994)), Optimal Brain Damage (OBD,
(Lecun et al., 1990)), Poda Iterativo (PI,
(Castellano et al., 1997)), Poda Dinamico
(PD, (Murase et al., 1991))
 Nao iterativos, os quais sao executados num
unico passo Sao eles Principal Component Analysis (ACP, (Park, 1996)), Gap
das Saliencias (GAPSAL, (Silvestre and
Ling, 1998)), Forca Bruta num Unico Passo
(FBUP, (Silvestre and Ling, 2002)).
Em (Silvestre and Ling, 2002), os metodos foram adaptados para a retirada de neuronios intermediarios e avaliados em 7 problemas de classificacao ja conhecidos e obtidos em (Murphy and
Aha, 1994), sao eles Ecoli, Pima Indians, Glass,
Iris, Monk3, Wine e Bupa. As variaveis estudadas
no experimento foram razao de erro de classificacao no conjunto de teste (REA), numero de neuronios intermediarios na rede final (neuronios) e
numero de epocas de treinamentore-treinamento
da rede (epocas). Os resultados obtidos indicaram que os metodos de poda apresentam o mesmo
desempenho para a variavel REA que a rede CONTROLE e, embora necessitem de um numero
maior de epocas de treinamentoretreinamento,
somente os procedimentos de poda iterativos sao
capazes de reduzir o numero de neuronios intermediarios.
Em (Silvestre and Ling, 2006)), e apresentada
uma investigacao experimental considerando somente os metodos de poda iterativos, envolvendo
cinco conjuntos de dados gerados artificialmente
e dois conjuntos de dados reais (Thyroid e Assinaturas). A analise_estatstica comprovou que os
metodos de poda aplicados a neuronios intermediarios de redes com bom desempenho conseguem
manter a mesma acuracia de classificacao mas com
a vantagem de produzir redes com um numero reduzido de neuronios.
Os pesos iniciais de uma rede geralmente de
forma eficiente sao gerados a partir de valores aleatorios. (Duch et al., 1997) sugere empregar analise
de agrupamentos e utilizar os centros dos agrupamentos (clus) gerados como pesos iniciais para
inicializar a rede.

ISBN 978-85-8001-069-5

Neste artigo, propomos uma correcao aos pesos iniciais da rede obtidos de acordo com (Duch
et al., 1997) considerando a minimizacao de uma
funcao custo, que culmina em menores erros quadraticos e distancias entre os centros e os dados de
entrada. Apresentamos resultados experimentais
envolvendo os metodos de poda iterativos com a
aplicacao do metodo proposto de inicializacao de
pesos.
O trabalho apresentado esta estruturado da
seguinte maneira na secao 1 e tecida uma introducao ao tema proposto, na secao 2 sao apresentados varios metodos de poda, a secao 3 trata
de metodos de inicializacao de pesos envolvendo a
transformacao de (Duch et al., 1997) e a proposta
pelos presentes autores, na secao 4 sao apresentados resultados experimentais para os metodos
de inicializacao e de poda comentados nas secoes
anteriores, e na secao 5 o trabalho e concludo.
2

Metodos de Poda

Considere uma rede MLP, contendo uma camada intermediaria. Denotamos o n-esimo (n 
1, ..., N ) par de exemplos de treinamento como
x(n), y(n) , tal que o vetor de entrada x(n) 
x1 (n), ..., xd (n)T e aplicado aos neuronios da camada de entrada e o vetor de respostas desejadas
t(n)  t1 (n), ..., tc (n)T e aplicado aos neuronios
da camada de sada, onde d representa o numero
de variaveis de entrada e c o numero de classes
(1)
do problema. Seja W (1)  wji (n) a matriz de
pesos sinapticos que conecta a camada de entrada
(2)
e intermediaria, e W (2)  wkj (n) , a matriz de
pesos sinapticos que conecta a camada intermediaria e de sada. O procedimento de otimizacao
para classificacao e minimizar a soma dos erros
quadraticos medios entre a sada da rede yk (n) e
a resposta desejada tk (n), definida por
E
E(n) 

1
2

N
X

E(N ),

onde

n1
c
X

yk (n)  tk (n)2

(1)

k1

Adotando g (1) e g (2) , funcoes de ativacao do
tipo sigmoide logstica, denota-se a k-esima sada
da rede por
M
d
X
X
(2)
(1)
yk (n)  g (2) (
wkj (n)g (1) (
wji (n)xi (n))) (2)
j0

i0

Varios metodos de poda descritos nesta secao
irao utilizar a sada do j -esimo neuronio da camada intermediaria (j  1, ..., M ), definido como
d
X
(1)
zj (n)  g (1) (
wji (n)xi (n))))

(3)

i0

3461

Anais do XIX Congresso Brasileiro de Automática, CBA 2012.

Os ndices i  0 e j  0, representam os biases
das camadas de entrada e intermediaria, respectivamente. Note que, todos os metodos descritos abaixo, necessitam de um retreinamento apos
cada passo de poda. Os metodos FB(Forca Bruta)
e OBD( Optimal Brain Damage) foram construdos para executar poda de pesos sinapticos, porem, serao apresentados com as modificacoes necessarias para que se faca poda de neuronios intermediarios. Uma breve discussao de cada um
destes metodos e dada a seguir.
O metodo denominado Forca Bruta (FB),
(Reed, 1993), (Karnin, 1990) e (Lecun et al.,
1990), tem como objetivo detectar e anular os pesos sinapticos que provocam pouca alteracao na
soma dos erros quadraticos medios (1). Este metodo foi repensado em (Silvestre and Ling, 2002)
para detectar e remover neuronios pouco influentes e consiste em definir um neuronio com valor
de sada zero e avaliar a alteracao
no
Pprovocada
N
erro E, utilizando a medida Sj  j1 Sj (n), tal
que
Sj (n)  E(zj (n)  0)  E(zj (n))

(4)

Quando definimos E(zj (n)  0), todos os pesos sinapticos que conectam o neuronio j as camadas de entrada e sada sao zerados, e um valor
aproximado para E e obtido para uma rede sem
a presenca daquele neuronio. Ja E(zj (n)) sera o
valor de E com todos os pesos da rede associados
a este neuronio. Se Sj for pequeno, entao o neuronio j pode ser removido da rede. Teoricamente,
isto pode ser realizado para todos os possveis subconjuntos de neuronios intermediarios da rede, e
o chamado procedimento otimo de Busca Exaustiva (BE), onde todas as possveis combinacoes
(2M  1) dos neuronios que restarem na rede devem ser avaliadas. De acordo com (Karnin, 1990)
e (Lecun et al., 1990) este procedimento e considerado ideal, porem, e computacionalmente impraticavel para redes grandes. Entao, foi introduzido
em (Silvestre and Ling, 2002) um melhoramento
no procedimento de busca, o qual sera descrito a
seguir.
1. Forca Bruta Iterativo (FBI) no metodo FB
descrito acima, ao inves de utilizar BE, sugerimos adotar o procedimento sub-otimo de
busca para tras (BPT), o qual e capaz de reduzir o numero de diferentes redes que deveriam ser investigadas de 2M 1 para M (M2 +1) .
A ideia principal e iniciar com uma rede
grande e calcular Sj somente para os neuronios que restarem na rede em cada passo de
poda. Por exemplo, para M  10 neuronios
intermediarios, seriam 1023 redes a serem investigadas com BE, utilizada no metodo FB,
contra somente 55 redes com o procedimento
BPT, para o metodo FBI proposto.

ISBN 978-85-8001-069-5

Alguns outros metodos existentes na literatura foram modificados quanto a parte a ser
removida, e todos foram implementados utilizando o procedimento BPT. Tais metodos
serao descritos a seguir.
2. Razao de Erro Aparente de Treinamento
(REAT) este metodo, tambem proposto em
(Silvestre and Ling, 2002), utiliza a razao de
observacoes que foram classificadas incorretamente pela rede (razao de erro aparente
REA), que neste caso, sera calculada sobre o
conjunto de treinamento. A variavel
Sj  REA(zj  0)  REA(zj )

(5)

e adotada como uma medida da importancia
de cada neuronio. A razao de erro e expressa
em percentual da seguinte forma apos o treinamento ter sido finalizado, o vetor de sadas
produzido pela rede para cada exemplo n, devera ser transformado num vetor contendo somente zeros e um unico valor um, o qual devera estar na posicao onde estiver o maximo
de todos os yk (n) para k  1, ..., c. Esse procedimento significa que estamos atribuindo o
exemplo n a classe representada na posicao
onde estiver o valor 1. Posteriormente, o novo
vetor de zeros e um, y(n)  y1 (n), ..., yc (n)T
sera comparado a sada desejada da rede,
t(n)  t1 (n), ..., tc (n)T , sendo caracterizado
um acerto de classificacao se as posicoes dos
valores 1 nos dois vetores coincidirem, e em
caso contrario, sera definido um erro de classificacao. Verifica-se o numero de exemplos
de treinamento para os quais houve erro de
classificacao. Em seguida, dividi-se pelo total
de exemplos de treinamento N e multiplicase por 100. Assim, tem-se o valor do REA em
percentual.
O processo de poda e iterativo e retira-se o
neuronio com menor valor de Sj , definida em
(5), em cada passo de poda. Dessa forma, estara sendo retirado o neuronio cuja ausencia
melhorou o desempenho de classificacao da
rede, ou seja, diminuindo o erro de classificacao. Deve-se posteriormente definir o ponto
de parada desse procedimento, retirando-se
neuronios ate que de um passo para o proximo, o erro de classificacao comece a aumentar, isso indica que os neuronios retirados a
partir desse passo eram importantes para a
rede, e sem eles houve um piora no desempenho, deve-se parar antes que isso ocorra. Em
todos os metodos de poda apresentados nessa
secao devera ser aplicado esse mesmo criterio
de parada para o processo iterativo de poda.
3. Optimal Brain Damage (OBD) este metodo,
proposto em (Lecun et al., 1990) para a retirada de pesos sinapticos foi adaptado anteriormente por (Silvestre and Ling, 2002) para

3462

Anais do XIX Congresso Brasileiro de Automática, CBA 2012.

retirar neuronios. Para selecionar qual neuronio deve ser retirado em cada passo de poda,
calcula-se a medida de importancia de cada
neuronio, a qual e definida pela perturbacao na funcao E causada pela retirada desse
neuronio da rede. A perturbacao e calculada
com a expansao em serie de Taylor da funcao
E, sendo definida como
Sj 

N
X

(6)

As derivadas sao obtidas pela regra da cadeia
e (zj (n))  zj (n). Note que na aproximacao pela serie de Taylor sugerida, os termos de primeira ordem e os termos cruzados
da matriz Hessiana, bem como os termos de
ordem superior foram descartados, restando
somente os elementos diagonais de segunda
ordem da serie de Taylor. Em cada passo de
poda retira-se da rede o neuronio que apresentar o menor valor de Sj , o que indica que
sua ausencia causou a menor perturbacao na
funcao E, sendo portanto um neuronio de
pouca importancia para a rede.
4. Saliencias (SAL) este metodo, definido em
(Mao et al., 1994), tambem considera a
perturbacao provocada pela retirada de um
neuronio, atraves da aproximacao pela serie
de Taylor da funcao E. Porem, acrescenta a
primeira derivada a medida definida em (6),
tal que
E(n)
(zj (n))
zj (n)
+

1  2 E(n)
zj (n)2 (7)
2 (zj (m))2

O metodo e iterativo e retira da rede o neuronio j com o menor valor de Sj em cada passo.
5. Poda Dinamico (PD) este metodo, proposto
em (Murase et al., 1991), utiliza a menor medida
Sj 

1
N

N X
c
X

(2)

(wkj )2 zj2 (n)

(8)

n1 k1

para detectar o neuronio que deve ser retirado
em cada passo de poda do processo iterativo
deste metodo.
6. Poda Iterativo (PI) este metodo, definido em
(Castellano et al., 1997), utiliza a mesma medida do metodo PD, equacao (8), para selecionar o neuronio que deve ser retirado da
rede, porem, o reajustamento dos pesos sinapticos restantes e feito pelo algoritmo do

ISBN 978-85-8001-069-5

Metodos de Inicializacao de Pesos

onde

j1



Dos metodos de poda apresentados nessa secao, os de mais facil implementacao sao os metodos FBI, REAT e PD. Enquanto que, os demais
metodos OBD, SAL e PI necessitam de uma implementacao computacional bem mais trabalhosa.
3

E(zj (n)),

1  2 E(N )
(zj (n))2
E(zj (n)) 
2 (zj (n))2

(zj (n))

gradiente conjugado da equacao normal precondicionada (Bjorck and Elfving, 1979).

Entre os metodos de inicializacao de pesos, podemos destacar o proposto em (Duch et al., 1997),
que apresenta eficiencia interessante. Os autores
sugerem um algoritmo para inicializacao, o qual
inicialmente faz um pre-processamento das observacoes, em seguida, e utilizada analise de agrupamentos para encontrar os centros dos clus os
quais sao utilizados para inicializar a rede_neural
MLP. O algoritmo e apresentado a seguir, na secao
3.1. Chamaremos este metodo de inicializacao de
DUCH CLUS neste artigo. Na secao 3.2 e apresentada uma proposta inovadora para melhorar esse
metodo que denominaremos DUCH CLUS MODIFICADO, cujo algoritmo pode ser verificado na
secao 3.3.
3.1

Algoritmo de Inicializacao de DUCH CLUS
para uma Rede Neural MLP

O algoritmo de inicializacao de Duch para a primeira camada e definido pelos seguintes passos
1. Faca o pre-processamento de todos os dados
de entrada n  1, . . . , N vetores ou unidades
amostrais, com i  1, . . . , d componentes ou
variaveis,X (n) e o n-esimo vetor, Xi e o vetor
do i-esimo componente de todos os vetores de
dados.
(a) encontre os valores mnimo, maximo, e
medio, definidos por
Ximin  minn Xin ,
max
Xi
 maxn Xin e
(Ximax  Ximin )
Xi 
2

(9)

(b) Transforme os vetores para os valores
medios da seguinte forma
X  X  X min  X

(10)

(c) Transforme os componentes dos vetores
para o setor 1
Xi 

Xi
X

(11)

(d) Encontre a maior norma kXkmax e renormalize todos os vetores

2X
X
(12)
kXkmax

3463

Anais do XIX Congresso Brasileiro de Automática, CBA 2012.

(e) Normalize os vetores adicionando uma
dimensao extra X  (X, Xr ), onde
r
2

Xr  sign(1  kXk )
satisf azendo

Metodo de inicializacao proposto DUCH
CLUS MODIFICADO

Neste trabalho propoe-se um algoritmo para realizar a inicializacao de uma rede_neural MLP. Para
tanto, considere a seguinte funcao custo J.
J

(rjn djn )2

(14)

j1 n1

onde C e o numero de agrupamentos ou clus,
indicado por uma analise de agrupamentos (analise de clus) e N e o numero de dados de treinamento. Em (14) rjn e o erro entre a n-esima sada
desejada para os dados de treinamento e a sada
do j -esimo clus (sada do neuronio j da camada intermediaria com a n-esima entrada), isto
e
rjn  y(n)  zj (n),

(15)

para j  1, 2, . . . , C e n  1, 2, . . . , N . Na mesma
equacao (14) djn e a distancia entre o n-esimo vetor de entrada e o centro do j-esimo grupo (cluster ) denotado como j , isto e
djn  x(n)  j (n),

(16)

Para minimizar a funcao custo J (14) em relacao
aos centros dos clus, implica que
N
X
d2jn
J

(rjn )2
0
j
j
n1

(17)

A equacao (17) fornecera os valores para os centros dos clus que minimizem a funcao custo
J, ou seja, para que se obtenham menores erros
quadraticos (15) e distancia entre os centros e os
dados de entrada. A condicao representada pela
equacao (17), pode ser simplificada uma vez que
d
a derivada parcial jn
nesta equacao e dada em
j
funcao da distancia entre a n-esima sada desejada e o centro do cluster j (equacao 16). Assim,
temos
d2jn
djn
 2djn
 2j  2x(n)
j
j

ISBN 978-85-8001-069-5

j 

Xr  1, +1 (13)

3. Escolha pesos (no espaco d + 1 dimensional)
igual ao centro dos clus C e os biases
iguais a +1.

C X
N
X

PN

2

1  kXk 

2. Faca analise de clus encontre as medias dos clus dos dados normalizados (use
dendrogramas e outros metodos de agrupamento).

3.2

Substituindo (18) em (17), obtem-se a seguinte
equacao para o centro do j-esimo cluster j 

(18)

2
x(n)
n1 rjn 
2
n1 rjn

PN

(19)

Espera-se com essa proposta obter um algoritmo
superior ao de Duch que leve em consideracao
agora os erros (15) e (16). Para executar a proposta, considere o algoritmo proposto a seguir.
3.3

Algoritmo de Inicializacao proposto para
uma Rede Neural MLP DUCH CLUS MODIFICADO

1. Fazer a transformacao (pre-processamento)
de variaveis sugerida no passo 1 do Algoritmo de inicializacao proposto por (Duch
et al., 1997), o qual foi apresentado anteriormente na Secao 3.1.
2. Aplicar os metodos de agrupamentos para definir quantos neuronios intermediarios se deve
usar (esta se implementando somente 1 camada intermediaria).
3. Escolher os pesos da camada intermediaria
da rede_neural como sendo igual ao valor dos
centros dos agrupamentos, os valores de bias
e dos pesos da camada de sada iguais a +1.
4. Utilizando essa rede_neural calcula-se novos
valores para os pesos da camada intermediaria como sendo iguais a j , ou seja, atraves
da equacao (19).
4

Avaliacao do Metodo de Inicializacao
de Pesos Proposto

Para avaliar o desempenho do metodo proposto
na Secao 3.2, foi escolhido o problema de classificacao cujo objetivo e prever a localizacao de
protenas em celulas eucarioticas, conhecido por
Ecoli. O conjunto de dados Ecoli, obtido em
(Frank and Asuncion, 2010). Nesse problema sao
definidos N  336 unidades amostrais, d  7 caracticas (variaveis) e c  8 classes classe 1
(citoplasma), classe 2 (membrana interna sem a
sequencia de sinais), classe 3 (periplasma), classe
4 (membrana interna com a sequencia de sinais
nao unida), classe 5 (membrana externa), classe 6
(membrana externa lipoproteica), classe 7 (membrana interna lipoproteica) e classe 8 (membrana
interna com a sequencia de sinais unida). Para a
aplicacao dos metodos de inicializacao, o conjunto
de dados foi dividido em duas partes, sendo uma
para treinamento e outra para teste. Na Tabela 1
e possvel observar como os dados sao distribudos
entre as oito classes, e a quantidade de casos de
cada classe que compoem os conjuntos de treinamento e teste. O conjunto de treinamento (ntre,k

3464

Anais do XIX Congresso Brasileiro de Automática, CBA 2012.

Tabela 1 Distribuicao do conjunto de dados
ECOLI, segundo a classe.
Classe k Nk Nk () ntrei,k ntes,k
1
143
42,56
108
35
2
77
22,92
58
19
3
52
15,48
39
13
4
35
10,42
26
9
5
20
5,95
15
5
6
5
1,49
4
1
7
2
0,59
1
1
8
2
0,59
1
1
Total
336 100,00
252
84

para k  1, 2, . . . , c), contem aproximadamente
75 de cada classe e foi utilizado para treinar a
rede e obter a rede final e o conjunto de teste
(ntes,k para k  1, 2, . . . , c) contem os 25 restantes dos dados de cada classe e foi utilizado somente para calcular o erro de classificacao da rede
final obtida.
Para este experimento, variou-se o numero de
neuronios intermediarios iniciais da rede mlp e o
tipo de transformacao aplicada aos dados. Foram
aplicados os metodos de transformacao
1. DUCH CLUS (Secao 3.1), aplica-se a transformacao DUCH usando os centros dos clus como pesos iniciais na camada intermediaria, d  8 variaveis
2. DUCH CLUS MODIFICADO (Secao 3.2)
aplica-se a transformacao DUCH usando os
centros dos clus corrigidos como pesos iniciais na camada intermediaria, d  8 variaveis.
Os metodos foram aplicados em redes com os seguintes numeros de neuronios intermediarios 1, 2,
3, 4, 5, 6, 7, 10, 15, 20, 30, 37, 57, 70, 100 e 252.
Em cada caso, a rede foi treinada de acordo com
os centros dos clus gerados com a aplicacao
do metodo de agrupamento Average, conhecido
tambem por metodo de Ligacao Media (Johnson
and Wichern, 2007), considerado como o melhor
metodo de acordo com os resultados obtidos pela
correlacao cofenetica para o conjunto de dados de
treinamento, considerando-se 252 observacoes. A
definicao da correlacao cofenetica pode ser encontrada de forma geral em (Romesburg, 1990, p.27)
ou (Silvestre et al., 2008).
Foi utilizada a funcao de ativacao sigmoide
logstica definida como
() 

1
1 + exp(a)

(20)

onde o parametro de inclinacao foi definido como
a  0, 8. Adotou-se para o treinamento o algoritmo de retropropagacao dos erros ou backpropagation sem regularizacao, o parametro de razao de

ISBN 978-85-8001-069-5

aprendizagem igual a 0,9, a constante de momento
nula e o criterio_de_parada do treinamento como
a norma euclidiana da estimativa do vetor gradiente (dew) menor ou igual a 0,05. O experimento
realizado permitiu observar o comportamento do
metodo proposto em funcao do numero de neuronios intermediarios empregados na rede. A partir
dos resultados obtidos, no conjunto de dados de
treinamento e teste, verificou-se que o metodo de
inicializacao DUCH CLUS MODIFICADO apresenta
 melhor resultado quanto a classificacao para
redes pequenas, ate no maximo 10 neuronios
 quanto ao numero de epocas de treinamento
exigido apresentou tendencia crescente ate 5
neuronios, porem, de 5 a 10 inicia-se um decrescimento, sendo zero para redes com mais
que 10 neuronios, para as quais o criterio de
parada (limiar dew<0,05) e atingido na primeira epoca
 o tempo de processamento, e maior no incio,
e a partir de 10 neuronios cai praticamente
a zero, pois nesses casos, o limiar ja e atingido na primeiro epoca, entretanto, o erro de
classificacao e alto para redes maiores.
4.1

Aplicando poda aos neuronios da camada intermediaria

Foi investigado se os metodos de poda aplicados as
redes inicializadas pelos metodos DUCH CLUS e
DUCH CLUS MODIFICADO conseguiriam produzir redes com menor numero de neuronios intermediarios. De todas as redes geradas com diferentes numeros iniciais de neuronios intermediarios, conforme a secao 4, optou-se por escolher
a rede inicial com 10 neuronios, pois ambos os
metodos apresentaram bons resultados para redes
MLP contendo de 5 a 10 neuronios na camada intermediaria. A partir das redes iniciais geradas
com cada metodo (DUCH CLUS e DUCH CLUS
MODIFICADO) foram aplicados os seguintes metodos de poda, com o objetivo de reduzir ainda
mais o tamanho da rede inicial quanto ao numero
de neuronios intermediarios REAT, FBI, OBD,
PI, SAL e PD. As definicoes dos metodos de poda
podem ser encontradas na Secao 2, ou ainda em
(Silvestre and Ling, 2006). Os resultados foram
organizados nas Tabelas 2 e 3.
Analisando a Tabela 2, pode-se observar que
o metodo proposto DUCH CLUS MODIFICADO
gerou uma Rede Inicial(primeira linha da Tabela
2) a qual apresentou menores valores para a razao
do erro aparente de treinamento (REAtrei) e de
teste (REAtst), numero de epocas e tempo computacional gasto em minutossegundos do que a
Rede Inicial produzida pelo metodo DUCH CLUS,
conforme a primeira linha da Tabela 3. Dessa

3465

Anais do XIX Congresso Brasileiro de Automática, CBA 2012.

Tabela 2 Conjunto de Dados Ecoli, rede inicial com 10 neuronios gerada pelo metodo proposto DUCH CLUS MODIFICADO (Rede Inicial
ou Inicial) com posterior aplicacao de metodos de
poda, com n sendo o numero de neuronios e ep
o numero de epocas.
Metodo
Inicial
REAT
FBI
OBD
PI
SAL
PD

n
10
4
7
6
7
6
7

REAtrei
10.7143
10.3175
9.9206
9.5238
11.1111
9.5238
9.9206

REAtst
19.0476
20.2381
19.0476
19.0476
19.0476
19.0476
19.0476

ep
110
356
334
412
165
441
324

Tempo
0034.65
0108.87
0107.18
0113.43
0038.51
0118.99
0105.12

Tabela 3 Conjunto de Dados Ecoli, rede inicial com 10 neuronios gerada pelo metodo DUCH
CLUS (Inicial) com posterior aplicacao de metodos de poda, com n sendo o numero de neuronios
e ep o numero de epocas.
Metodo
Inicial
REAT
FBI
OBD
PI
SAL
PD

n
10
4
4
4
5
4
4

REAtrei
11.5079
11.5079
11.1111
11.5079
11.5079
11.5079
11.5079

REAtst
21.4286
21.4286
21.4286
21.4286
21.4286
21.4286
21.4286

ep
119
243
243
242
152
325
326

Tempo
0059.64
0131.42
0127.10
0127.65
0112.04
0137.68
0150.93

forma, o algoritmo proposto apresentou bons resultados no conjunto de dados analisado. Alem
disso, na Tabela 2 pode-se concluir que aplicar
metodos de poda a partir da rede treinada com
o metodo proposto (Secao 3.2) possibilita a reducao do numero de neuronios, sendo que todas as
redes produziram uma razao de erro aparente de
teste menor ou igual ao obtido com a Rede Inicial
(metodo proposto) exceto para o metodo REAT,
o que e um bom resultado. Os metodos de poda
produziram redes com 4 a 7 neuronios intermediarios.
Ainda na Tabela 2 nota-se que alguns metodos
de poda demoram mais para convergir, como e o
caso dos metodos SAL e OBD. E o metodo PI se
destaca pela rapidez.
Para o metodo DUCH CLUS, Tabela 3, os
metodos de poda conseguiram reduzir a rede inicial de 10 para 4 a 5 neuronios, porem, a razao do
erro aparente de classificacao de teste se manteve
igual em todos os metodos, nao sendo um resultado ruim, porem nao tao bom como os resultados
com o metodo proposto DUCH CLUS MODIFICADO, para o qual os erros finais de classificacao
obtidos no conjunto de teste com a aplicacao de
poda foram iguais e na maioria inferiores ao da
Rede Inicial.

ISBN 978-85-8001-069-5

5

Conclusoes

O problema de classificacao abordado permitiu
observar o comportamento do metodo proposto
em funcao do numero de neuronios intermediarios
empregados na rede. A partir dos resultados obtidos, no conjunto de dados de treinamento e teste,
foi possvel observar que o metodo de inicializacao DUCH CLUS MODIFICADO apresenta bons
resultados para redes pequenas (poucos neuronios
intermediarios), e sugere que se inicialize a rede
com no maximo 10 neuronios. A aplicacao de metodos de poda tambem e viavel para se reduzir
mais ainda o tamanho da rede final, com razao de
erro aparente de teste menor que o da rede inicial.
Como sugestoes para estudos futuros, poderiam ser estudados outros problemas (conjuntos
de dados) de classificacao e tambem apos a aplicacao do metodo proposto, empregar tecnicas de
poda para que se produza redes com boa capacidade de generalizacao aliada a um numero mnimo
de neuronios intermediarios.
Referencias
Bjorck, A. and Elfving, T. (1979). Accelerated
projection methods for computing in artificial neural networks, BIT Numerical Mathematics 19 145163.
Castellano, G., Fanelli, A. M. and Pelillo, M.
(1997). An iterative pruning algorithm for
feedforward neural networks, IEEE Trans.
Neural Networks 8(3) 519531.
Duch, W., Adamczak, R. and Jankwski, N.
(1997). Inicialization and optimization of
multilayered perceptrons, Third Conference
on Neural Networks and Their Applications
pp. 1418.
Frank, A. and Asuncion, A. (2010). UCI Machine Learning Repository, Disponvel em
<httparchive.ics.uci.eduml>. Acesso em
23 fev. 2012 .
Johnson, R. A. and Wichern, D. W. (2007).
Applied multivariate statistical analysis, 6th
edn, Pearson Education.
Karnin, E. D. (1990).
A simple procedure
for pruning back-propagation trained neural networks, IEEE Trans. Neural Networks
1(2) 239242.
Lecun, Y., Denker, J. S. and Solla, S. A. (1990).
Optimal brain damage, Advances in Neural
Information Processing 2 pp. 598605.
Mao, J., Mohiuddin, K. and k. Jain, A. (1994).
Parsimonious network design and feature selection through node pruning, In Proc. International Conf. Pattern Recognition pp. 622
624.

3466

Anais do XIX Congresso Brasileiro de Automática, CBA 2012.

Murase, K., Matsunaga, Y. and Nakade, Y.
(1991). A backpropagation algorithm which
automatically determines the number of association units, In Proc. International Conf.
Neural Networks pp. 783788.
Murphy, P. M. and Aha, D. W. (1994). UCI
Repository of machine learning databases. Irvine, CA University of California, Department of Information and
Computer Science Online,
Disponvel em
<httpwww.ics.uci.edu mlearnMLRepository.html> .
Park, Y. R. (1996). Predicting sun spots using
a layered perceptron neural network, IEEE
Trans. Neural Networks 7(2) 501505.
Reed, R. (1993). Pruning algorithms  a survey,
IEEE Trans. Neural Networks 4(5) 740747.
Romesburg, H. C. (1990). Cluster analysis for
researchers, Krieger Pub Co.
Silvestre, M. R. and Ling, L. L. (1998). Reduzindo a arquitetura de uma rede via gap das
saliencias dos neuronios, Anais V Simposio
Brasileiro de Redes Neurais pp. 9196.
Silvestre, M. R. and Ling, L. L. (2002). Optimization of neural classifiers based on Bayesian
decision boundaries and idle neurons pruning, Proc. International Conf. Pattern Recognition pp. 387390.
Silvestre, M. R. and Ling, L. L. (2006). Avaliacao estatstica de metodos de poda aplicados em neuronios intermediarios da rede_neural mlp, IEEE Latin American Transactions
4(4) 247254.
Silvestre, M. R., Teles, F. H., Oikawa, S. and
Ling, L. L. (2008). A clustering based method
to stipulate the number of hidden neurons
of mlp neural networks application in pattern recognition, Tendencias em Matematica
Aplicada e Computacional 9 351361.

ISBN 978-85-8001-069-5

3467