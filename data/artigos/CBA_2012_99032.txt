Anais do XIX Congresso Brasileiro de Automática, CBA 2012.

MODELO NEBULOSO EVOLUTIVO PARA SISTEMAS DE TEMPO REAL E
DINAMICA RAPIDA
Alisson Marques da Silva, Walmir Matos Caminhas, Andre Paim Lemos, Fernando
Gomide


Programa de Pos-Graduacao em Engenharia Eletrica - Universidade Federal de Minas Gerais
Avenida Antonio Carlos 6627, 31270-901, Belo Horizonte, MG, Brasil


Escola de Engenharia Eletrica e Computacao - Universidade de Campinas
Avenida Albert Einstein, 400, 13083-852, Campinas, SP, Brasil

Emails alissonmarques@cpdee.ufmg.br, caminhas@cpdee.ufmg.br, andrepl@cpdee.ufmg.br,
gomide@dca.fee.unicamp.br
Abstract This paper presents an evolving model inspired in Neo-fuzzy Neuron (NFN). The evolving model
proposed adapts the parameters and the number of membership functions during its execution. The performance
of the proposed model is evaluated through experiments on problem prediction and identification of nonlinear
systems. The Evolving Neo-fuzzy Neuron has a performance comparable with other evolving models and a
computational time roughly an order of magnitude lower. The experiments suggest that model proposed is a
promising approach for modeling adaptive systems in real-time, due to good results and its low computational
cost.
Keywords

Evolving Neural Fuzzy Systems, Neo-fuzzy Neuron, Real-Time Systems.

Resumo Este trabalho apresenta um novo modelo nebuloso evolutivo baseado na rede_neural nebulosa
Neo-fuzzy Neuron (NFN). E proposto um algoritmo_evolutivo, como uma abordagem de aprendizado_incremental
que, simultaneamente, atualiza os parametros da rede e granulariza o domnio das variaveis de entrada. O
desempenho da abordagem proposta e avaliado, por meio de experimentos, em problema de previsao e na
identificacao_de_sistemas_nao_lineares. O modelo Neo-fuzzy Neuron Evolutivo (NFNe) tem um desempenho
comparavel ao de outros modelos evolutivos e um tempo computacional cerca de uma ordem de magnitude
inferior. Os experimentos sugerem o modelo proposto como uma abordagem promissora para modelagem de
sistemas_adaptativos em tempo_real, devido aos bons resultados alcancados e pelo seu baixo_custo_computacional.
Palavras-chave

.

INTRODUCAO

Os sistemas_nebulosos e suas variacoes tem sido
amplamente utilizados em atividades nas quais
sao necessarias capacidades de raciocnio semelhantes as dos seres humanos. Estes sistemas
sao capazes de tratar problemas de modelagem
(Angelov and Buswell, 2002), controle (Barros and
Dexter, 2007), previsao (Lemos et al., 2011b),
classificacao (Angelov et al., 2007), identificacao
e deteccao (Lemos et al., 2010), dentre outros.
Os problemas de modelagem_de_sistemas_dinamicos e de previsao de series_temporais podem
ser vistos como um mapeamento nao linear do espaco de entrada para um espaco de sada. As redes_neurais artificiais, principalmente as redes perceptron_multicamadas, e tambem os sistemas neurais nebulosos vem sendo utilizados para resolver
este tipo de problema. Porem, tanto as redes_neurais quando os modelos neurais nebulosos possuem
uma estrutura fixa, cujos parametros e estrutura
sao estimados durante a etapa de treinamento, e
entao fixos.
O desafio atual e desenvolver metodologias
para construir sistemas que tenham alto grau de
adaptabilidade, autonomia, desenvolvidos a partir de um fluxo de dados coletado em tempo_real
(Angelov et al., 2008). A adaptabilidade e a au-

ISBN 978-85-8001-069-5

tonomia sao obtidas pela alteracao dos parametros e da estrutura do sistema a medida que ocorram alteracoes nos dados de entrada. Os sistemas
nebulosos com tais caractersticas sao denominados sistemas_nebulosos_evolutivos (Evolving Fuzzy
Systems - EFS) (Angelov and Zhou, 2008).
Neste trabalho e proposto um modelo evolutivo baseado na estrutura neural nebulosa denominada Neo-fuzzy Neuron (NFN), desenvolvida por (Yamakawa et al., 1992) e expandida
em (Caminhas et al., 1999 Caminhas and Gomide, 2000). A escolha da topologia do NFN se
da pelo seu baixo_custo_computacional, fator extremamente importante para a implementacao em
sistemas_de_tempo_real e dinamica rapida, aliado
ao seu bom desempenho. A complexidade computacional do NFN foi analisada e comparada com
uma rede_neural artificial (RNA) em (Caminhas
and Gomide, 2000). Os resultados apresentados
mostram que o custo_computacional do NFN e
bem menor que o da RNA e sugerem que o tempo
de processamento do NFN seja muito inferior, se
comparado a RNA.
No NFN (Yamakawa et al., 1992 Caminhas
et al., 1999 Caminhas and Gomide, 2000) o valor modal e o numero de funcoes de pertinencia e
fixo e, consequentemente o numero de regras nebulosas nao varia no processo de aprendizagem.

2016

Anais do XIX Congresso Brasileiro de Automática, CBA 2012.

Na abordagem proposta sao acrescentadas funcionalidades que permitem ao algoritmo evoluir sua
estrutura iterativamente de acordo com o fluxo de
dados, possibilitando que, a cada nova amostra
apresentada ao modelo, os parametros dos conjuntos_nebulosos possam ser modificados e novos
conjuntos_nebulosos adicionados. Os conjunto nebulosos sao adicionados em funcao dos dados de
entrada e do erro. A abordagem proposta tem por
objetivo manter um erro mnimo e uniforme para
todas as regioes do espaco de entrada.
De forma a alcancar os objetivos apresentados, o restante deste artigo esta organizado como
se segue. Na Secao 2 o Neo-fuzzy Neuron (NFN) e
descrito. A Secao 3 detalha o algoritmo de treinamento do NFN. O algoritmo_evolutivo proposto
e apresentado na Secao 4. A Secao 5 aborda a
aplicacao do modelo proposto na previsao de serie
temporal e na identificacao_de_sistemas_nao_lineares e a comparacao dos resultados obtidos com o
de outros modelos evolutivos. Na Secao 6 e realizada a analise da complexidade computacional do
modelo proposto e de outros modelos evolutivos.
Por fim, as conclusoes sao descritas na Secao 7.
2

NEO-FUZZY NEURON - (NFN)

Esta secao descreve a rede_neural nebulosa Neofuzzy Neuron (NFN) (Yamakawa et al., 1992). A
estrutura do NFN pode ser vista por um conjunto
de n estruturas de Takagi-Sugeno (Takagi and Sugeno, 1985) de ordem 0, cujas entradas sao desacopladas e as sadas sao agregadas por uma soma
algebrica. A estrutura do NFN e ilustrada na Figura 1 e sua sada pode ser representada por (1).

paco de entrada e realizada por m funcoes de pertinencia triangulares e complementares (Figura 2).
A partir dessa particao do espaco de entrada temse as seguintes m regras

Ri1
Ri2

Rim

Se xi e Ai1
Se xi e Ai2
.
.
.
Se xi e Aim

Entao yi e yi1  qi1
Entao yi e yi2  qi2

Entao

yi e yim  qim

Os valores de fi (xi ) podem ser determinados
por (2)
fi (xi )  yi 

Pm
j1 wij yij
P
m
j1 wij



a
b

(2)

onde, wij  Aij (xi ) e yij  qij .
Para cada variavel de entrada xi , no maximo
duas das m regras sao ativas, sendo indexadas por
ki e ki + 1. Uma funcao e dita ativa quando seu
grau de pertinencia e maior que zero. A Figura
2 mostra um exemplo com as duas funcoes ativadas pela variavel de entrada xi . Nesse exemplo
somente as regras Ri1 e Ri2 sao ativadas, pois sao
referentes aos conjuntos_nebulosos Ai1 e Ai2 respectivamente . O valor de ki e dado pelo ndice
da primeira funcao ativa, que e obtido pela menor
diferenca positiva entre a variavel de entrada xi e
o valor modal das funcoes de pertinencia.

Figura 2 Funcoes de Pertinencia Triangulares e
Complementares.
Assumindo que as funcoes de pertinencia sao
complementares b pode ser definido por (3)
b

Pm

j1

Aij (xi )  Aik (xi ) + Aik
i

fi (xi )  a  Aik (xi )qiki + Aik
i

y  f1 (x1 ) + ... + fn (xn ) 

i1

yi (xi ) 

n
X

fi (xi )

(1)

i1

As sadas yi sao definidas por um conjunto de
regras nebulosas de Takagi-Sugeno. Para cada variavel do universo de discurso xi , a particao do es-

ISBN 978-85-8001-069-5

(xi )  1.

(3)

Assim, a expressao final para fi (xi ) pode ser
descrita por (4)

Figura 1 Estrutura do Neo-fuzzy Neuron.

n
X

i +1

3

i +1

(xi )qiki +1 .

(4)

ALGORITMO DE APRENDIZADO
DO NFN

Nesta secao e apresentado o metodo de treinamento do NFN com taxa de aprendizado otima.

2017

Anais do XIX Congresso Brasileiro de Automática, CBA 2012.

E empregado o treinamento supervisionado, no
qual sao ajustados os parametros qij . O procedimento de atualizacao dos pesos descrito a seguir
consiste de atualizacao incremental. Outros procedimentos, tais como, atualizacao em lote e atualizacao incremental com termo de Momentum, foram apresentados em (Yamakawa et al., 1992).
Dado um conjunto de entrada X, sendo xt 
(xt1 , xt2 , ..., xti , ..., xtn ) o t-esimo padrao de entrada yt a sada estimada pelo NFN e ytd a sada
desejada, onde n e numero de variaveis por padrao
de entrada. Com os valores de yt e ytd pode-se calcular o erro quadratico definido por (5)
1
(yt  ytd )2  Et (qij )
(5)
2
O aprendizado do NFN e realizado pela atualizacao dos pesos para cada padrao de entrada, de
forma a minimizar o erro Et . Para cada coordenada do padrao de entrada apresentada ao NFN,
somente as funcoes iki (xti ) e iki +1 (xti ) sao diferentes de zero. Assim, a funcao de minimizacao
do erro pode ser descrita como (6)
Et 

min Et  Et (qij )  Et (q),

(6)

com qij  < ,  i  1, ..., n e j  ki , (ki + 1), onde
q e calculado por (7)
q  q1k1 q1(k1 +1) ...qiki qi(ki +1) ...qnkn qn(kn +1) . (7)
Se adotado o erro quadratico para ajuste dos
parametros. Independente do numero de funcoes
de pertinencia, a funcao a ser minimizada Et (q)
possui 2n variaveis e e uma funcao quadratica e
convexa (Caminhas and Gomide, 2000) e o mnimo local e tambem mnimo global (Uchino and
Yamakawat, 1994).
Como Et (q) e uma funcao nao linear, a convergencia para a solucao otima depende do ponto
inicial, da direcao de busca e do tamanho_do_passo.
Para o NFN a condicao inicial pode ser nula, sem
comprometer a convergencia do algoritmo. Para
a escolha da direcao de busca sera adotado o metodo do gradiente por ser o metodo mais simples.
j
Portanto, qik
e atualizada por (8)
i
j+1
j
qik
 qik
 j (ytj  ytd )iki (xti )
i
i

1
2+
2

(x
)
ik
ti
iki +1 (xti )
i
i1

(9)

  Pn

Somente as funcoes ativadas sao relevantes
para o processamento da rede. Por isso, so os
pesos referentes a estas funcoes sao ajustados no
aprendizado do NFN. Isso explica por que o NFN
tem um menor tempo de aprendizado e processamento do que os modelos alternativos de redes
neurais feedforward.
4

NEO-FUZZY NEURON EVOLUTIVO
(NFNe)

O modelo Neo-fuzzy Neuron Evolutivo (NFNe)
proposto neste trabalho utiliza a atualizacao dos
pesos com taxa de aprendizado otima descrita na
Secao 3. Ao contrario do modelo original no qual
o valor modal das funcoes de pertinencia, a quantidade de funcoes e o numero de regras sao fixos,
no modelo evolutivo estes podem ser alterados iterativamente de acordo com o fluxo de dados.
O algoritmo proposto pode ser descrito em
tres etapas como se segue 1) calcule os parametros iniciais das funcoes de pertinencia 2) calcule
para cada padrao de entrada xt o grau de ativacao das funcoes de pertinencia e encontre a funcao mais ativa. Decida se o valor modal da funcao
mais ativa deve ser alterado 3) analise se a funcao
de pertinencia mais ativa representa bem a regiao
do espaco de entrada a que pertence o ponto xt .
Decida se uma nova funcao de pertinencia deve
ser criada e inserida naquela regiao.
A primeira etapa e executada uma unica vez
na inicializacao do modelo. A segunda e a terceira
etapa sao executadas para cada um dos t padroes
de entrada. A seguir, nas proximas secoes as etapas do algoritmo sao detalhadas.

(8)

A taxa de aprendizado , e chamada de tamanho_do_passo no algoritmo do gradiente e,
pode ser determinada empiricamente ou utilizando algum metodo de busca unidirecional indireto ou direto (Bazaraa et al., 2006). Neste trabalho, sera utilizado, para modificacao dinamica
da taxa de aprendizado a alternativa proposta por
(Caminhas and Gomide, 2000) por levar em consideracao o fato da funcao objetivo a ser minimizada
ser quadratica. Dado um padrao de entrada xt e
a sada desejada ytd , e possvel determinar uma

ISBN 978-85-8001-069-5

expressao fechada para a taxa de aprendizado 
tal que utilizando o metodo do gradiente, os pesos do NFN que proporcionam erro de aproximacao nulo pode ser determinado em um passo. O
resultado do teorema descrito em (Caminhas and
Gomide, 2000) e dado por (9), onde, independente
da condicao inicial q 0 , e possvel encontrar q 1 tal
que o erro seja nulo para o padrao t.

4.1

Inicializacao das Funcoes de Pertinencia

As funcoes de pertinencia utilizadas no modelo sao
triangulares e complementares. Uma funcao de
pertinencia triangular pode ser representada por
tres parametros o limite inferior (a), o valor modal (b) e o limite superior (c). Como as funcoes sao
complementares, a k-esima funcao e definida pelo
seu valor modal (bk ), o limite inferior e dado pelo
valor modal funcao adjacente inferior (ak  bk1 )
e o limite superior pelo valor modal da funcao adjacente superior (ck  bk+1 ).

2018

Anais do XIX Congresso Brasileiro de Automática, CBA 2012.

O numero inicial de funcoes de pertinencia m
e definido empiricamente. Neste trabalho optou-se
por iniciar o algoritmo com duas funcoes de pertinencia para cada uma das n variaveis do padrao
de entrada. O valor modal das funcoes e definido
por (10)
bij  xmini + (j  1)  i

(10)

onde, i indexa a variavel do padrao de entrada,
j indexa a funcao de pertinencia, xmini e menor
valor para a variavel i e i e calculado por (11)
(xmaxi  xmini )
(11)
mi  1
onde, xmaxi e maior valor para a variavel i e mi
numero de funcoes para a variavel i.
i 

4.2

Atualizacao do Valor Modal das Funcoes de
Pertinencia

Para cada variavel i do padrao de entrada xt e
encontrada o ndice da funcao de pertinencia mais
ativa (bi ), isto e, das duas funcoes ativadas a que
possui maior grau de pertinencia. Se bi > 1 e
bi < mi , entao a funcao tem o seu valor modal
atualizado por (12). Caso contrario, se bi  1 ou
bi  mi , o valor modal da funcao mais ativa nao
sera atualizado, pois, neste caso, o valor modal da
funcao e igual respectivamente a xmini e xmaxi .
bijt  bijt1 + (xti  bijt1 )

Criacao e Insercao de Funcoes de Pertinencia

A metodologia proposta para criacao e insercao
de funcoes de pertinencia tem por objetivo manter
um erro mnimo e uniforme para todas as regioes
do espaco de entrada. Para isso, e analisado se a
media do erro da funcao de pertinencia mais ativa
e maior que o erro geral do modelo. Caso positivo,
indica que aquela funcao nao representa bem a
regiao e que esta regiao precisa ser refinada. O
refinamento da regiao e realizado pela insercao de
uma nova funcao de pertinencia. O procedimento
para criacao de funcoes de pertinencia e detalhado
a seguir.
Para cada padrao de entrada xt e calculado
2
recursivamente o valor medio m e a variancia m
do erro do modelo, respectivamente por
mt  mt1  (mt1  et )

2
2
m
 (1  )(m
+ (mt  et )2 ).
t
t1

onde, et  yt  ytd .

ISBN 978-85-8001-069-5

bt  bt1  (bt1  et )

(15)

Para a criacao de uma nova funcao de pertinencia sao utilizados o erro medio do modelo
2
(m ) a variancia do modelo (m
) e o erro medio
da funcao mais ativa (b ). Se o erro medio da funcao mais ativa foi maior que a soma do erro medio
do modelo com a variancia do modelo uma nova
funcao e criada. A funcao e criada com base na
funcao mais ativa (bi ), conforme descrito a seguir
 Caso 1 se bi 6 1 e bi < mi , entao a funcao
mais ativa sera dividida em duas funcoes e o
valor modal das novas funcoes sera definido
por (16) e (17).

nf1  bi,bi 1 +

(bi,bi +1  bi,bi 1 )
3

nf2  bi,bi 1 + 2 

(16)

(bi,bi +1  bi,bi 1 )
(17)
3

 Caso 2 se bi  1, entao uma nova funcao e
criada entre a primeira e a segunda, e o valor
modal da funcao criada e dado por (18).

(12)

onde,  e um parametro definido empiricamente.
O valor tpico utilizado nos experimentos e  
0.01.
4.3

Para a funcao de pertinencia mais ativa (bi )
do padrao de entrada xt e calculado recursivamente a media do erro local da funcao b (15)

(13)

(14)

nf  bi,bi +

(bi,bi +1  bi,bi )
2

(18)

 Caso 3 se bi  mi , entao uma nova funcao
sera criada entre a penultima e a ultima e tera
seu valor modal, conforme descrito por (19).
nf  bi,bi 

(bi,bi  bi,bi 1 )
2

(19)

A Figura 3 apresenta um exemplo de criacao
de funcoes de pertinencia para cada um dos tres
casos a Figura 3a mostra o espaco de entrada
dividido inicialmente em tres funcoes de pertinencia a Figura 3b ilustra o espaco de entrada apos
a criacao das funcoes para o Caso 1 a Figura 3c
apresenta o espaco de entrada apos a criacao da
funcao para o Caso 2, e a Figura 3d mostra o espaco de entrada apos a criacao da funcao para o
Caso 3.
Nos tres casos, as funcoes criadas recebem o
erro medio da funcao mais ativa e o numero de
funcoes para aquela variavel e incrementado em
um. Note que, a criacao de uma nova funcao altera os limites das funcoes vizinhas a funcao criada
(como pode ser visto na Figura 3), porem o valor
modal das funcoes vizinhas nao e alterado. A alteracao nos limites inferior ou superior e realizada
para que as funcoes continuem sendo complementares.

2019

Anais do XIX Congresso Brasileiro de Automática, CBA 2012.

resduos dos dois modelos. A estatstica do teste e
dada por uma distribuicao t de Student com n  1
graus de liberdade. Para esse teste, se os modelos
sao igualmente precisos, a correlacao entre s e d
sera zero.
5.1

Figura 3 Criacao de funcao de pertinencia.

5

O modelo Neo-fuzzy Neuron Evolutivo1 (NFNe)
foi avaliado e os resultados obtidos foram comparados com quatro sistemas_evolutivos DENFIS2 (Kasabov and Song, 2002), eMG3 (Lemos
et al., 2011b), eTS4 (Angelov and Filev, 2004) e
xTS4 (Angelov and Zhou, 2006).
Os experimentos foram executados de forma
on-line, isto e, os parametros e a estrutura dos
modelos evoluem para todas as amostras do conjunto de dados. Os modelos sao avaliados pelo
RMSE (Root Mean Squared Error ) e pelo NDEI
(Non Dimensional Error Index ). O RMSE e calculado por (20) e o NDEI (21) e a razao entre o
RMSE e o desvio padrao da sada desejada
N
1
1 X
(yt  yt )) 2
N t1

N DEI 

RM SE
std(yt )

sd
12sd
n1

(23)

onde, x(0)  1.2 e   17. O objetivo e realizar a
previsao de xt+85 para qualquer valor de t tendo
como entrada os valores de xt18 xt12 xt6 xt .
O conjunto de dados e composto por 3500
amostras, sendo que, 3000 foram coletadas com
t  201, 3200 e 500 com t  5001, 5500
(Kasabov and Song, 2002). O desempenho dos
modelos foi avaliado por meio do RMSE, NDEI e
MGN para as 700 ultimas amostras.
A Figura 4 ilustra as funcoes de pertinencia
iniciais e as finais do NFNe e a Figura 5 mostra a
evolucao da estrutura (numero de funcoes de pertinencia) do modelo para cada variavel de entrada.
A Figura 6 ilustra a sada desejada e a sada estimada pelo NFNe para as 250 primeiras amostras,
onde percebe-se a rapida convergencia do modelo.

(20)

(21)

onde, N e o numero de elementos do conjunto de
teste, yt e a sada desejada, yt e a sada do modelo
e std() e a funcao do desvio padrao. Para fins de
avaliacao dos modelos as medidas de desempenho
foram calculadas para os 20 ultimos pontos.
As medidas de erros sao boas para medir a
precisao do modelo. Porem, elas nao revelam se
um modelo e estatisticamente superior a outro
(Lemos et al., 2011a). Para isso, sera utilizado um
teste estatstico para comparar a precisao dos modelos. O teste MGN (Diebold and Mariano, 1995)
e um teste parametrico empregado para comparar
a acuracia de dois modelos e e definido por
M GN  q

Nos experimentos de previsao de serie_temporal de
longo prazo e empregada a serie de Mackey-Glass
(Mackey and Glass, 1977). A serie e criada com
o uso da equacao diferencial com tempo de atraso
(23)
0.2x(t   )
dx(t)

dt
1 + x10 (t   )

Experimentos

RM SE  (

Previsao de Serie Temporal de Longo Prazo

Figura 4 Funcoes de pertinencia iniciais e finais.

(22)

onde sd e o coeficiente de correlacao estimado
entre s  r1 + r2 , e d  r1  r2 , sendo r1 e r2 os
1 Implementado

em Matlab.
em Matlab disponvel em httpwww.aut.
ac.nzresearchresearch-instituteskedribooks).
3 Versao em Matlab fornecida pelos autores.
4 Versao em Java fornecida pelos respectivos autores.
2 Versao

ISBN 978-85-8001-069-5

Figura 5 Evolucao das funcoes de pertinencia.

2020

Anais do XIX Congresso Brasileiro de Automática, CBA 2012.

mensionalidade do espaco de entrada. O sistema
nao-linear a ser identificado e descrito por 24
Pm
yti
Pi1
+ ut1
(24)
yt 
m
1 + i1 (yti )2
onde, ut  sin(2t20), e yj  0 para j  1, ..., m
e m  10 (Lemos et al., 2011b). O objetivo e prever a corrente de sada utilizando a entrada com
atraso e as sadas. O modelo para este conjunto
de dados e definido por (25)

Figura 6 Previsao da serie_temporal MackeyGlass
Os parametros dos modelos foram definidos
como se segue DENFIS dthr
mof n  3
P  0.1 e 2
eMG   0.05, w  30,
.I3 ,  
init  10
0.01 eTS r  0.06 e   750 xTS   750.
A Tabela 1 mostra os resultados experimentais obtidos, descritos pelo numero de regras e pelas medidas de erro. Esses sugerem que o melhor
desempenho foi obtido pelo DENFIS, seguido pelo
NFNe e eMG. Os resultados alcancados por estes
tres algoritmos sao comparaveis e superam os obtidos pelo eTS e xTS em uma ordem de grandeza.

yt  f (yt1 , yt2 , ..., yt10 , ut1 )

(25)

onde, yt e a sada do modelo.
Para o experimento foram criadas 3300 amostras, sendo estas utilizadas no processo de aprendizagem e evolucao dos modelos. Figura 7 ilustra
a sada desejada e a sada do NFNe para as ultimas 800 observacoes.

Tabela 1 Desempenho on-line dos modelos na
previsao da serie de Mackey-Glass.
Modelo
DENFIS
eMG
NFNe
eTS
xTS

N. Regras
27
08
36
05
09

RMSE
0.0627
0.0843
0.0828
0.3761
0.3761

NDEI
0.2758
0.3713
0.3644
1.6554
1.6619

A Tabela 2 ilustra as comparacoes entre o
NFNe e os demais modelos analisados utilizando
o teste MGN (Diebold and Mariano, 1995). A tabela mostra a estatstica MGN e o p-valor correspondente. Os resultados apresentados mostram
que o DENFIS foi estatisticamente superior ao
NFNe, que o NFNe foi superior estatisticamente
ao eTS e xTS e obteve um desempenho semelhante
ao eMG.
Tabela 2 Avaliacao do teste MGN na previsao da
serie de Mackey-Glass.
Modelo
DENFIS
NFNe
NFNe
NFNe

5.2

vs
vs
vs
vs

NFNe
eMG
eTS
xTS

MGN
8.4109
0.2456
5.2205
4.8901

p-valor
0.0000
0.4030
0.0000
0.0000

Identificacao de Sistema Nao-Linear com
Alta Dimensionalidade

Este experimento visa analisar o comportamento
do modelo proposto em problemas com alta di-

ISBN 978-85-8001-069-5

Figura 7 Identificacao de sistema linear com alta
dimensionalidade
As medidas de desempenho dos modelos foram calculadas para as ultimas 660 amostras. Os
parametros dos modelos foram definidos como se
segue DENFIS dthr
P  0.1 e mof n  3 eMG
  0.05, w  25, init  101 .I11 ,   0.01
eTS r  0.06 e   750 xTS   750.
Os resultados experimentais sao apresentados
na Tabela 3. Estes indicam que o melhor desempenho foi obtido pelo eMG. O NFNe obteve o segundo melhor desempenho entre os modelos testados. Os resultados do eMG e NFNe superam os
dos DENFIS, eTS, e xTS em cerca de duas ordens
de grandeza.
Tabela 3 Desempenho on-line dos modelos na
identificacao de sistema nao linear.
Modelo
DENFIS
eMG
NFNe
eTS
xTS

N. Regras
17
9
86
23
9

RMSE
1.4389
0.0288
0.0591
0.8355
0.8358

NDEI
1.5267
0.0306
0.0628
0.8865
0.8868

A Tabela 4 mostra as comparacoes entre o
NFNe e os outros modelos analisados utilizando o
teste de MGN (Diebold and Mariano, 1995). Os

2021

Anais do XIX Congresso Brasileiro de Automática, CBA 2012.

resultados apresentados na Tabela 4 mostra evidencias que o NFNe e estatisticamente superior
ao DENFIS, eTS, e xTS e inferior ao eMG.
Tabela 4 Avaliacao do teste MGN na identificacao_de_sistemas nao linear.
Modelo
NFNe vs
eMG vs
NFNe vs
NFNe vs

6

DENFIS
NFNe
eTS
xTS

MGN
272.6573
20.8403
143.9223
143.9907

p-valor
0.0000
0.0000
0.0000
0.0000

Analise do Custo Computacional

A analise do custo_computacional de um algoritmo
pode ser realizada por duas abordagens 1) expressar o tempo de execucao por meio de uma
expressao matematica 2) medir o tempo de processamento. A seguir as duas abordagens sao empregadas para avaliar o modelo proposto.
6.1

Analise da Ordem de Complexidade

O NFNe possui uma complexidade computacional
linear para avaliar, atualizar os parametros e estimar a sada. A complexidade do algoritmo e definida em funcao do numero de variaveis de entrada
e e da ordem de O(n), onde n e o numero de variaveis de entrada. O numero de regras nebulosas
nao interfere no custo_computacional do algoritmo
pois, independente do numero de regras nebulosas,
no maximo 2n regras sao ativadas para cada variavel de entrada e somente os pesos referentes as
regras ativadas sao ajustados no aprendizado.
Geralmente, em um modelo nebuloso evolutivo tpico, baseado em agrupamento, a complexidade e definida em funcao do numero de grupos e
de variaveis de entrada, sendo no mnimo O(ng),
onde g e o numero de grupos.
Um fator importante que deve ser considerado alem da ordem de complexidade e a complexidade das operacoes executadas pelos algoritmos.
A complexidade dos calculos computacionais e um
fator de grande influencia no tempo de processamento dos modelos.
Na analise da ordem de complexidade dos algoritmos pode-se perceber que o NFNe possui a
menor ordem de complexidade, ele e o unico algoritmo que a ordem de complexidade e dada somente em funcao das variaveis de entrada. Note
que, as operacoes executadas pelo NFNe sao mais
simples, o que indica em conjunto com a menor
ordem de complexidade que o tempo de processamento do NFNe seja muito menor que o dos
demais algoritmos.
6.2

Analise do Tempo de Processamento

Os experimentos desta secao tem como objetivo
avaliar o tempo de processamento dos algoritmos.

ISBN 978-85-8001-069-5

Os modelos foram executados com os mesmos parametros utilizados na Secao 5. A Tabela 5 mostra
o tempo de execucao dos algoritmos na previsao
da serie de Mackey-Glass e a Tabela 6 na identificacao de sistema nao linear com alta dimensionalidade. Os tempos estao expressos em milissegundos (ms) e se referem ao tempo medio e desvio
padrao para processamento de cada amostra do
conjunto de dados. Esses foram calculados com
base em 10 repeticoes de cada experimento.
Tabela 5 Tempo de execucao dos algoritmos para
previsao da serie de Mackey-Glass.
Modelo
DENFIS
eMG
NFNe
eTS
xTS

N. Regras
27
08
36
05
09

Tempo (ms)
2.6341
3.2818
0.2672
1.5101
1.7586

Desvio Padrao
0.0130
0.0316
0.0130
0.0114
0.0088

Tabela 6 Tempo de execucao dos algoritmos na
identificacao de sistema nao linear.
Modelo
DENFIS
eMG
NFNe
eTS
xTS

N. Regras
17
09
86
23
09

Tempo (ms)
2.7533
3.6015
0.5394
215.7535
12.8427

Desvio Padrao
0.0165
0.0117
0.0075
0.3786
0.1825

Analisando o tempo de execucao dos algoritmos verifica-se que o NFNe possui o menor custo
computacional entre os algoritmos analisados. O
tempo de processamento do NFNe e cerca de uma
ordem de grandeza menor do que o segundo algoritmo mais rapido.
7

Conclusao

Este artigo introduziu um novo modelo neural nebuloso evolutivo (NFNe) que foi construdo sob a
estrutura do Neo-fuzzy Neuron. O modelo utiliza
funcoes de pertinencia triangulares e complementares. O NFNe possui uma abordagem incremental que, simultaneamente, granulariza o espaco
das variaveis de entrada e atualiza os pesos da
rede. A estrutura do modelo evolui pela insercao
de funcoes de pertinencia de acordo com os dados de entrada e com o erro e seus parametros sao
ajustados por um metodo baseado no gradiente
com taxa de aprendizado otima.
O NFNe foi avaliado na previsao de serie_temporal e na identificacao de sistema nao linear com
alta dimensionalidade. Os resultados experimentais mostram que o modelo proposto possui um
desempenho comparavel aos demais modelos evolutivos e um custo_computacional menor em cerca
de uma ordem de grandeza. Esses sugerem que o
modelo proposto seja uma promissora alternativa
para a construcao de modelos evolutivos. As equacoes simples, a baixa complexidade computacional

2022

Anais do XIX Congresso Brasileiro de Automática, CBA 2012.

e boa acuracia credenciam o NFNe para aplicacoes
em sistemas_de_tempo_real com dinamica rapida.
Agradecimentos
O presente trabalho foi realizado com o apoio financeiro da CAPES - Brasil. Os primeiros autores deste trabalho agradecem o apoio da CAPES,
CNPq e FAPEMIG. O ultimo autor e grato ao
CNPq, processos 3045962009-4 e 4711382010-0.
Referencias
Angelov, P. and Buswell, R. (2002). Identification
of evolving fuzzy rule-based models, IEEE
Transactions on Fuzzy Systems 10(5) 667 
677.
Angelov, P. and Filev, D. (2004). An approach to online identification of takagi-sugeno
fuzzy models, IEEE Transactions on Systems, Man and Cybernetics, Part B Cybernetics 34(1) 484  498.
Angelov, P., Filev, D. and Kasabov, N. (2008).
Guest editorial evolving fuzzy systems  Preface to the special section, IEEE Transactions on Fuzzy Systems 16(6) 1390  1392.
Angelov, P. and Zhou, X. (2006). Evolving fuzzy
systems from data streams in real-time, Proceedings of the International Symposium on
Evolving Fuzzy Systems, pp. 29  35.
Angelov, P. and Zhou, X. (2008). Evolving
fuzzy-rule-based classifiers from data streams, IEEE Transactions on Fuzzy Systems
16(6) 1462  1475.
Angelov, P., Zhou, X., Filev, D. and Lughofer, E.
(2007). Architectures for evolving fuzzy rulebased classifiers, Proceedings of the IEEE International Conference on Systems, Man and
Cybernetics, pp. 20502055.
Barros, J. and Dexter, A. (2007). Evolving fuzzy
model-based adaptive_control, Proceedings of
the IEEE International Conference on Fuzzy
Systems, FUZZ-IEEE 07, pp. 1  5.
Bazaraa, M., Sherali, H. and Shetty, C. (2006).
Nonlinear Programming Theory and Algorithms, 3 edn, John Wiley  Sons,.

Caminhas, W., Tavares, H., Gomide, F. and Pedrycz, W. (1999). Fuzzy set based neural networks Structure, learning and application, Journal of Advances Computational
Intelligence and Intelligent Informatics (JACIII) 3(3) 151  157.
Diebold, F. and Mariano, R. (1995). Comparing
predictive accuracy, Journal of Business 
Economic Statistics 13(3) 25363.
Kasabov, N. and Song, Q. (2002). Denfis Dynamic evolving neural-fuzzy inference system and its application for time-series prediction, IEEE Transactions on Fuzzy Systems
10(2) 144  154.
Lemos, A., Caminhas, W. and Gomide, F. (2010).
Fuzzy multivariable gaussian evolving approach for fault detection and diagnosis, Computational Intelligence for Knowledge-Based
Systems Design, Vol. 6178 of Lecture Notes
in Computer Science, Springer Berlin  Heidelberg, pp. 360369.
Lemos, A., Caminhas, W. and Gomide, F.
(2011a). Fuzzy evolving linear regression
trees, Evolving Systems 2 114.
Lemos, A., Caminhas, W. and Gomide, F.
(2011b). Multivariable gaussian evolving
fuzzy modeling system, IEEE Transactions
on Fuzzy Systems 19(1) 91  104.
Mackey, M. and Glass, L. (1977). Oscillation and
chaos in physiological control systems, Science 197(4300) 287  289.
Takagi, T. and Sugeno, M. (1985). Fuzzy identification of systems and its applications to
modeling and control, IEEE Transactions on
Systems, Man and Cybernetics 15(1) 116 
132.
Uchino, E. and Yamakawat, T. (1994). Neo-fuzzyneuron based new approach to system modeling with application to actual system, Proceedings of the International Conference on
Tools with Artificial Intelligence, pp. 564 
570.
Yamakawa, T., Uchino, E., Miki, T. and Kusabagi, H. (1992). A neo fuzzy neuron and its
applications to system identification and predictions to system behavior, Proceedings of
the International Conference on Fuzzy Logic
and Neural Networks, Vol. 1, pp. 477  484.

Caminhas, W. and Gomide, F. (2000). A fast learning algorithm for neofuzzy networks, Proceedings of the Information Processing and Management of Uncertainty in Knowledge Based
Systems, IPMU 00, Vol. 1, Madrid, Spain,
pp. 1784  1790.

ISBN 978-85-8001-069-5

2023