Anais do XIX Congresso Brasileiro de Automática, CBA 2012.

DESENVOLVIMENTO DE UM SISTEMA DE APRENDIZADO POR REFORCO
PARA TIMES DE ROBOS - UMA ANALISE DE DESEMPENHO POR MEIO DE
TESTES ESTATISTICOS
Andre Luiz Carvalho Ottoni, Rubisson Duarte Lamperti, Erivelton Geraldo
Nepomuceno, Marcos Santos de Oliveira


Grupo de Controle e Modelagem, Departamento de Engenharia Eletrica, Universidade Federal de Sao
Joao del-Rei, Praca Frei Orlando, 170, Centro, 36307-352 - Sao Joao del-Rei, MG, Brasil



Grupo de Controle e Modelagem, Programa de Pos-Graduacao em Engenharia Eletrica, Universidade
Federal de Sao Joao del-Rei, Praca Frei Orlando, 170, Centro, 36307-352 - Sao Joao del-Rei, MG,
Brasil


Departamento de Matematica e Estatstica, Universidade Federal de Sao Joao del-Rei, Praca Frei
Orlando, 170, Centro, 36307-352 - Sao Joao del-Rei, MG, Brasil

Emails andreottoni@ymail.com, duartelamperti@yahoo.com.br, nepomuceno@ufsj.edu.br,
mso@ufsj.edu.br
Resumo O presente trabalho apresenta uma metodologia de modelagem da tecnica de aprendizado por
reforco para times de futebol_de_robos 2D. A implementacao da estrategia de aprendizagem consiste de quatro
etapas definicao e discretizacao das acoes dos agentes definicao e discretizacao dos estados do ambiente no
qual os agentes estao inseridos definicao dos valores dos reforcos da tabela R, para cada par Estado (S) X Acao
(A) implementacao no Simulador RcSoccerSim da Robocup de Futebol de Robos. Alem disso, e proposta uma
analise_estatstica por meio do teste T-Pareado para compreender o sistema de aprendizagem quando submetido
a competicao entre outros times.
Palavras-chave



Abstract This paper presents a methodology for modeling the technique of reinforcement learning for teams
of robots 2D. The implementation of the learning strategy consists of four steps definition and discretization
of the actions of agents definition and discretization of the states of the environment in which the agents are
inserted, defining the values of the reinforcement R table for each pair state (S) X Action (A), implementation
of the simulator of RoboCup Robot Soccer. Furthermore, we propose a statistical analysis by paired t-test to
understand the learning system when subjected to competition from other teams.
Keywords

Reinforcement Learning, Robot Soccer, Statistical Analysis

1

Introducao

A possibilidade do desenvolvimento de maquinas
ou sistemas Inteligentes tem sido o alvo de pesquisadores de diversas partes do mundo (Russell,
2004). Na decada de 90, as maquinas inteligentes tornaram-se realidade e estao cada vez mais
presentes na sociedade (Nascimeno Jr., 2009 Texeira et al., 2009 Lacerda et al., 2009a Garcia, 2003 Lacerda et al., 2009b). Dentre os diversos exemplos da utilizacao de maquinas inteligentes estao o uso de sistemas especialistas em
tomadas de decisoes, o uso da Inteligencia Computacional para o controle_de_processos_industriais,
e a utilizacao de robos na realizacao de tarefas
repetitivas ou de elevado risco para o ser humano.
A complexidade de algumas tarefas pode se
tornar ainda maior quando se trata de maquinas
autonomas, ou seja, maquinas capazes de estabelecerem acoes devido a eventos dinamicos imprevisveis (Kim and Oh, 2010). Porem, a evolucao
nesse cenario traz grandes avancos as pesquisas
tecnologicas (Garcia, 2003).
A Inteligencia Artificial (IA) e uma area da
ciencia que desenvolve a autonomia de maqui-

ISBN 978-85-8001-069-5

nas. Varias tecnicas de inteligencia_artificial sao
adotadas como, algoritmos_geneticos (Fraccaroli,
2010a), aprendizado_por_reforco (Silva et al., 2010
Kerbage et al., 2010), aprendizado_por_reforco acelerado por heursticas (Celiberto Jr, 2006), redes
neurais artificiais (Simoes et al., 2007) e sistemas
fuzzy (Fraccaroli, 2010a) na tentativa de encontrar
a melhor solucao para o problema.
O Aprendizado por Reforco (AR) e uma tecnica de aprendizado, na qual o agente aprende por
meio de interacao direta com o ambiente e seu algoritmo converge para uma situacao de equilbrio
(Sutton, 1998). No AR, um agente pode aprender em um ambiente nao conhecido previamente,
por meio de experimentacoes. Dependendo de sua
atuacao, o agente recebe uma recompensa ou uma
penalizacao e, desta forma, o algoritmo encontra
um conjunto de acoes que levam o agente a percorrer o caminho otimo. A este conjunto, formado pelas melhores acoes, da-se o nome de poltica otima.
Na busca de desenvolver a Inteligencia Artificial (IA) e a robotica, uma importante organizacao internacional, a Robocup, escolheu o jogo
de futebol como o principal topico de pesquisa,
procurando inovacoes que pudessem ser aplicadas

3557

Anais do XIX Congresso Brasileiro de Automática, CBA 2012.

em problemas reais. Para o desenvolvimento de
times de robos sao necessarias varias tecnologias,
como exemplo a cooperacao de sistemas_multiagentes, a aquisicao de estrategias, o sensoreamento
em tempo_real, a fusao_de_sensores, entre outros.
A categoria de simulacao 2D da Robocup simula partidas de futebol_de_robos_autonomos.
Nesta liga existem robos (agentes) virtuais, o qual
todo o ambiente e simulado. Um simulador fornece aos agentes todos os dados que seriam obtidos na realidade por meio dos seus sensores e calcula o resultado das acoes de cada agente. Cada
jogador e visto como um agente individual, e o
time como um sistema_multiagente totalizando 11
(onze) jogadores por equipe.
Este trabalho apresenta uma metodologia
para a Aprendizagem por Reforco para times de
futebol_de_robos 2D. Alem disso, e proposta uma
analise_estatstica por meio do teste T-Pareado
para compreender o sistema de aprendizagem
quando submetido a competicao entre equipes que
utilizam de outras tecnicas de inteligencia_artificial.
Este artigo esta organizado em secoes, a qual a
Secao 2 introduz o conceito da Aprendizagem por
reforco, algoritmo Q-learning (Watkins, 1989), e
o teste T-Pareado (Hines et al., 2006). Na Secao
3 e apresentada a implementacao da Estrategia
de Aprendizagem, por meio da Discretizacao das
Acoes, dos Estados e a Definicao da Matriz de
Recompensa. A analise dos resultados obtidos e
apresentada Secao 4. Em seguida, a conclusao do
trabalho e apresentada na Secao 6.
2
2.1

Fundamentacao Teorica

Processos de Decisao de Markov

Um Processo de Decisao de Markov (MDP - Markov Decision Process) e uma forma de modelar
processos, na qual as transicoes entre estados sao
probabilsticas. No MDP e possvel observar e
interferir no processo periodicamente executando
acoes (Bertsekas, 2001 Puterman, 1994). Cada
acao tem uma recompensa, a qual depende do processo. A definicao de recompensas pode ser dada
em funcao apenas do estado, sem que estas dependam da acao executada. Os processos sao ditos de
Markov (ou Markovianos) quando sao modelados
obedecendo a propriedade de Markov, ou seja, o
efeito de uma acao em um estado depende apenas da acao e do estado atual do sistema, e nao
de como o processo chegou a tal estado. Os processos de decisao modelam a possibilidade de um
agente (ou tomador de decisoes) interferir periodicamente no sistema executando acoes, diferentemente da Cadeia de Markov, na qual nao se interfere no processo. MDPs podem ser aplicados
diversas areas (White, 1993).
A cada epoca de decisao, o agente tomador

ISBN 978-85-8001-069-5

de decisoes usa uma regra de decisao para escolher a proxima acao. Uma forma simples de regra
de decisao e um mapeamento_direto de estados
em acoes. O conjunto de todas as regras de decisao (uma para cada epoca de decisao) e chamado
de poltica. Normalmente, o objetivo e encontrar
uma poltica que otimize um criterio de desempenho das decisoes.
Uma especificacao das probabilidades de resultados para cada acao em cada estado possvel
e chamada de modelo de transicao, denotado por
T (s, a, s). T (s, a, s) e utilizado para denotar a probabilidade de alcancar o estado s se a acao a for
executada no estado s (Russell, 2004).
Um MDP e definido pela quadrupla
(S, A, T, R) onde (Bianchi, 2004)
 S e um conjunto finito de estados do ambiente
 A e um conjunto finito de acoes que o agente
pode realizar
 T  SxA  (S) e a funcao de transicao
de estado, onde (S) e uma funcao de probabilidades sobre o conjunto de estados S.
T (st , at , st+1 ) define a probabilidade de realizar a transicao do estado st para o estado
st+1 quando se executa a acao at .
 R  SxA  R e a funcao de recompensa,
que especifica a tarefa do agente, definindo a
recompensa recebida (ou o custo esperado),
ao longo do tempo.
Resolver um MDP consiste em computar a
poltica  SxA que maximiza (ou minimiza)
alguma funcao, geralmente a recompensa recebida (ou o custo esperado), ao longo do tempo
(Bianchi, 2004).
Na secao 2.2 sera mostrada a tecnica de
Aprendizagem por Reforco, na qual e fundamentada nos Processos de Decisao de Markov.
2.2

Aprendizado Por Reforco

O Aprendizagem por reforco (AR) e um formalismo da IA que permite a um agente aprender a
partir da sua interacao com o ambiente no qual ele
esta inserido (Watkins, 1992 Neri et al., 2011).
A ideia central da tecnica de aprendizado por
reforco e que as percepcoes sao utilizadas nao apenas para agir, mas ao mesmo tempo, para melhorar a habilidade do agente para agir no futuro. A
aprendizagem ocorre a medida que o agente observa suas interacoes com o ambiente e com seus
proprios processos de tomada_de_decisao. Normalmente, o tipo de realimentacao disponvel para o
aprendizado e um fator importante na determinacao da natureza do problema (Russell, 2004).
O aprendizado_por_reforco pode ser entendido
como uma forma dos agentes aprenderem o que

3558

Anais do XIX Congresso Brasileiro de Automática, CBA 2012.

fazer, particularmente quando nao existe nenhum
professor dizendo ao agente que acao ele deve executar em cada circunstancia. Para isso, o agente
precisa saber que algo de bom aconteceu quando
ganhar, e que algo de ruim aconteceu quando perder. Essa especie de realimentacao e chamada de
recompensa ou reforco (Russell, 2004).
2.2.1

Algoritmo Q-learning

O metodo de aprendizagem por reforco Q-learning
(Watkins, 1992 Neri et al., 2011) e um algoritmo
que permite estabelecer autonomamente uma poltica de acoes de maneira interativa.
A ideia basica do Q-learning e que o algoritmo
de aprendizagem aprende um funcao de avaliacao
otima sobre todo o espaco de pares estado-acao
SxA. Desde que o particionamento do espaco de
estados do robo e do espaco de acoes nao omita
e nao introduzam novas informacoes relevantes.
Quando a funcao otima Q for aprendida, o agente
sabera qual acao resultara na maior recompensa
em uma situacao particular s futura (Monteiro,
2004).
A funcao Q(s, a) de recompensa futura esperada ao se escolher a acao a no estado s, e aprendida atraves de tentativa e erro segundo a Equacao
(1)
Qt+1  Qt (st , at )+rt +Vt (st+1 )Qt+1 (st , at )
(1)
onde  e a taxa de aprendizagem, rt e a recompensa, resultante de tomar a acao a no estado
s,  e fator de desconto e o termo Vt (st+1 ) 
maxa Q(st+1 , at ) e a utilidade do estado s resultante da acao a, obtida utilizando a funcao Q que
foi aprendida ate o presente (Monteiro, 2004).
A poltica de acoes adotada foi a -gulosa(Greedy), onde o agente tem grande probabilidade
de escolher a acao a com o maior reforco Q(s, a)
no estado s.
A forma procedimental do algoritmo Qlearning e (Watkins, 1992 Monteiro, 2004)
Para cada s,a inicialize Q(s,a)0
Observe s
Repita
 Selecione acao a usando a poltica de acoes
-gulosa
 Execute a acao a
 Receba a recompensa imediata r(s,a)
 Observe o novo estado s
 Atualize o item Q(s,a) de acordo com a equacao (1)
 s  s
Ate que criterio_de_parada seja satisfatorio

ISBN 978-85-8001-069-5

2.3

Futebol de Robos Simulado

A categoria de simulacao 2D da Robocup simula partidas de futebol_de_robos_autonomos.
O simulador apresenta caractersticas de um ambiente dinamico, ruidoso, cooperativo e coordenado (Fraccaroli, 2010a). Nesta liga nao existem
robosagentes fsicos, todo o ambiente e os agentes sao virtuais. O simulador fornece aos agentes
todos os dados que sao obtidos na realidade por
meio de sensores e calcula o resultado das acoes
de cada agente. A simulacao 2D permite definir as
estrategias de jogo para um time de robos formado
por doze agentes robos.
Cada partida realizada tem duracao de aproximadamente dez minutos, valor este correspondente a seis mil ciclos de simulacao, sendo separado em dois tempos de aproximadamente cinco
minutos (Fraccaroli, 2010b).
2.3.1

Time Base Agent2D (Helios Base)

O time Helios (Akiyama et al., 2011) do Japao,
foi o campeao da categoria de simulacao 2D da
Robocup em 2010 e 2012. Os desenvolvedores resolveram disponibilizar na Internet o codigo base
do time. Essa iniciativa veio para facilitar o surgimento de novas equipes na categoria de simulacao
2D da Robocup.
Foi escolhido utilizar o codigo base do Helios
(Agent2D) pois ele disponibiliza algumas habilidades basicas ja implementas, como interceptacao de
bola, chute, drible e movimentacao. Alem disso,
ja oferece implementado a conexao com o RoboCup Soccer Server, servidor de execucao dos jogos
2D da Robocup, criada atraves de um socket, que
possibilita enviar e receber mensagens.
2.3.2

Time UaiSoccer2D (UFSJ)

Na UFSJ o time de futebol_de_robos simulado e o
UaiSoccer2D (Ottoni et al., 2012). O Time UaiSoccer2D, que foi analisado neste trabalho, foi resultado da implementacao do aprendizado_por_reforco e modificacoes na estrutura do Agent2D.
A Equipe UaiSoccer2D utiliza testes estatsticos para verificar o comportamento do sistema_multiagente cooperativo (time de robos) de
acordo com a estrategia de jogo utilizada (Ottoni
et al., 2011). Nas proximas secoes, e feita uma
breve introducao sobre a analise_estatstica empregada neste trabalho.
2.4

Hipoteses Estatsticas

Uma hipotese estatstica e uma afirmativa sobre
a distribuicao de probabilidade de uma variavel
aleatoria. Os procedimentos de teste de hipotese
se apoiam no uso de informacao em uma amostra
aleatoria da populacao de interesse. Para testar
uma hipotese, e necessario extrair uma amostra

3559

Anais do XIX Congresso Brasileiro de Automática, CBA 2012.

aleatoria, calcular uma estatstica de teste apropriada a partir dos dados amostrais e, entao, usar
a informacao contida na estatstica de teste para
tomar a decisao (Hines et al., 2006).

1. Definicao e discretizacao das acoes dos agentes

2.5

3. Definicao dos valores dos reforcos da tabela
R, para cada par Estado (S) X Acao (A)

Teste t Emparelhado

O Teste t Emparelhado (teste t-pareado) de duas
amostras ocorre quando as observacoes sobre as
duas populacoes sao coletadas aos pares. Cada
par de observacoes (X1j , X2j ), e extrado sob condicoes identicas, mas essas condicoes podem mudar de um para outro par (Hines etc all, 2006).
Seja (X11 , X21 ), (X12 , X22 ), ..., (X1n , X2n ) um
conjunto de n observacoes emparelhadas, para
as quais supomos que X1  N (1 , 12 ) e X2 
N (2 , 22 ). Defina as diferencas entre cada par de
observacoes como Dj  X1j , X2j , j  1, 2, ..., n
(Hines et al., 2006). As diferencas Dj sao distribudas normalmente, com media
  E(X1 X2)  E(X1 )E(X2 )  1 2 , (2)
de modo que um teste sobre igualdade de 1 e
2 pode ser obtido realizando-se um teste t de
amostra unica sobre D . Este teste e apropriado
quando as diferencas pareadas seguem uma distribuicao normal. Especificamente, testar H0  1 
2 contra H1  1  2 e equivalente a testar
H0  D  0, H1  D  0,

(3)

com D  1  2 .
A estatstica de teste apropriada para a Equacao (3) e
t0 

D
 ,
SD  n

(4)

D 

1
Dj
n j1

(5)

em que
n

e
n
2
SD



j1

Dj2  nD2

n1

(6)

sao a media e a variancia amostrais das diferencas. Rejeitaramos H0  D  0 (o que
implica que 1  2 ) se t0 > t2,n1 ou se
t0 < t2,n1 . As alternativas unilaterais sao
tratadas de maneira analoga (Hines et al., 2006).
3

Implementacao da Estrategia de
Aprendizagem

A metodologia adotada para a desenvolvimento da
estrategia de aprendizagem e dividida em quatro
etapas, as quais sao

ISBN 978-85-8001-069-5

2. Definicao e discretizacao dos estados do ambiente no qual os agentes estao inseridos

4. Implementacao no Simulador RcSoccerSim
da Robocup de Futebol de Robos.
3.1

Discretizacao das Acoes

Nesta etapa sao definidas as possveis acoes de um
agente no campo de futebol_de_robos simulado. As
acoes abaixo sao apenas para o agente com posse
de bola.
1. Acao Drible Rapido (Carregar a bola em direcao ao gol com velocidade alta)
2. Acao Drible Lento (Carregar a bola em direcao ao gol com velocidade baixa)
3. Acao Drible Normal (Carregar a bola em direcao ao gol com velocidade normal)
4. Acao PasseChute (Passar a bola para um
companheiro ou chutar em gol)
5. Acao Segurar a Bola (Prender a bola junto
ao corpo)
6. Acao Avancar com a Bola.
3.2

Discretizacao dos Estados

A interacao dos agentes com o mundo virtual
e interpretado por meio dos estados do ambiente. Nestes estados sao definidas as caractersticas do ambiente durante uma partida de futebol
de robos. As caractersticas levadas em consideracao sao o posicionamento dos robos da propria
equipe com a posse da bola e a distancia dos adversarios.
Nos Estados de 1 a 3, o agente adversario mais
proximo esta posicionado atras do robo com bola
em relacao ao eixo X, ou seja, o X do oponente e
menor. Ja nos Estados de 4 a 6, o agente adversario mais proximo esta posicionado a frente do
robo com bola em relacao ao eixo X, ou seja, o X
do oponente e maior.
1. Estado Adversario Longe Atras (A distancia
entre os dois agentes e maior que 4.5 m)
2. Estado Adversario Perto Atras (A distancia
entre os dois agentes e menor que 4.5 m e
maior que 2.5 m)
3. Estado Adversario Muito Perto Atras (A
distancia entre os dois agentes e menor que
2.5 m)

3560

Anais do XIX Congresso Brasileiro de Automática, CBA 2012.

4. Estado Adversario Longe na Frente (A distancia entre os dois agentes e maior que 4.5
m)
5. Estado Adversario Perto na Frente (A distancia entre os dois agentes e menor que 4.5
m e maior que 2.5 m)
6. Estado Adversario Muito Perto na Frente (A
distancia entre os dois agentes e menor que
2.5 m).
As figuras 1 e 2 apresentam o sistema de eixos
X,Y na plataforma de futebol_de_robos simulado
da Robocup.

forma, o objetivo de marcar um golpode ser desmembrado em obter posse de bola, driblar em
direcao a metae chutar em direcao ao gol.
A partir disso, neste trabalho, ao definir as
recompensas imediatas, o objetivo foi valorizar
cada passo necessario para que o time de robos
marcasse um gol. Ou seja, o objetivo foi que o
time aprendesse uma estrategia de jogo visando
um comportamento ofensivo com posse de bola.
Essa abordagem e distinta de usar reforcos somente quando ha gols (recompensas) ou perda de
bola (penalizacoes).
A Matriz de Recompensa possui cinco valores
distintos, sendo que
 -1 - significa que o par sxa nao e indicado
para ser executado
 0 - significa que o par sxa possui grau de relevancia muito pequeno
 5 - significa que o par sxa possui grau de relevancia pequeno
 10 - significa que o par sxa possui grau de
relevancia medio
 20 - significa que o par sxa possui grau de
relevancia alto.

Figura 1 Sistemas de coordenadas X,Y do campo
de futebol simulado 2D para o time que esta atacando para a direita.

Figura 2 Sistemas de coordenadas X,Y do campo
de futebol simulado 2D para o time que esta atacando para a esquerda.

Os estados (linhas) que apresentam os valores
10 ou 20 foram definidos como os mais relevantes.
Ou seja, de acordo com o modelo, e melhor para
o robo com posse de bola se estiver em um estado
que possui algum reforco imediato medio ou alto.
A matriz de recompensas Estado (S) x Acao
(A) e apresentada na tabela 1 de reforco (R), em
que as linhas representam os estados do ambiente
e as colunas as acoes que os agentes podem tomar.
Tabela 1 Matriz de
EstadoAcao A1 A2
E1
-1
-1
E2
0
-1
E3
5
-1
E4
-1
-1
E5
-1
5
E6
-1
-1

3.4
3.3

Definicao da Matriz de Recompensas

O ambiente do futebol_de_robos simulado envolve
uma grande complexidade, em termos de numero
de acoes, para que o time de robos alcance a
recompensa principal ao marcar um gol. Um
metodo comum, usado originalmente no treinamento de animais, e chamado de modelagem de
recompensa, e envolve oferecer recompensas adicionais por fazer progressos(Russell, 2004). Dessa

ISBN 978-85-8001-069-5

Recompensa
A3 A4 A5
-1
20
-1
0
-1
-1
-1
-1
-1
-1
20
-1
0
0
-1
-1
10 10

A6
-1
0
-1
-1
0
-1

Implementacao no Simulador

A etapa de implementacao da estrategia de aprendizagem por reforco foi realizada no simulador RcSoccerSim de futebol_de_robos em duas dimensoes
da Robocup. O objeto de estudo foi o Time UaiSoccer2D.
Os parametros do algoritmo Q-learning, a
taxa de aprendizagem () e o fator de desconto
() foram fixados em 0,95.
Para armazenar as informacoes aprendidas
pelos robos foi criado um arquivo denominado

3561

Anais do XIX Congresso Brasileiro de Automática, CBA 2012.

q.txt. Nesse arquivo, a matriz Q (6x6) de aprendizado foi iniciada com zero para cada par estado X
acao, indicando a inexistencia de inteligencia no
time antes da primeira simulacao.
O modelo apresentado visou apenas o aprendizado quando o agente estivesse com a posse de
bola. Dessa forma, somente um robo por vez
acessa o arquivo q.txt, mas o conhecimento de cada
um dos agentes fica acumulado na Matriz Q. Resultando em uma comunicacao entre os jogadores
denominada de Quadro-Negro. O Quadro-Negro e
uma estrutura comum a todos os agentes, no caso
o q.txt, onde podem realizar a escrita e a leitura
das informacoes aprendidas por cada robo. Essa
estrutura de comunicacao visa acelerar o aprendizado dos robos, visto que, as experiencias dos
agentes de acumulam em uma unica estrutura.
O processo de aprendizagem (treinamento)
aconteceu adotando o Time Helios2011 (Akiyama
et al., 2011) como adversario.
4

Analise dos Resultados

Procurou-se com este trabalho, verificar o comportamento do time de robos (UaiSoccer2D), apos
duas fases de treinamento do algoritmo de aprendizado_por_reforco. Avaliando a eficiencia da Matriz Q acumulada apos 30 e 90 simulacoes.
 Fase 1 Apos 30 jogos de treinamento foi acumulada a Matriz Q1 
 Fase 2 Apos 90 jogos de treinamento foi acumulada a Matriz Q2 .
Questionando entao, se houve evolucao de desempenho do time de robos na Fase 2 em relacao
a Fase 1.
Como foi analisado o comportamento apos
duas fases da mesma estrategia de aprendizado,
verificou-se uma relacao de dependencia entre as
amostras. O teste mais adequado a relacao de dependencia de duas amostras e o teste t-pareado
(Bussab, 2012). O teste t-pareado unilateral foi
aplicado utilizando as seguintes hipoteses
 H0  a media de gols sofridos na validacao da
Fase 2 e igual a media de gols sofridos na
validacao da Fase 1
 Ha  a media de gols sofridos na validacao da
Fase 2 e menor a media de gols sofridos na
validacao da Fase 1
Dessa forma, apos 30 e 90 jogos de treinanento, a Matriz Q foi congelada para efetuar os
jogos de validacao. Nessas simulacoes de validacao, a Matriz Q nao sofreu atualizacoes. Para
a validacao das Fases de aprendizado foram selecionados aleatoriamente 6 times adversarios da
simulacao 2D, disponibilizados na Internet. Entre esses times se encontram as equipes participantes do mundial 2011 da Robocup Helios2011

ISBN 978-85-8001-069-5

(2o colocadoJapao), Hfutengine2011 (9o colocadoChina), Aua2d (11o colocadoChina) e Rione (13o colocadoJapao) o time base Agent2D
(Helios Base) e o time UaiSoccer2D2011.
Estes times, testados em 5 jogos contra cada
Matriz Q (Q1 e Q2 ) geradas pelas duas fases do
aprendizado do UaiSoccer2D, totalizaram 30 jogos de validacao por fase, 60 jogos no total. O
somatorio dos gols sofridos total contra cada time
foi utilizado como um banco_de_dados para a aplicacao do teste (Tabela 2).
Foram realizadas todas as etapas de testes no
software Minitab 14 (versao academica). Inicialmente foi realizado um teste de normalidade para
a diferenca, entre validacao da Fase 1 e Fase 2, dos
Gols Sofridos, Saldo de Gols e Gols Feitos, a fim de
verificar a aplicabilidade do teste t-pareado. Nos
tres casos a suposicao de normalidade foi satisfeita
a partir do teste de normalidade de KolmogorovSmirnov (Kolmogorov, 1933 Smirnov, 1948).
Tabela 2 Gols Sofridos pelo UaiSoccer2D na Validacao de cada Fase de Aprendizado
Time Adversario
Q1 Q2
Agent2D
17 11
Aua2d
0
1
Helios2011
25 14
Hfutengine2011
7
3
Rio-ne
8
4
UaiSoccer2D2011
0
0
Media
9,5 5,5
p-valor 0,03668822
Se o p-valor (P-Value) for menor ou igual a
0,05, isto indica que devemos rejeitar H0 e aceitar
Ha , ou seja, a Matriz Q2 obteve melhor desempenho que a Matriz Q1 , mas se o p-valor (P-Value)
for maior que 0,05, devemos aceitar H0 , ou seja,
o algoritmo de aprendizado nao evoluiu da Fase
1 para a Fase 2. Como o p-valor (P-Value) resultou em 0,03668822, conclui-se que o comportamento do Time de Robos apos 90 jogos de treinamento (Fase 2) foi superior estatisticamente em
relacao ao desempenho apos apenas 30 jogos de
treino (Fase 1), com respeito ao numero de gols
sofridos em um jogo.
Ja as tabelas 3 e 4, com os saldo de gols e
os gols feitos revelam que a Matriz Q2 (Fase 2)
nao obteve desempenho superior significativo em
relacao a esses criterios.
5

Conclusoes

Este trabalho teve como meta modelar uma estrategia de aprendizado_por_reforco (AR) no domnio do futebol_de_robos em duas dimensoes (2D).
Para isso, foi adotado o algoritmo Q-learning. A
modelagem de uma inteligencia_artificial por meio
da tecnica de aprendizagem por reforco para um

3562

Anais do XIX Congresso Brasileiro de Automática, CBA 2012.

Tabela 3 Saldo de Gols por Fase de Apendizado
Time Adversario
Q1
Q2
Agent2D
-1
-6
Aua2d
23
22
Helios2011
-24
-13
Hfutengine2011
26
39
Rio-ne
23
30
UaiSoccer2D2011
120
121
Media
27,833 32,167
p-valor 0,098053308

Tabela 4 Gols Feitos por Fase de Apendizado
Time Adversario
Q1
Q2
Agent2D
16
5
Aua2d
23
23
Helios2011
1
1
Hfutengine2011
33
42
Rio-ne
31
34
UaiSoccer2D2011
120
121
Media
37,333 37,667
p-valor0,452474988

time de robos auxilia na tomada decisao do sistema_multiagente no ambiente de simulacao. A
implementacao da tecnica de aprendizagem, faz
com que a equipe de futebol_de_robos se adapte
as condicoes da partida em resposta as acoes do
adversario.
As analises estatsticas indicaram uma evolucao no comportamento do time em relacao a diminuicao no numero de gols sofridos, apos 90 jogos
de treinamento (Fase 2). Ou seja, estatisticamente
o time apos a Fase 2 sofreu menos gols que o time
apos a Fase 1.
Dessa forma, a estrategia de AR implementada apresentou melhores resultados apos mais jogos de treinamento.
No entanto, os testes tambem revelaram que
nao ocorreu evolucao estatisticamente significante
com relacao aos gols feitos e saldo de gols.
Nos proximos trabalhos, serao propostas analises sobre uma quantidade de jogos de treinamento superiores as apresentadas neste artigo.
Dessa forma, e pretendido verificar se ao longo
do tempo o numero de gols sofridos continuara a
cair. Alem disso, visualizar se em algum momento
o numero de gols feitos e saldo de gols aumentarao
significativamente entre Fases de Treinamento do
Aprendizado por Reforco.

Agradecimentos
Agradecemos a CAPES, CNPQ, FAPEMIG, PPGEL, UaiSoccer2D, UAIrobots e UFSJ pelo apoio.

ISBN 978-85-8001-069-5

Referencias
Akiyama, H., Shimora, H., Nakashima, T., Narimoto, Y. and Okayama, T. (2011). Helios2011 team description, Robocup 2011 .
Bertsekas, D. (2001). Dynamic programming and
optimal control., Belmont, MA Athena Scientific. (ISBN 1-886259-08-6.).
Bianchi, R. A. C. (2004). Uso de heurstica para a
aceleracao do aprendizado_por_reforco., Masters thesis, Tese (Doutorado) Escola Politecnica da Universidade de Sao Paulo.
Bussab, W. B. e Morettin, P. (2012). Estatstica
Basica, 7st edn, Saraiva.
Celiberto Jr, L. A. e Bianchi, R. A. C. (2006).
Aprendizado por reforco acelerado por heurstica para um sistema_multi-agentes, In
3rd Workshop on MSc dissertations and PhD
thesis in Artificial Intelligence. .
Fraccaroli, E. S. (2010a). Analise de Desempenho de Algoritmos Evolutivos no Domnio do
Futebol de Robos, Dissertacao apresentada a
Escola de Engenharia de Sao Carlos da Universidade de Sao Paulo.
Fraccaroli, E. S. e Carlson, P. M. (2010b). Time
gearsim 2010 da categoria robocup simulation 2d., In Competicao Latino Americana de
Robotica. 2010.
Garcia, A. C. B. e Sichman, J. S. (2003). Agentes
e Sistemas Multiagentes., Vol. 11, In Sistemas Inteligentes Fundamentos e Aplicacoes,
Rezende, S. O., chapter 11, pp. 269306.
Hines, W. W., Montgomery, D. C., Goldsman,
D. M. and Borror, C. M. (2006). Probabilidade e Estatstica na Engenharia, LTC.
Kerbage, S. E. H., Antunes, E. O., Almeida, D. F.
and Rosa, P. F. F. (2010). Generalizacao
da aprendizagem por reforco Uma estrategia para robos_autonomos cooperativos., In
Competicao Latino Americana de Robotica.
2010.
Kim, Y. C., C. S.-B. and Oh, S. R. (2010). Mapbuilding of a real mobile robot with ga-fuzzy
controller., International Journal of Fuzzy
Systems. pp. 696703.
Kolmogorov, A. . (1933). Sulla determinazione
empirica di una legge di distribuzione, G.
Inst. Ital. Attuari 4 83.
Lacerda, M. J., Texeira, W. W. M. and Nepomuceno, E. G. (2009a). Alocacao de agentes
para controle de epidemias utilizando algoritmo_genetico., In IX Simposio Brasileiro de
Automacao Inteligente .

3563

Anais do XIX Congresso Brasileiro de Automática, CBA 2012.

Lacerda, M. J., Texeira, W. W. M. and Nepomuceno, E. G. (2009b). Controle da propagacao
espacial de epidemias Uma abordagem utilizando agente inteligente., In IX Simposio
Brasileiro de Automacao Inteligente .
Monteiro, S. T. e Ribeiro, C. H. C. (2004). Desempenho de algoritmos de aprendizagem por
reforco sob condicoes de ambiguidade sensorial em robotica_movel, Revista Controle 
Automacao Vol.15 no.3.
Nascimeno Jr., C. L. e Yoneyama, T. (2009). Inteligencia Artificial em Controle e Automacao.,
1st edn, Edgard Blucher.

Watkins, C. J. (1989). Models of delayed reinforcement learning., Masters thesis, PhD thesis,
Psychology Department, Cambridge University, Cambridge, United Kingdom.
Watkins, C. J. e Dayan, P. (1992). Technical note
q-learning, Machine Learning .
White, D. J. (1993).
A survey of applications of markov decision processes., The
Journal of the Operational Research Society
44(11) 10731096.

Neri, J. R. F., Santos, C. H. F. and Fabro, J. A.
(2011). Descricao do time gpr-2d 2011, Competicao Brasileira de Robotica 2011.
Ottoni, A. L. C., Lamperti, R. D. and Nepomuceno, E. G. (2012). Uaisoccer2d Team description paper robocup 2012, Robocup 2012
2012.
Ottoni, A. L. C., Nepomuceno, E. G., Oliveira,
F. F. and Oliveira, M. S. (2011). Analise do
comportamento de sistemas_multiagentes cooperativos por meio de testes estatsticos, In
X Encontro Mineiro de Estatstica .
Puterman, M. L. (1994). Markov decision processes Discrete stochastic dynamic programming., New York, NY Wiley-Interscience
(ISBN 0471619779.).
Russell, S. J. e Norving, P. (2004). Inteligencia
Artificial., 2st edn, Campus.
Silva, A. T. R., Silva, H. G., Santos, E. G., Ferreira, G. B., Santos, T. D. and Silva, V. S.
(2010). ibots 2010 Descricao do time., In
Competicao Latino Americana de Robotica.
2010.
Simoes, M. A. C., Silva, H., Meyer, J., Oliveira,
J. D., Cruz, L., Pessoa, H. and L., A. R.
(2007). Bahia 2d Descricao do time., In
XIII Simposio Brasileiro de Automacao Inteligente .
Smirnov, N. (1948). Tables for estimating the goodness of fit of empirical distributions, Annals of Mathematical Statistics 19 279.
Sutton, R., e. B. A. (1998). Reinforcement Learning an introduction., 1st edn, Cambridge,
MA MIT Press.
Texeira, W. W. M., Teles, F., Barbosa, A. M.,
Silva, M. A., Nepomuceno, E. G. and Queiroz Melo, M. F. A. (2009). Modelagem e controle de um sistema_multiagente cooperativo
visao global e local aplicadas ao problema da
alocacao de recursos., In IX Simposio Brasileiro de Automacao Inteligente. .

ISBN 978-85-8001-069-5

3564