MÉTODOS DE ACELERAÇÃO DE APRENDIZADO APLICADO AO MODELO
NEURO-FUZZY HIERÁRQUICO POLITREE COM APRENDIZADO POR REFORÇO
FÁBIO MARTINS, KARLA FIGUEIREDO, MARLEY VELLASCO
Laboratório de Inteligência Computacional Aplicada
Departamento de Engenharia Elétrica
Pontifícia Universidade Católica do Rio de Janeiro
Rua Marquês de São Vicente, 225, Rio de Janeiro - 22453-900 RJ Brasil
fabio.jessen@gmail.com, karla@ele.puc-rio.br, marley@ele.puc-rio.br
Abstract This paper presents two methods for accelerating the learning process of Reinforcement Learning - Neuro-Fuzzy
Hierarchical Politree model (RL-NFHP) policy Q-DC-Roulette and early stopping. This model is used to provide an agent with
intelligence, making it autonomous, due to the capacity of ratiocinate (infer actions) and learning, acquired knowledge through
interaction with the environment. The characteristics of the RL-NFHP allow the agent to automatically learn its structure and action for each state. The RL-NFHP model was evaluated in an application benchmark known in the area of autonomous agents
car mountain problem. The results demonstrate the acceleration of learning process and the potential of this model, which works
without any prior information, such as number of rules, rules of specification, or number of partitions that the input space should
possess.
Keywords Neuro-Fuzzy Systems Learning Reinforcement Learning, Automatic Learning
Resumo Este trabalho apresenta dois métodos de aceleração de aprendizado no modelo Reinforcement Learning - NeuroFuzzy Hierárquico Politree (RL-NFHP) política Q-DC-Roulette e early stopping. Este modelo é utilizado para dotar um agente
com inteligência, tornando-o autônomo, devido s capacidades de raciocinar (inferir ações) e aprender, adquirido conhecimento
pela interação com o ambiente. As características do RL-HNFP permitem ao agente aprender automaticamente a sua estrutura e a
ação para cada estado. O modelo RL-HNFP foi avaliado em uma aplicação benchmark conhecida na área de agentes autônomos
carro na montanha. Os resultados obtidos demonstram a aceleração do aprendizado e o potencial deste modelo, que funciona sem
qualquer informação prévia, tais como número de regras, regras de especificação, ou número de partições que o espaço de entrada deve possuir.
Palavras-chave 

1

regras e de suas limitações quanto ao número de
variáveis de entrada no modelo.
O modelo RL-NFHP (Reinforcement Learning 
Neuro-Fuzzy Hierárquico Politree) desenvolvido por
Figueiredo (2004) teve por objetivo contornar estes
problemas e garantir a sua utilização em ambientes
grandes eou contínuos. Tais ambientes apresentam
uma característica denominada curse of dimensionality que inviabiliza a aplicação direta de métodos
tradicionais de RL. Assim sendo, a decisão de se usar
uma metodologia de particionamento recursivo, já
explorada com excelentes resultados em Souza
(2000), resultando em regras hierárquicas, reduzindo
significativamente as limitações deste modelo.
Para o RL-NFHP não é necessário um conhecimento prévio detalhado do ambiente nem uma grande quantidade de informações. Durante o processo de
aprendizado e conforme a necessidade, o modelo
refina determinados espaços de estados a fim de
obter uma melhor resposta de acordo com o objetivo
almejado.
No entanto, deve-se destacar que uma das características dos modelos baseados em Reinforcement
Learning é o tempo despendido de aprendizado.
Assim, o objetivo deste trabalho foi acelerar e
melhorar o processo de aprendizado do modelo RLNFHP, inserindo neste o método de early stopping e
uma nova política Q-DC-roulette. O modelo RLNFHP modificado foi testado na aplicação bench-

Introdução

Com o avanço tecnológico, problemas de controle cada vez mais complexos surgem. Neste contexto,
o controle tradicional, onde é necessário o programador antever todas as possíveis situações, torna-se
inviável. Sistemas autônomos inteligentes, que incorporam a capacidade de autodecidir que ação tomar para satisfazer seus objetivos, tornam-se uma
solução para estes problemas.
Sistemas como estes têm sido concebidos através do uso de técnicas como Lógica Fuzzy (LF)
(Mendel, 1995) e Redes Neurais (RN) (Haykin,
1998) para áreas onde a abordagem convencional não
tem conseguido fornecer soluções satisfatórias. Diversos pesquisadores buscam integrar estas duas
técnicas gerando modelos híbridos que associam as
vantagens de cada abordagem e minimizam suas
limitações.
As principais limitações de alguns dos modelos
Neuro-Fuzzy, tais como NEFCON (Neuro-Fuzzy
Controller), ELF (Evolucionary Logic Fuzzy), FACL
(Fuzzy-Actor-Critic-Learning) e FQL (Fuzzy-QLearning) estão na necessidade de pré-definição do
número de conjuntos_fuzzy, das funções de pertinência destes conjuntos (usados nos antecedentes e consequentes das regras fuzzy), da pré-definição do número de regras, dos antecedentes que compõem as

397

mark carro na montanha, conhecida na área de agentes autônomos.
O restante deste trabalho é organizado da seguinte forma a seção 2 descreve brevemente o modelo
RL-NFHP original na terceira seção são apresentas
as melhorias visando a aceleração do processo de
aprendizado a seção 4 mostra os resultados obtidos
com o benchmark carro da montanha, fazendo-se
comparações com outras técnicas e, finalmente, na
seção 5 as conclusões são apresentadas.

(a1, a2, ... at), onde cada ação está associada a uma
função de valor-Q (Sutton, 1998). Através do aprendizado_por_reforço, uma ação de cada polipartição
será definida como aquela que representa o comportamento desejado do sistema quando o mesmo se
encontra em um determinado estado. Um estado é
definido por um conjunto de células ativas ao mesmo
tempo.
a1
a2
...
at

x2

Q1
Q2
...
Qt

2 Modelo RL-NFHP
O modelo RL-NFHP (Figueiredo, 2004) tem três
componentes fundamentais o aprendizado_por_reforço, a característica fuzzy e o particionamento hierárquico.
No modelo RL-NFHP, o aprendizado_por_reforço baseia-se no método SARSA (Sutton, 1998), sendo o processo de identificação dos estados realizado
ao longo do aprendizado, não sendo conhecidos
previamente. As regras são geradas por um processo
automático de particionamento do espaço de entrada.
O componente RL faz com que o modelo aprenda a
encontrar a ação mais adequada a ser executada para
um determinado estado. O componente fuzzy do
modelo agrega estados que têm comportamentos
similares, associando-os a uma mesma ação. O aspecto hierárquico refere-se ao fato de que cada partição do espaço de entrada pode ter como consequente
um subsistema.
O modelo RL-NFHP é composto de uma ou várias células RL-Neuro-Fuzzy Politree. Estas células
são dispostas numa estrutura hierárquica em forma
de árvore. A célula de maior hierarquia gera a saída.
As de menor hierarquia trabalham como consequentes das células de maior hierarquia.

a1
a2
...
at

Q1
Q2
...
Qt

1  e ( a ( x b ))

e

 ( x)  1   ( x)

1

3
x1

2

2

1
1

a1
a2
...
at

Q1
Q2
...
Qt

Cada célula avalia todas as n variáveis de entrada, formando os antecedentes das regras. Já os consequentes das polipartições da célula podem ser do
tipo singleton (inferência Takagi-Sugeno de ordem
zero) ou ser a saída de um subsistema num nível
anterior na arquitetura do modelo RL-NFHP. A
quantidade de polipartições é dada pela combinações
entre os conjuntos_fuzzy baixo e alto de cada entrada
2n. Apesar de o consequente singleton ser simples,
este não é conhecido a priori.
Os is indicam os níveis de disparo das regras.
Estes níveis de disparo são calculados usando-se uma
operação AND (T-norm) sobre os graus de pertinência de 1,  12 e 2, conforme equação 2. Nesta
equação, o símbolo * representa a operação AND
realizada através da multiplicação.
1 1 (x1) * 2 (x2)
 2 1 (x1) * 2 (x2)
 3 1 (x1) * 2 (x2)
 4 1 (x1) * 2 (x2).

Uma célula RL-Neuro-Fuzzy Politree (RL-NFP)
é um mini-sistema neuro-fuzzy que realiza um particionamento politree em um determinado espaço,
utilizando, para cada variável de entrada, as funções
de pertinência descritas na equação 1. A célula RLNFP gera uma saída exata (crisp) após um processo
de defuzzificação.
1

4

Q1
Q2
...
Qt

Figura 1. Célula RL-NFP com 2 entradas.

2.1 Célula RL-Neuro-Fuzzy Politree

 ( x) 

2

a1
a2
...
at

x1

Célula
Consequentes
a1,a2,...,at
a1,a2,...,at
a1,a2,...,at
a1,a2,...,at

(1)

Estas funções de pertinência possuem dois parâmetros, a e b, que definem os perfis dos conjuntos_fuzzy alto () e baixo ().
A Figura 1 apresenta uma célula RL-NFP de apenas duas entradas enquanto a Figura 2 mostra seu
interior. Nestas figuras, os valores obtidos para as
variáveis de entradas x1 e x2, localizados na polipartição (quadrante) 4, geram os antecedentes das quatro
regras fuzzy após serem computados os graus de
pertinência 1(x1), 1(x1), 2(x2) e 2(x2). Os valores
definidos como consequentes são conjuntos de ações

1

x2

1

2

(2)

Entradas
2

Regras

ai
aj
ap
aq



y

Saída Crisp

Figura 2. Interior de uma célula RL-NFP com 2 entradas.

A interpretação do conhecimento armazenado na
célula RL-NFP da Figura 2 é representada pelo conjunto de regras linguísticas descrito a seguir
regra1
regra2
regra3
regra4

398

Se x1 1 e x2  2 então y  ai
Se x1 1 e x2  2 então y  aj
Se x1 1 e x2 2 então y  ap
Se x1 1 e x2 2 então y  aq

(3)

para atingir seus objetivos. Maiores detalhes ver
Figueiredo (2004).

Cada regra corresponde a uma polipartição da
Figura 1. Quando os valores das entradas incidem
sobre o quadrante 4, é a regra 4 que tem maior nível
de disparo. Cada quadrante, por sua vez, pode ser
subdividido em quatro partes através de outra célula
RL-NFP.
A saída y de cada célula RL-NFP é dada pela
média ponderada em suas polipartições mostrada na
equação 4. Onde i é o grau de ativação da regra i e
ai corresponde ao consequente.

y

 i  ai



(4)

a22 ...

...

am-1

a2m

am1

am2 ...

m

 i

  .a

2i

2i

  .a

mi

Seleção da ação

4

Atualização da função valor Q

5

Particionamento (sn)

3  Seleção da ação
A exploração do espaço_de_estados através da seleção de ações é fundamental para a descoberta das
melhores ações (melhor resposta do agente visando
um objetivo quando este se encontra em um determinado estado do ambiente). Convencionalmente é
utilizada a política -greedy (Sutton, 1998), que
seleciona a ação gulosa associada ao maior valor
esperado de Q com probabilidade (1-) e, com probabilidade , seleciona aleatoriamente uma ação.

amm

A saída do sistema exemplificado na Figura 3 é
dada pela equação 5.
1 .a1
2


 i  i

3

2  Cálculo e retropropagação do retorno
Após a execução da ação, uma nova leitura do
ambiente é realizada permitindo que se avalie a ação
tomada pelo agente. Esta avaliação é feita através de
uma função de retorno global R definida segundo os
objetivos do agente, premiando-o ou punindo-o.
O retorno global pode ser transmitido para todas
as polipartições das células ativas, mediante o cálculo de sua participação na ação resultante.

Figura 3. Exemplo de arquitetura RL-NFHP com três células
(círculos), onde cada célula possui m consequentes (quadrados).

y

Cálculo e retropropagação do retorno

1  Criação do modelo RL-NFHP
Uma célula raiz, ou pai, é gerada tendo como
domínio dos seus conjuntos_fuzzy a faixa de valores
das variáveis de entrada. Estas variáveis de entrada
são lidas do ambiente e normalizadas.

A arquitetura RL-NFHP em árvore é formada
pela interligação das células RL-NFP. No exemplo
de árvore da Figura 3 os nós desenhados através de
círculos cinza representam células RL-NFP e os nós
terminais quadrados são folhas e representam as
polipartições que não sofreram subdivisões. A raiz da
árvore simboliza todo o espaço a ser particionado.

a21

2

O agente baseado em RL deve executar muitos
ciclos para garantir o aprendizado do sistemaambiente. Um ciclo, episódio ou época, é definido pelo um número de passos, ou iterações, que o
agente executa no ambiente, saindo de um ponto
inicial até o ponto objetivo. Cada passo compreende
a execução do algoritmo desde a leitura do ambiente
até a execução da ação pelo agente.

2.2 Arquitetura RL-NFHP

a3

Criação do modelo RL-NFHP

Figura 4. Fluxograma de aprendizado do modelo RL-NFHP.

i

As células RL-NFP formam uma estrutura hierárquica que resulta nas regras que compõem o raciocínio do agente. Neste modelo, as variáveis de
entrada, lidas pelos sensores, são avaliadas nos conjuntos_fuzzy dos antecedentes. Os consequentes são
as ações que o agente deve aprender ao longo do
processo e são realizadas pelo seu atuador. Sendo
assim, o modelo RL-NFHP também cria e determina
sua estrutura mapeando estados em ações.

a1

1

2i



 3 .a3

 i

(5)

mi

4  Atualização da função valor Q
A partir dos valores de retornos calculados para
cada célula ativa da estrutura, os valores-Q associados s ações que contribuíram para a ação resultante
executada pelo agente devem ser atualizados. Esta
atualização ocorre de duas formas distintas segundo a
equação 6 prêmio (Rt+1 > Rt) ou punição (Rt+1  Rt).

mi

2.3 Algoritmo de Aprendizado RL-NFHP
O modelo RL-NFHP realiza a tarefa de identificação da estrutura e ajuste dos parâmetros de forma
integrada. O fluxograma exibido na Figura 4 descreve o algoritmo de aprendizado do modelo RL-NFHP,
onde se destacam seis passos correspondentes  numeração no fluxograma. O processo de aprendizado começa com a definição das entradas relevantes
para o sistemaambiente no qual o agente está inserido e dos conjuntos de ações de que ele pode dispor

Q(st, at)  (1 -  t) . Q(st, at) +
se Rt+1 > Rt
+ t  rt+1 +  Q(st+1, at+1)
Q(at, st)  (1  fp) . Q(at ,st)

399

se Rt+1  Rt

(6)

onde o valor Q(st,at) é atualizado a partir do seu
valor atual Q(st,at) rt+1 é o retorno local imediato
(definido através da retropropagação de Rt+1)  é um
parâmetro que fixa um percentual da contribuição do
valor Q associado  próxima ação at+1 escolhida no
estado st+1 t é o parâmetro proporcional  contribuição relativa desta ação local na ação global e fp é o
fator de punição que varia entre 0,1 definido como
rt+1Rt+1. O valor Q(st+1,at+1) é calculado a partir da
soma_ponderada dos valores-Q com relação aos graus
de pertinência.

3.2 Early stopping
O fim do aprendizado, ou treinamento, ocorre
quando a estrutura RL-NFHP deixa de mudar significativamente, ou seja, o agente aprende. Na prática, é
difícil determinar o fim do treinamento. Pode-se
treinar demasiadamente a estrutura, perdendo-se
generalização e tempo, ou, ao contrário, treiná-la
insuficientemente, impossibilitando o agente de
cumprir seu objetivo ou de cumpri-lo de maneira
insatisfatória.
Na versão do modelo anterior o número de episódios necessários para o aprendizado era avaliado
empiricamente.
Baseado no conceito de early stopping praticado
em Redes Neurais (Haykin, 1998) foi desenvolvido
um procedimento de parada automática do aprendizado, que avalia a convergência do algoritmo. O
treinamento é interrompido quando, durante o teste,
no procedimento de validação, o número médio de
passos em cada episódio aumenta e é menor que o
número de passos máximo estipulado empiricamente.
Semelhante ao processo aplicado a RN, onde o treinamento é interrompido quando o erro de validação
aumenta e é menor que um limiar definido. Esta
validação é executada a cada N episódios.

5  Particionamento
Para a realização do particionamento cada polipartição deve atender a dois critérios. O primeiro
critério evita que a estrutura cresça devido ao mau
desempenho ocasionado pela escolha ainda imatura
das ações, e o segundo critério incentiva o particionamento quando existem significantes variações das
funções de valor Q das ações.
Quando estes dois critérios são atendidos, uma
célula filha é criada e conectada quela polipartição.
Seu domínio será o subdomínio correspondente 
polipartição.
3 Melhorias do Modelo RL-NFHP

3 Resultados

Com o intuito de melhorar e acelerar o aprendizado foram desenvolvidos dois métodos a política de
seleção de ação Q-DC-roulette e o término automático do treinamento early stopping.

O carro na montanha é um benchmark altamente
relevante, uma vez que vem sendo usado por diferentes pesquisadores (Jouffe, 1998 Sutton, 1998 Figueiredo, 2004 Figueiredo, 2005) com o objetivo de
testar seus modelos de aproximação de funções.
O problema pode ser descrito da seguinte forma
um carro tem como objetivo subir ao topo de uma
montanha, entretanto, o carro possui potência menor
do que a necessária para vencer a força da gravidade.
A única forma possível de alcançar o objetivo é ganhar inércia suficiente movendo-se no sentido contrário ao alvo para poder adicionar a aceleração da
gravidade  sua própria aceleração.
O problema possui duas variáveis de estados
contínuas, a posição do carro xt  -1,20,5 e sua
velocidade vt  -0,070,07, e três ações discretas
F  -1,0,1, impulsão para a esquerda, nenhuma
impulsão e impulsão para a direita. A montanha
suave é formada por dois picos, um possui uma parede inelástica (x  -1,2) e o outro o objetivo (x  0,5),
e um vale (x  -0,5). A dinâmica do sistema é descrita pela equação 8.

3.1 Política de Seleção Q-DC-roulette
Política Q-DC-roulette é apresentada nesse trabalho como uma inovação que visa mesclar os benefícios do aprendizado através das funções de valor Q
com a possibilidade de exploração de acordo com o
quanto cada ação foi visitada. Trata-se de uma mistura entre a Dual Counter Roulette (Flesch, 2009) e a
Q-roulette (Sutton, 1998). A seleção de ação com
probabilidade p   é baseada numa distribuição de
probabilidade conforme a equação 7 e a ação é
selecionada de forma gulosa no restante das vezes
(p  1-).


1   .V ( s, a i ) Q( s, a i )
  V ( s, a ) 
k 

P(a i  s) 



.
V
(
s
,
a
)
  1  V (s, ak ) Q(s, a k ) 
k 
 


(7)

onde P(ais) é a probabilidade de se escolher a ação
ai, dado que o agente está no estado s, V(s,ai) é o
número de vezes que determinada ação foi escolhida
(visitada) em ciclos anteriores,  é uma constante de
ponderação, e Q(s,ai) é a função de valor da ação ai.
A ideia é realizar um exploitation mais inteligente dando menos importância na seleção de ações que
tenham função valor alta, mas que foram pouco visitadas, ou a ações muito escolhidas, mas com baixo
Q(s,a).

vt 1  vt  0,001F t 0,0025 cos(3xt )

(8)

xt 1  xt  vt 1

Os valores de retorno foram calculados considerando a distância d entre o carro e o objetivo e a
velocidade v do carro pela equação 9, os parâmetros
k1 e k2 são constantes de ajuste.
R  k1e-d  k 2ev

R  k1e

400

(d -1)

 k 2e

se xt < xt+1
v

se xt  xt+1

(9)

precisa aprender a identificar os estados. Porém as
modificações feitas nos pesos da rede pelo algoritmo
backpropagation afetam toda a rede, prejudicando o
aprendizado. Nos modelos CMAC e FQL, os estados
são fixados a priori (grids e regras) e as modificações
das funções de valor afetam somente as regras ou os
grids que estão ativos em cada passo do aprendizado.
A percepção sobre o conjunto de estados ativos
pelo modelo CMAC é discreta (muitos estados vizinhos ativam os mesmos conjuntos de grids) já no
caso dos modelos FQL e RL-NFHP a percepção
sobre o conjunto de estados é contínua. A generalização introduzida no espaço de entrada pelas regras
fuzzy é mais suave do que a introduzida pelos grids
do modelo CMAC. Por esse motivo os resultados dos
modelos FQL e RL-NFHP são superiores. Interessante notar que o resultado do FQL mostrou-se aproximadamente igual ao obtido pelo modelo RL-NFHP2
na fase de testes.

3.1 Comparação entre Políticas

Histograma do Número de Épocas
Necessárias para o Aprendizado

30
25

Q-DC-roulette

20

-greedy

15
10
5

Tabela 1. Comparação entre modelos para o benchmark
Carro na Montanha.

2000

1900

1800

1700

1600

1500

1400

1300

1200

1100

900

1000

800

700

600

500

400

300

200

0
100

Frequência de Ocorrência ()

Inicialmente foram realizados testes no modelo
RL-NFHP com variando apenas as políticas de escolha das ações Q-DC-roulette e -greedy. Os parâmetros foram definidos conforme Figueiredo (2004).
As condições iniciais foram alternadas entre
(x0v0)  (1,20) e (0,50), no extremo oposto ao
objetivo e no vale com velocidade nula, respectivamente. O early stopping foi realizado a cada N  100
episódios, com número máximo de passos de 200,
determinado heuristicamente.

Épocas

Figura 5. Gráfico do histograma do número de épocas
de treinamento do modelo RL-NFHP para as
políticas Q-DC-roulette e -greedy.

Constata-se pelo gráfico da figura 5 que o aprendizado, usando tanto a política Q-DC-roulette
quanto -greedy, ocorre na maioria das vezes no
início do treinamento, em 100 épocas. Em geral,
Q-DC-roulette possui um aprendizado mais rápido,
menor dispersão do número de épocas necessárias
para treinamento.

Modelo

Características

Fase de
Aprendizado

Fase de Teste

Neural-Q-learning

4

1724

2189

CMAC Q-learning

343

262

85

FQL

25

112

61

RL-NFHP1

0

91

69

RL-NFHP2

0

136

63

A tabela 2 compara o modelo original RLNFHP1 (Figueiredo, 2004) com sua versão modificada RL-NFHP2. A primeira coluna representa o modelo, a segunda, o número de épocas necessárias de
aprendizado, a terceira, o número de células do modelo após treinamento (tamanho da estrutura) e a
quarta e quinta, a média de passos por episodio durante o treinamento e durante o teste.

3.2 Comparação entre Modelos
Os testes conduzidos nesta seção visaram comparar o modelo RL-NFHP2, apresentado neste trabalho, com diferentes modelos baseados em RL desenvolvidos por outros pesquisadores Neural-QLearning, CMAC Q-Learning, FQL e RL-NFHP1.
Estes testes foram extraídos de Figueiredo (2004) e o
modelo RL-NFHP2 foi o melhor obtido na seção 3.1
para a política Q-DC-roulette.
A tabela 1 compara os resultados obtidos. A
primeira coluna indica o modelo, a segunda a característica, ou seja, o número de neurônios na camada
escondida para o caso do Neural Q-Learning, o número de grids para o caso CMAC e o número de
regras para o FQL, enquanto a terceira e a quarta
colunas indicam o desempenho obtido para cada
modelo em termos de números de passos para a fase
de aprendizado e teste.
Embora o modelo RL-NFHP não necessite de
informações prévias relativas ao processo de aprendizado, como o Neural-Q-Learning, o CMAC e o
FQL, sua função de retorno, relacionada ao objetivo
e não ao aprendizado, deve ser elaborada.
Segundo Jouffe (1998), a explicação para a Rede
Neural ter o pior resultado deve-se ao fato de que,
além do aprendizado dos valores Q, ela também

Tabela 2. Comparação entre modelos RL-NFHP
para o benchmark Carro na Montanha.
Média de Passos
Fase de
Fase de
Aprendizado
Teste

Modelo

Número
de
Épocas

Número de
Células

RL-NFHP1

5000

96

91

69

RL-NFHP2

540

23

136

63

Observa-se, pela tabela 2, que o treinamento
(número de épocas) do agente através do modelo
RL-NFHP2 é cerca de 10 vezes mais rápido que com
o modelo original, possui menos células, o que diminui a quantidade de cálculos por passo, e aprende tão
bem, ou melhor, (valor médio de passos na fase de
teste menor) a concluir seu objetivo.
A Figura 6 mostra o particionamento do estado
(posição-velocidade) após aprendizado do modelo
RL-NFHP2 de 23 células. O eixo das abscissas é
composto pela velocidade normalizada -1,1, enquanto o eixo das ordenadas a posição normalizada
0,1. A estrutura em árvore das células deste mesmo
modelo encontra-se na Figura 7. Observando a árvore é possível entender melhor o particionamento do
estado. A título de exemplo, a célula 2 representa o
quadrado com vértices (-10),(-10,5),(00,5),(00),
401

enquanto a célula 13 o quadrado com vértices
(00,25),(00,5),(0,50,5),(0,50,25).

Estão sendo feitos estudos em outros casos
benchmark para comprovar a melhoria e a aceleração
do treinamento através da inserção destas modificações no modelo RL-NFHP.

Particionamento do Estado no Modelo RL-NFHP de 23 Células

Posição Normalizada

1,0

Referências Bibliográficas
0,5

Figueiredo, K., Vellasco, M., Pacheco, M. A., and
Souza, F. J. (2005). Modified Reinforcement
Learning-Hierarchical Neuro-Fuzzy Politree
Model for Control of Autonomous Agents. International Journal of Simulation Sys.,
Sci.Tech, UK, v.6, n. 10-11, p. 4-13.

0,0
-1,0

-0,5

0,0
Velocidade Normalizada

0,5

1,0

Figura 6. Particionamento do estado do modelo
RL-NFHP de 23 células.

Figueiredo, K., Vellasco, M., Pacheco, M. A. C., and
Souza, F.J. (2004). Reinforcement LearningHierarchical Neuro-Fuzzy Politree Model for
Control of Autonomous Agents, Fourth International Conference on Hybrid Intelligent
Systems (HIS04),pp.130-135, (ISBN 0-76952291-2), IEEE Comp. Society, Japan,
Dec.,2004.

Figura 7. Estrutura hierárquica do modelo
RL-NFHP de 23 células.

Flesch, G. G. (2009). Policy Improvement for the
Reinforcement Learning - Hierarchical Neuro
Fuzzy Politree Model (RL-HNFP). Master
Thesis of University of Duisburg-Essen Chair
of Dynamics and Control.

Constata-se, como esperado, que é necessário
um maior particionamento da região de velocidade
normalizada positiva 0,1, deslocamento para a
direita, e de posição normalizada baixa 00,5, a
esquerda do vale da montanha. Esta é uma região
crítica, pois o carro deve adicionar inércia  sua própria aceleração para conseguir subir a face direita da
montanha.

Haykin, S. (1998). Neural Networks  A Comprehensive Foundation, Macmillan College Publishing Company, Inc.
Jang, J. S. R. (1993). ANFIS Adaptive-Netwo rkBased Fuzzy Inference System, IEEE Trans.
on Sys., Man, and Cybernetics, Vol.23, N.3,
mayjune 1993, p.665-685.

4 Conclusão
Este artigo apresentou uma nova política de
escolha de ação Q-DC-roulette que, para o caso
benchmark estudado, obteve um desempenho superior ao da política -greedy originalmente empregada.
A proposta de early stopping para interrupção do
aprendizado revelou-se de suma importância para a
diminuição do número de épocas necessárias de
treinamento para o caso estudado.
Estas propostas de melhoria e aceleração de
aprendizado para o modelo hierárquico neuro-fuzzy
RL-NFHP mantiveram a capacidade do modelo de
criar e expandir a estrutura de regras sem qualquer
conhecimento a priori (regras ou conjuntos_fuzzy)
extrair conhecimento a partir da interação direta do
agente com ambientes grandes eou contínuos, utilizando aprendizado_por_reforço, de modo a aprender
quais ações devem ser executadas e produzir resultados linguisticamente interpretáveis, sob a forma de
regras fuzzy, que constituam o raciocínio que o agente deve inferir para atingir suas metas.
Este estudo de caso mostra a boa aplicabilidade
do modelo RL-NFHP na área de controle e robótica,
encorajando o prosseguimento da pesquisa de aprendizado_automático utilizando Sistemas Neuro-Fuzzy
Hierárquicos.

Jouffe, L. (1998). Fuzzy Inference System Learning
by Reinforcement Methods, IEEE Trans. on
Sys., Man and Cybernetics, part c, vol.28, n.
3, pp. 338-355.
Kruse, R., and Nauck, D. (1995). NEFCLASS-A
Neuro-Fuzzy Approach for the Classification
of Data, Proc. of the ACM Symposium on
Applied Computing, Nashville.
Mendel, J. M. (1995). Fuzzy Logic Systems for Engineering A Tutorial, Proceedings of the
IEEE, Vol.83 , n. 3, march, p. 345-377.
Souza, F. J., Vellasco, M. M. B. R., and Pacheco, M.
A. (2000). Hierarchical Neuro-Fuzzy BSP
Model  HNFB, Proceedings of the VIth Brazilian Symp. on Neural Networks  Volume 2
 (ISBN 0-7695-0856-1), IEEE Computer Society, Rio de Janeiro, RJ, 22-25 Nov.
Sutton, R. S., and Barto, A. G. (1998). Reinforcement Learning An Introduction, MIT Press.
Vuorimaa, P. (1994). Fuzzy self-organing map,
Fuzzy Sets and Systems, No.66, pp.223-231.

402