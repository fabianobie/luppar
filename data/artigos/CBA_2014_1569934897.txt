Anais do XX Congresso Brasileiro de Automática
Belo Horizonte, MG, 20 a 24 de Setembro de 2014

MAPEAMENTO EXPLICITO PARA MÁQUINAS DE VETORES DE SUPORTE
C ARLA C ALDEIRA TAKAHASHI, A NTÔNIO PÁDUA B RAGA


Departamente de Eletrônica
Escola de Engenharia, UFMG
Belo Horizonte, Minas Gerais, Brasil
Emails ctakahashi@ufmg.br, apbraga@ufmg.br
Abstract In this paper it is studied a explicit mapping approach in the context of large margin classifiers using Support Vector
Machines (SVM). This study is based on the explicit mapping procedure employed on Extreme Learning Machines and a large
margin classifier created with SVMs. It is argued that the mapping used projects any pattern to a feature space where it is linearly
separable. A linear large margin classifier created in this feature space would result in a large margin classifier also in the input
space. The main advantage of this approach is the lack of any relevant tuning parameter, as is the number of neurons on the mapping
procedure is large enough it does not affect the classification process. The viability of this approach is evaluated according to the
experimental results for benchmark classification problems.
Keywords Explicit Mapping, ELM, SVM.
Resumo Este artigo consiste no estudo de um mapeamento_explícito para classificadores de margem larga utilizando Máquinas
de Vetores de Suporte (SVM). Foi feito, desta forma, um estudo investigativo baseado no mapeamento empregado em Extreme
Learning Machines (ELM) aplicado a classificadores feitos a partir de SVMs lineares. É avaliado que este mapeamento é capaz
de projetar qualquer padrão em um espaço de características de forma que ele se torne linearmente separável. Um classificar de
margem larga linear empregado neste espaço de característica poderá resultar em um classificador de margem larga no espaço
de entrada. A principal vantagem desta abordagem é a inexistência de parâmetros para ajuste fino, uma vez que a eficiência de
classificação do método é robusta  quantidade de neurônios utilizado no mapeamento se esta for grande o suficiente. A viabilidade
desta abordagem é avaliada a partir de testes experimentais com problemas de benchmark.
Palavras-chave

.

Introdução

que elas não requerem o ajuste de nenhum parâmetro da SLFN e que procedimento de ajuste de pesos
é feito analiticamente. Uma ELM é uma rede_neural artificial (RNA) que possui uma camada escondida
e, similarmente s RNAs tradicionais, possui seus pesos e bias atribuídos aleatoriamente. No entanto, estes
valores não são ajustados de forma alguma, neste contexto foi provado que isso permite que erro da rede seja
limitado desde que a função de ativação seja infinitamente diferenciável. Diferente de implementações tradicionais de SLFN que utilizam métodos baseados em
gradiente, as ELM tem os pesos da camada de saída
calculada analiticamente a partir da pseudo-inversa da
camada escondida. Esta estratégia reduz consideravelmente o tempo de treinamento de redes_neurais artificiais do tipo SLFN.

Dentre os métodos_de_classificação_de_padrões propostos atualmente, a maioria não foi capaz de superar
o desempenho das Máquinas de Vetores de Support
(SVM) ou outros métodos já consolidados no estado
da arte. Embora tenham sido propostos algoritmos que
apresentaram alguma melhora, muitos deles não se estabeleceram como opções viáveis por diversos motivos, por exemplo a melhora pode não ter sido significativa para justificar a complexidade do método, a
eficiência apresentada só é reproduzida em casos bastante específicos, dentre outros. Paralelamente, existe,
hoje, um esforço em se transpor as limitações existentes nos métodos atualmente no estado da arte. Neste
contexto a questão do ajuste_de_parâmetros de métodos
de aprendizado possui uma certa importância, uma vez
que resultados completamente distintos podem ser obtidos a partir de pequenas variações de parâmetros.
Neste contexto, será estudado a viabilidade de um
método que funcionaria sem a necessidade de ajuste
de parâmetros a partir da incorporação de um mapeamento_explícito a uma Máquina de Vetores de Suporte
com kernel linear. O mapeamento é feito de tal fora
que ele substituiria um kernel mais eficiente que requer ajuste_de_parâmetros_sem que o desempenho sofra alguma degradação significativa.
Extreme Learning Machines(ELM) (Huang et al.,
2006) foram propostas com o objetivo de acelerar
o aprendizado de redes_neurais feedfoward de uma
camada(SLFN). As ELM apresentam características
bastante vantajosas em relação aos métodos tradicionais de aprendizado, dentre eles é possível destacar

Apesar da simplicidade do método, as ELM conseguem atingir altas taxas de sucesso equivalentes a
máquinas_de_vetores_de_suporte. Neste caso, devido a
redução do tempo de treinamento, apesar de não haver melhora na exatidão da rede, é possível dizer o
mecanismo de aprendizagem apresentou uma evolução (Huang et al., 2006).
Contudo, existe uma limitação para as ELM em
relação ao nível de confiança do classificador, uma vez
que a largura da margem de classificação não é maximizada. Apesar das ELM apresentarem um bom grau
de generalização, a sua capacidade de generalização
não é garantida uma vez que ela está diretamente relacionada a margem do classificador (Smola and Bartlett, 2000).
Neste artigo é feito um estudo verificando a vi-

1203

Anais do XX Congresso Brasileiro de Automática
Belo Horizonte, MG, 20 a 24 de Setembro de 2014

abilidade de se implementar um método que faça uso
do mapeamento_explícito como o empregado em ELM
aplicado a um método que garanta a maximização da
margem de classificação, como as Máquinas de Vetores de Suporte (Cortes and Vapnik, 1995). Com isso é
esperado que um método robusto em termos de generalização e que não possua parâmetros de ajuste fino
seja possível.
O artigo é estruturado da seguinte forma A segunda Seção 2 introduz os fundamentos do mapeamento_explícito. A Seção 3 tem como foco a elaboração de um método simplificado, de forma a propiciar
um experimento investigativo. Os experimentos realizados e seu planejamento constam na Seção 4. Por
fim, na Seção 5 é feita uma breve discussão sobre os
resultados e algumas considerações finais são traçadas
para a conclusão do artigo.
2

que a ordem das operações pode ser invertida na implementação e, neste caso, ocorre o mapeamento implícito. Nele a comparação entre cada par de padrões
é feita com o produto escalar ainda no espaço de entrada o que resulta em um espaço de características e,
somente depois, é feita uma transformação não_linear.
K(xi  xj )  (xi )  (xj )

O classificador SVM utiliza, a partir de um kernel,
um mapeamento implícito não_linear afim de se atingir
a separabilidade dos padrões. Este mapeamento permite a computação do produto escalar para a criação
do espaço de características utilizando apenas alguns
padrões no espaço de entrada. Os padrões escolhidos,
denominados vetores de suporte, são capazes de definir um hiperplano que maximiza a margem de separação no espaço de características. A computação destes
vetores de suporte é feita a partir de um problema de
otimização quadrático e, com isso, um classificador
com maximização_de_margem pode ser implementado
(Cortes and Vapnik, 1995).
Em Extreme Learning Machines(Huang et al.,
2006), foi provado que um mapeamento aleatório,
para qualquer problema de probabilidade contínua, resulta em uma matriz de posto completo. Isso ocorre
desde que a função de ativação de cada neurônio na
camada escondida da ELM seja não_linear, não regular
e infinitamente diferenciável em qualquer intervalo.
Liu et al. (2008) e He et al. (2011) propuseram
um método denominado Extreme Support Vector Machine, no qual é determinada a orientação e a posição do hiperplano de separação em relação  origem.
Neste método, ao invés de se calcular os vetores de suporte, as variáveis que definem o plano de separação
são iterativamente atualizadas.

Mapeamento Explícito

De acordo com Cover (1965), a capacidade de separação é fundamental em se determinar uma generalização não ambígua na classificação de um conjunto
de padrões. É mostrado que, se o mapeamento feito
resulta em uma matriz mapeada de posto completo e
que funções não_lineares são utilizadas nesta transformação, é possível determinar uma classe de superfícies com a quantidade de graus de liberdade reduzida
equivalente ao posto da matriz mapeada.
Considere um conjunto de padrões X 
x1 , x2 , . . . , xN ,cujas características da cada padrão
são x  x1 , x2 , . . . , xk , mapeado em um espaço de
maior dimensionalidade
  X  Ed

(2)

(1)

A projeção  é uma função dada por (x) 
1 (x), 2 (x), . . . , d (x), x  X, em que E d é um
espaço Euclidiano d-dimensional.
Se existe uma -superfície de separação w0 
(x)  0, que é restringida de forma a conter k
padrões linearmente separáveis, e a projeção de X
no espaço ortogonal determinado por esses k padrões
estiver em uma posição in genérica, então existem
C(N, d  k) hiperplanos -separáveis (Cover, 1965).
Assim, em um espaço mapeado não_linear suficientemente grande para um dado conjunto de padrões,
é possível reduzir a quantidade de graus de liberdade
da superfície de separação de forma que uma superfície de separação linear seja capaz de discriminar as
classes do conjunto de padrões.
De forma geral é feito o mapeamento do espaço
de medidas para um espaço de características. Com
uma abordagem explícita os padrões são projetados
em um espaço de características de alta dimensionalidade de acordo com uma transformação não_linear e
somente depois é feita a medida da diferença entre padrões, por meio de um produto escalar. Entretanto, a
equação (2), provada por Boser et al. (1992), mostra

3

Implementação

É esperado um método que atinja resultados de classificação semelhantes aos da LS-SVM. Mas, ao invés
de se utilizar um kernel, é proposta a projeção dos dados de entrada em um espaço de maior dimensionalidade com a utilização de uma rede_neural, semelhante
a ELM, seguida da aplicação de uma função de ativação não_linear. Após isso alguns vetores de suporte serão calculados neste espaço projetado, utilizando uma
implementação da SVM equivalente a uma SVM com
o kernel linear.
Com esta abordagem, espera-se que seja possível
atingir resultados similares a SVMs com kernels mais
complexos, mas com a vantagem de não precisar de
ajustar nenhum parâmetro. É possível fazer essa consideração, uma vez que a quantidade de neurônios da
RNA que determina o espaço de mapeamento causa
um efeito assimptótico sobre a exatidão da classificação. Ou seja, se a quantidade de neurônios for grande
o suficiente, o desempenho da rede tenderá ao melhor
resultado possível de ser atingindo por esse método.
Dessa forma, independentemente do número exato de

1204

Anais do XX Congresso Brasileiro de Automática
Belo Horizonte, MG, 20 a 24 de Setembro de 2014

4

neurônios escolhido para a camada escondida, o melhor desempenho da rede pode ser obtido o que torna
desnecessário o ajuste deste parâmetro e o torna não
sensítivo ao método.
A seguinte função de decisão foi utilizada, com
base no princípio das máquinas_de_vetores_de_suporte

4.1

I(x)  sign

i i (x)

(3)

i

A qual pode ser reescrita, em termos de pesos para
os hiperplanos de separação, da seguinte forma
w  (xi ) + b  1,
ifyi  1
w  (xi ) + b  1, ifyi  1

(4)

A função de mapeamento  na equação 1 é o produto escalar dos pesos que são aleatoriamente atribuídos, semelhantemente a ELM. E a não-linearidade é
imposta na transformação pode ser exponencial ou senoidal, entre outras, desde que satisfaçam as condições
da ELM mencionadas na seção anterior. A função de
ativação foi arbitrariamente escolhida
(x0 i )  exp(x0 i )

(5)

A determinação do hiperplano ótimo w0 (x)+
b0  0 no espaço de características é atingida a partir
da resolução do seguinte problema de otimização

4.2

w  w,
yi (w  (xi ) + b)  1, i  1, . . . , N
(6)
Os padrões cujo lagrangiano forem diferentes de
0 são os vetores de suporte. Uma vez que eles são
obtidos, a função de decisão é reduzida a

4.2.1

!
I()  sign

i i   + b0

, i  vet. suporte
(7)

Tabela 1 Descrição das Bases de Dados Sintéticas

Neste caso , os pesos são dados por
N
X

yi i0 (xi )

Dados Sintéticos

As três bases de dados sintéticas estão especificadas
na seguinte tabela

i

w0 

Bases de Dados

Foram feitos testes com 3 bases de dados sintéticas e
4 bases reais, descritas nas tabelas 1 e 2.
A princípio só foram avaliadas bases de dados
com atributos numéricos, porque o processo de mapeamento investigado não é necessariamente capaz de
lidar com características não comparáveis, como gênero ou forma ou cor. Portanto, os conjuntos de dados
escolhidos não apresentam características categóricas.

min
sujeito a

X

Configuração do Experimento

O critério de avaliação utilizado foi, principalmente, a
exatidão dos testes de classificação.
Foram investigados os efeitos intrínsecos do método afim de determinar fatores que corroborem a viabilidade do método. Neste contexto foi avaliada a
exatidão da classificação para diferentes tamanhos da
dimensão de projeção. Dessa forma é possível verificar a ocorrência de um comportamento assimptótico.
Ao mesmo tempo, a quantidade de vetores de suporte
foram observadas.
O experimentos foram aleatorizados utilizando o
método k-fold para determinar os grupos de treinamento e teste, com 3 repetições. A dimensão do mapeamento foi estipulada como nO  index, em que
index  2i com 0, 1, 2 . . . 10 e nO é a dimensão original do problema.
A função não-linear utilizada neste estudo requer
argumentos de valor positivo. Além disso, existe a necessidade de se minimizar os efeitos de valores muito
discrepantes dos atributos. Assim, melhor solução é a
normalização de cada um dos atributos de todos os dados para o intervalo 0, 1 de acordo com os dados de
treinamento, neste caso os dados de teste são sujeitas
aos mesmos parâmetros de normalização.

!
X

Procedimento Experimental

(8)

Nome
Gaussian

Atributos
2

Padrões
100

Spiral

2

100

Circle

2

100

i1

O algoritmo do método está detalhado a seguir
Algoritmo M APEAMENTO E XPLÍCITO(X, Y )
R  Random Matrix
. R  <(k,d)
b  Random Matrix
. b é o bias
U X R+b
H  (U )
.  é uma função não-linear
Resolva o problema da eq. (6)
Determina os vetores de suporte
Construa o classificador a partir da eq. (3)
end Algoritmo

1205

Descrição
Duas
distribuições
Gaussianas
levemente
superpostas.
Duas espirais
entrelaçadas
com
ruido
(sd0.05).
Um circulo
dentro de um
quadrado.

Anais do XX Congresso Brasileiro de Automática
Belo Horizonte, MG, 20 a 24 de Setembro de 2014

Os testes com bases sintéticas tem como objetivo
a verificação da viabilidade do método proposto a partir de entradas totalmente controladas.
4.2.2

Dados Reais

As bases de dados reais foram obtidas do Repositório
de Aprendizado de Máquina da UCI (Bache and Lichman, 2013), e estão relacionados na tabela a seguir.
(a) Exatidão

Tabela 2 Descrição das Bases de Dados Reais
Nome
Ionosphere
(Ion)

Atrib.
33

Padr.
351

Class.
2

Pima
Indian
Diabetes
(PID)
Bupa
Liver
Disease
(BLD)

8

392

2

6

345

2

Iris
(Iri)

4

150

3

Wine
(Win)

13

178

3

4.3
4.3.1

Descrição
Elétrons
livres observados
na ionosfera, foi
avaliada
a
existência de
estrutura.
Dados
clínicos
de diagnóstico de
diabetes.
Exames
de sangue
e quantidade de
bebida
para diagnóstico de
doenças
hepáticas
em
homens.
Catálogo
de
três
variedades
da
flor
Íris.
Chemical
analysis
of green
wines for
discriminating
three
different
types.

(b) Número de Vetores de (c) Exemplo de ClassificaSuporte
ção

Figura 1 Resultado para o teste com duas distribuições gaussianas em um plana bidimensional.(1a Média da exatidão e seu intervalo de confiança para diferentes tamanhos de mapeamento, com regularização
C  1 C  0.1. (1b) Média da quantidade de vetores de suporte culculados para diferentes dimensões
de mapeamento, com C  1 e C  0.1. Em (1c), há
um exemplo da classificação.

(a) Exatidão

(b) Número de Vetores de (c) Exemplo de ClassificaSuporte
ção

Figura 2 Resultados para as duas espirais entrelaçadas. Em (2a), a exatidão média e seu intervalo de confiança são mostrados para diferentes dimensões de mapeamento. São mostrados os rezultados para a variável
de regularização C  1 e C  0.1. (2b) É a quantidade média de vetores de suportes calculados. (2c)
mostra um resultado de classificação possível.

Resultados
Dados Sintéticos

guras 1 a 3.
Para cada base de dados são mostradas a exatidão

Os resultados encontrados para os problemas de classificação com dados sintéticos estão mostrados nas fi-

1206

Anais do XX Congresso Brasileiro de Automática
Belo Horizonte, MG, 20 a 24 de Setembro de 2014

4.3.2

da classificação, o número de vetores de suporte e um
exemplo de classificação.
São mostrados os resultados encontrados para a
exatidão e quantidade de vetores de suporte para dois
valores diferentes da variável de regularização C  1
e C  0.1.

Real Data

O resultado da exatidão para as bases de dados reais
estão mostrados nas tabela 3. De acordo com os resultados de benchmark obtidos por Van Gestel et al.
(2004), a exatidão de classificação atingida pelo método estudado pode atingir um dos melhores algoritmos no estado da arte, o Least Square Support Vector
Machines (LS-SVM).
Tabela 3 Exatidão dos Teste para Bases de Dados Reais
BD
Ion
PID

(a) Exatidão

BLD
Iri
Win

1
74.1
(4.7)
59.2
(1.8)
59.2
(1.8)
68.7
(8.1)
83.1
(13.9)

2
80.1
(2.7)
62.2
(5.5)
62.2
(5.5)
72.7
(17.5)
94.9
(4.5)

4
83.5
(3.6)
63.0
(5.2)
63.0
(5.2)
92.0
(0.5)
96.1
(2.0)

8
87.7
(4.7)
59.7
(2.9)
59.7
(2.9)
96.0
(2.0)
97.8
(1.9)

Índice
16
87.5
(4.0)
61.7
(2.1)
61.7
(2.1)
96.0
(5.3)
97.2
(1.9)

32
90.6
(4.8)
65.8
(1.8)
65.8
(1.8)
97.3
(3.1)
97.8
(0.9)

64
92.0
(3.5)
70.7
(1.1)
70.7
(1.1)
96.7
(3.1)
98.3
(1.9)

128
91.2
(0.5)
73.2
(5.0)
73.2
(5.0)
97.3
(2.3)
97.2
(1.9)

256
91.7
(1.8)
75.0
(1.7)
75.0
(1.7)
97.3
(3.1)
97.2
(1.9)

BM
96.0
(2.1)
77.3
(2.5)
70.2
(4.1)
98.6
(1.3)
99.2
(1.2)

O benchmark de referência está disponível em
(Van Gestel et al., 2004).
(b) Número de Vetores de (c) Exemplo de ClassificaSuporte
ção

As quantidades de vetores de suporte observadas
para diferentes tamanhos do espaço de características
estão relacionadas na tabela 4, nela é possível perceber
um decrescimento assimptótico deste valor.

Figura 3 Resultado para o problema do círculo dentro do quadrado. (3a) é a média da exatidâo com seu
intervalo de confiança, para diferentes dimensões do
mapeamento e regularização C  1 e C  0.1. A
quantidade ve vetores de suporte para os mesmos casos do item anterior estão em (3b). Um exemplo da
solução da classificação é mostrada em (3c).

Tabela 4 Vetores de Suporte para Bases de Dados Reais

BD

1
Ion 187
(24)
PID 130
(10)
BLD 178
(5)
Iri
44
(9)
Win 32
(10)

Para estas bases sintéticas a exatidão da classificação obtida é alta o suficiente para que um método
utilizando este mapeamento seja considerado viável.
Como a premissa de que a exatidão realmente cresce
assimptoticamente para um resultado final  medida
que a dimensão de mapeamento aumenta foi constatada para estes casos, o método de fato poderá ser robusto  quantidade de neurônios escolhida. A variação
da variável de regularização afete de forma muito mais
significatica a precisão do método do que a sua exatidão.
O número de vetores de suporte também apresentam um comportamento assimptótico, neste caso a
regularização influencia na quantidade encontrada de
vetores de suporte para cada caso.
Os problemas de duas distribuições Gaussianas e
do Círculo no Quadrado podem ser utilizados para verificar a escalabilidade do método. Uma vez que eles
podem ser implementados para qualquer dimensão arbitrária maior que 2, a medida dos tempos de treinamento e teste para resolução destes problemas podem
ser utilizadas para gerar a curva de escalabilidade do
método para diferentes dimensões de entrada. E dessa
forma a aferir a capacidade de escalonamento do método.

2
155
(3)
131
(11)
170
(6)
28
(4)
25
(3)

4
121
(1)
124
(12)
155
(12)
27
(3)
25
(6)

8
101
(10)
121
(12)
149
(8)
19
(3)
30
(3)

Índice
16 32
86 74
(3) (5)
118 116
(11) (12)
145 140
(9) (5)
17 17
(3) (4)
26 27
(1) (2)

64
68
(8)
116
(11)
137
(6)
16
(4)
27
(2)

128
61
(2)
112
(11)
138
(4)
15
(4)
27
(2)

A comparação dos resultados é feita com os valores obtidos a partir de um benchmark já existente
preiviamente na literatura(Van Gestel et al., 2004) pelos próprios autores do método Least Square Support
Vector Machines. Com isso é possível admitir que os
resultados já encontrados pelo método LS-SVM foram
os mais favoráveis para o mesmo, com os parâmetros
perfeitamente ajutados e validados.
5

Conclusões

Considerando os dados reais, os resultado são bastante
satisfatórios, uma vez que eles atingem resultados semelhantes aqueles previamente obtidos na literatura,

1207

256
56
(2)
113
(12)
136
(5)
14
(2)
28
(1)

Anais do XX Congresso Brasileiro de Automática
Belo Horizonte, MG, 20 a 24 de Setembro de 2014

por Van Gestel et al. (2004), Huang et al. (2006) e He
et al. (2011). Entretanto é importante observar que é
buscado um método que não necessite de parâmetros
que precisem ser ajustados.
Foi avaliada a abordagem das ELM para a geração
de um espaço de características de maior dimensionalidade. Com isso, o único parâmetro que pode ser ajustado é o número de neurônios da camada escondida,
que representam a dimensão do mapeamento. Isto é
vantajoso uma vez que os testes mostram que o método é robusto a tal parâmetro, que pode ser ajustado
para qualquer valor grande o suficiente sem influenciar
significativamente nos resultados.
Os tempos de treinamento e teste não foram determinados, mas, devido a características do próprio
método estes devem ser maiores do que outros métodos como SVM e ELM.
Os diferentes valores da variável de regularização
apresentaram alguma influência sobre as quantidades
de vetores de suporte calculadas, mas não houve efeitos significativos sobre a exatidão do método.
Em trabalhos futuros é necessário estudar funções não-lineares diferentes para o mapeamento, e os
efeitos de tal mudança sobre bases de dados distintas.
Além disso, como este método seria propício a bases
de dados em que há superposição, a variável de regularização deve ser melhor avaliada.
Como já foi mencionado, existe ainda a questão
da escalabilidade que deve ser aferida afim de se configurar melhor o método e suas limitações. E com isso,
compará-lo com outras abordagens na literatura.
Existe um método proposto por Liu et al. (2008),
e mais tarde aperfeiçoado por He et al. (2011) que
apresenta uma solução bastante elegante para a questão do cálculo dos hiperplanos de separação utilização
mapeamento aleatório para uma dimensão maior. A
partir deste artigo, e outros, é possível avaliar diferentes formas de se implementar o método de forma
eficiente.

Applications in Pattern Recognition, Electronic
Computers, IEEE Transactions on 14(3) 326
334.
He, Q., Du, C., Wang, Q., Zhuang, F. and Shi, Z.
(2011). A parallel incremental extreme SVM
classifier, Neurocomputing 74(16) 2532  2540.
Huang, G. B., Zhu, Q. Y. and Siew, C. K. (2006). Extreme learning machine theory and applications,
Neurocomputing 70(1-3) 489501.
Liu, Q., He, Q. and Shi, Z. (2008). Extreme support
vector machine classifier, in T. Washio, E. Suzuki, K. Ting and A. Inokuchi (eds), Advances in Knowledge Discovery and Data Mining,
Vol. 5012 of Lecture Notes in Computer Science,
Springer Berlin Heidelberg, pp. 222233.
Smola, A. J. and Bartlett, P. J. (eds) (2000). Advances
in Large Margin Classifiers, MIT Press.
Van Gestel, T., Suykens, J. A. K., Baesens, B., Viaene, S., Vanthienen, J., Dedene, G., De Moor, B.
and Vandewalle, J. (2004). Benchmarking least
squares support_vector_machine classifiers, Machine Learnig 54(1) 532.

Agradecimentos
Ficam aqui registrados os agradecimentos a CAPES
pelo seu apoio.
Referências
Bache, K. and Lichman, M. (2013). UCI machine learning repository.
Boser, B. E., Guyon, I. M. and Vapnik, V. N. (1992).
A training algorithm for optimal margin classifiers, Proceedings of the Fifth Annual Workshop
on Computational Learning Theory, COLT 92,
ACM, New York, NY, USA, pp. 144152.
Cortes, C. and Vapnik, V. (1995). Support-vector
networks, Mach. Learn. 20(3) 273297.
Cover, T. M. (1965). Geometrical and Statistical Properties of Systems of Linear Inequalities with

1208