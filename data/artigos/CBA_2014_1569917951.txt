Anais do XX Congresso Brasileiro de Automática
Belo Horizonte, MG, 20 a 24 de Setembro de 2014

STRUCTURE OPTIMIZATION OF THE RECURRENT RANDOM NEURAL
NETWORK
Erivelton Geraldo Nepomuceno


GCOM - Grupo de Controle e Modelagem
Departamento de Engenharia Eletrica, Universidade Federal de Sao Joao del-Rei
Praca Frei Orlando, 170 - Centro, 36307-352, Sao Joao del-Rei, MG, Brazil
Email nepomuceno@ufsj.edu.br
Abstract The RNN is a recurrent fully connected neural network model inspired by the spiking behaviour
of biophysical neurons. Various learning algorithms (including gradient, reinforcement and associative) have
been developed and for the RNN, and they consist in updating the weights of the network. Little attention has
been given to determining the number of neurons in the RNN, and the number of neurons has been chosen as
a function of the specific problem being considered, or they have been chosen empirically. This paper presents
a method based on multi-objective optimization to compute the number of neurons of the RNN when it is used
with gradient descent learning, based on a trade-off between the minimization of the mean square error and the
network size in number of neurons. This method is illustrated with four case studies.
Keywords

Recurrent Random Neural Network, Structure Optimization, Multiobjective Optimization.

Resumo Random Neural Network (RNN), ou Rede Neural Aleatoria, e um modelo de rede recorrente totalmente conectada inspirada no comportamento biologico de neuronios. Usualmente, o aprendizado da RNN
consiste na atualizacao dos pesos da rede. Pouca atencao tem sido observada em investigar a determinacao do
numero de neuronios. Em varios trabalhos, o numero de neuronios e escolhido empiricamente. Este trabalho
apresenta um metodo baseado na otimizacao_multiobjetivo para determinar o numero de neuronios de uma RNN.
O metodo consiste em encontrar solucoes que sao um compromisso entre a otimizacao do erro quadratico medio
e o numero de neuronios. Foi desenvolvido um algoritmo que permite determinar o numero de neuronios para
uma RNN. O metodo proposto neste trabalho foi aplicado em 4 estudos de caso.
Palavras-chave

.

Introduction

Loukas, 2007 Gelenbe et al., 2008 Gelenbe, 2009).
Gradient descent learning in the RNN consists in optimizing an objective function, which
is based on an error between input and output
data (Gelenbe, 1993 Georgiopoulos et al., 2011).
There also well succeed attempts to use second
order methods to solve the optimization problem
that the RNN poses examples of these methods are the quasi-Newton method (Likas and
Stafylopatis, 2000), the LevenbergMarquardt
method (Basterrech et al., 2011) and the RPROP
(Riedmiller and Braun, 1993). Regarding the
number of objectives, one of the first attempts to
apply multiobjective optimization in learning of
the RNN is seen in (Nepomuceno, 2013). In such
work, the author updates the weights of the RNN
by means of a optimization of two objectives simultaneously. It has been shown that it is possible
to find a trade-off between two objectives and a
general procedure for M objectives is stated.
Although, learning in the RNN has been received a lot of attention, the determination of
number of neurons remains an empirical procedure and according to Georgiopoulos et al. (2011)
there is no attempt to conjointly optimize the
weights and the structure of the RNN network
using a multi-objective optimization approach.
This reveals a different research stage in comparison to Multilayer Percepton, in which there
are a considerable number of works that deals
with optimization of topology and number of

The random neural network (RNN) is inspired
by the spiking behaviour of biophysical neurons
(Gelenbe, 1989 Gelenbe, 1990). It is a recurrent
network, in which all N neurons may be fully connected. Signals propagate between neurons, and
enter or leave the network, both as excitatory and
inhibitory spikes of amplitude +1 and 1, respectively.
Over the past two decades, much research
has been done on developing the RNN and on
applications in diverse areas of engineering and
science. In (Timotheou, 2010), some features
that explain the success of the RNN are summarized i) it presents an efficient computation
method by means of an analytical equation for its
steady-state probability distribution (Timotheou,
2010 Gelenbe, 1989) ii) it has low complexity for
standard learning algorithms (Gelenbe and Hussain, 2002 Gelenbe, 1993) iii) it has a close relation with biological neuronal network (Gelenbe
and Cramer, 1998) iv) and it can serve as an
universal approximation for bounded continuous
functions (Gelenbe et al., 1999). Some of the most
important applications of the RNN concern on optimization (Gelenbe et al., 1993 Aguilar and Gelenbe, 1997 Gelenbe and Timotheou, 2008), image processing and video compression (Gelenbe
et al., 1996) and packet routing in Internetlike networks (Gelenbe et al., 2006 Gelenbe and

3862

Anais do XX Congresso Brasileiro de Automática
Belo Horizonte, MG, 20 a 24 de Setembro de 2014

neurons (Desideri, 2012 Vieira et al., 2007 Abbass, 2003 Costa et al., 2003). One strategy
usually applied to deal with structure selection
is based on multiobjective optimization (Martins
et al., 2013).

must satisfy the global balance equations
p(k)

N
X
(i) + (i) + r(i)1ki >0 
i1

This paper aims at dealing with structure optimization of recurrent random neural network.
For structure optimization we are interested in
determining the optimal number of neurons for
the RNN. We propose a multiobjective optimization problem (MOP), in which mean squared error
(MSE) and number of neurons (N ) are conjointly
minimized. The result is a Pareto-set of solutions
for which a decision maker chooses the best one.
In this paper, we present three methods to choose
the solution among the Pareto-set i) minimum N 
ii) minimum MSE and iii) minimum norm. We
applied our method in four case studies. The first
one is a theoretical and it aims at showing that
Pareto-set obtains the precise number of neurons.
To achieve this feature, we have designed a three
input and one output RNN with arbitrary weights
in order to be considered as the system to be modelled. It was possible to show that the precise
number of neurons consists in one of the solutions
of Pareto-set. Two classification problems and a
function approximation for a piecewise nonlinear
system complete the four case studies. In all cases
it was possible to present a Pareto-set and offer a
systematic way to determine the number of neurons for the RNN.



N
X
p(ki+ )r(i)d(i) + p(ki )(i)1ki >0

(1)

i1

+p(ki+ )(i) +

N
X
+
p(kij
)r(i)p + (i, j)1kj >0
j1

++
+p(kij
)r(i)p (i, j)

+ p(ki+ )r(i)p (i, j)1kj 0 

In (Gelenbe, 1989), the solution for (1) is given
as
p(k) 

N
Y

1  qi qiki

(2)

i1

where
qi 

+ (i)
r(i) +  (i)

(3)

and + (i),  (i) for i  1,    , N satisfy the following system of nonlinear simultaneously equations
+ (i)

N
X



qj r(j)p+ (j, i) + (i),

j1

 (i)

N
X



qj r(j)p+ (j, i) + (i)

(4)

j1

Consider the following rates

2

w+ (i, j)  ri p+ (i, j)
w (i, j)  ri p (i, j),

The RNN Model

(5)
(6)

from Eq. (5)(6) and taking into account that the
sum of the three probabilities for all j neurons is
1, the following expression for ri is derived

In this section, we present the RNN model based
on (Gelenbe, 1989 Gelenbe, 1993 Timotheou,
2010). The RNN is a recurrent network of N
fully connected neurons which exchange positive
and negative impulse signals. At any time t, the
state of neuron i is described by its signal potential
ki (t), which is the accumulated positive signal. A
neuron is excited when ki (t) > 0. The excitation
probability of the neuron is qi (t)  P rki (t) >
0  1.

1

ri  (1  d(i)

N
X
w+ (i, j) + w (i, j),

(7)

j1

Positive and negative signals can also arrive from
the outside world according to Poisson processes
of rates i and i , respectively. Each neuron accumulates signals as they arrive, and fires if its total
signal count at a given instant of time is positive.

When a neuron is excited, it can randomly
fire according to the exponential distribution with
rate ri resulting in the reduction of its potential by
1. The fired spike may behave in three different
ways. It may reach another neuron as positive
signal with probability p+ (i, j), or as a negative
signal with probability p (i, j), or it departs from
the network with probability d(i).

2.1

Learning in the RNN

A gradient descent supervised learning algorithm
for the recurrent RNN was developed in (Gelenbe,
1993). In the RNN, the mth input training
 
pattern xm is represented by the vectors 

m1 , . . . , mN  and   m1 , . . . , mN . The
procedure to assign the input to exogenous rates
is described in (Timotheou, 2010).
The output of the mth pattern, ym are represented by the steady state excitation probabilities

Let k(t)  ki (t),    , kn (t) be the vector of
signal potentials at time t, and k  (ki ,    , kn )
be a particular value of the vector. p(k) is
the stationary probability distribution p(k) 
limt P rk(t)  k. In the steady state p(k)

3863

Anais do XX Congresso Brasileiro de Automática
Belo Horizonte, MG, 20 a 24 de Setembro de 2014

of the neurons qm  qm1 , . . . , qmN  obtained from
applying input training pattern m to the RNN.
The learning process consist in updating the RNN
weights w+ (i, j)  RN N and w (i, j)  RN N .
Or in a more general way, it consists in updating
the matrix W  R2N N , such that
 +

w (i, j)
W 
(8)
w (i, j)
3

z2
Feasible set

z1

f1

Figure 1 Two-dimensional illustration of a multiobjective problem, where z  R2 f  R2  R2 .
Adapted from (Chong and Zak, 2013).

The multiobjective optimization problem (MOP)
is stated as follows (Chong and Zak, 2013)


f1 (z1 , z2 , . . . , zn )
 f2 (z1 , z2 , . . . , zn ) 


(9)
minimize
f(z)  

..


.

2004). Usual approaches to obtain the Pareto-set
are Weighting Problem (Chankong and Haimes,
1983) Constraint Method and Goal Programming
(Diwekar, 2008). For a convex problem, a local
Pareto-set is also a global Pareto-set. For a nonconvex problem, it is possible to determine the
Pareto-set by means of finding a subset of nondominated solutions of all local Pareto-set. Once
a Pareto-set is determined, the choice of a single most preferred solution of P by a decision
maker (DM) (Chankong and Haimes, 1983) is undertaken. This process is usually called decisionmaking task.
As mentioned, a MOP solution may use a
stochastic optimization process. In such situation,
each point of Pareto-set is a vector of solutions, as
the optimization procedure runs several times. We
consider the following definition for a biobjective
case of N and MSE, which describes a procedure
to exclude dominated solutions.

fA (z1 , z2 , . . . , zn )
z  ,

where f  Rn  RA and   Rn is the constraint
set.
Consider the following definition (Takahashi
et al., 1997)
Definition 3.1 Let f  Rn  RA and z   be
given. For the optimization problem
minimize
subject to

Objective
function space



Multiobjective Optimization

subject to

f2

f(z)
z  

a point z   is called a Pareto minimizer or
nondominated solution if there is no z   such
that f(z)  f(z ) and f(z) 6 f(z ).

In other words, the point z is a Pareto minimizer, if there is no other z   that would decrease some objectives without causing simultaneous increase in at least one other variable. The
set of z is called the Pareto-set (P) (Chong and
Zak, 2013). In this way, we define  as the objective function space of the Pareto-set, such that
f  P  . Multiobjective function defines to
each variable z a multiobjective vector function
value in the objective function space. Figure 1
shows an illustration of MOP where n  2 and
A  2, such that z  R2 and the vector of objective functions is given by f  R2  R2 .
Nonconvex problems might present local solutions. In such case, we may state the following
definition.

Definition 3.3 Consider a pair of solutions in
objective function space given by (f1i , f2i ), where
f1i is N and f2i is mean value of MSE for all runs
of gradient descent simulation. A ith solution will
be excluded if one of the following conditions is
satisfied. Table 1 shows an example of this exclusion.
(a) It exists at least one pair (f1j , f2j ), with j 6 i
that f1j < f1i , f2j < f2i and f2j 6 f2i at the
0.05 level of significance.
(b) It exists at least one f1j with j 6 i that
f1j < f1i and f2j  f2i at the 0.05 level of
significance.

Definition 3.2 A solution z is a local Pareto
minimizer in a region N (z , )   if exists
 > 0 and there is no exist any other solution
z  N (z , ) such that for f(z)  f(z ) and
f(z) 6 f(z ).


4

Structure Optimization of the RNN

In this section, we present a structure optimization algorithm of the RNN. In general terms, the
algorithm consist in minimize simultaneously two
functions mean square error and number of neurons as stated in the following equations

The main objective to solve a MOP is to
determine the Pareto-set (Marler and Arora,

3864

Anais do XX Congresso Brasileiro de Automática
Belo Horizonte, MG, 20 a 24 de Setembro de 2014

each value of N in a predefined range in step
1.

Table 1 Exclusion of dominated solution. In this
hypothetical solution, for each number of neurons
N  2 3 4 5 there are 5 solutions of MSE. The mean
of MSE is calculated and it is presented in the second
column. If one makes a traditional process of exclusion, only the pair (40.130) will be excluded. That is
not statistical correct, as the mean of 0.094 is not statistical different to 0.100. Therefore, the pair (30.094)
should also be excluded and only two pairs of solution,
namely, (20.100) and (50.014) will belong to Paretoset.

N
2
3
4
5

Mean
0.100
0.094
0.130
0.014

minimize
W

subject to

1
0.100
0.090
0.110
0.010

2
0.090
0.080
0.150
0.020

Runs
3
0.080
0.070
0.140
0.010

4
0.110
0.110
0.120
0.010

5
0.120
0.120
0.130
0.020


M X
N

X

f  1
ci (gi (qi,m )  yi,m )2
1
2 m1 i1


f  N
2
W, N   ,

(10)

where f1 is the error function for mth inputoutput pair, ci  0, 1 and gi is a differentiable
function of neuron i. f2 is the number of neurons.
4.1

Steps of the algorithm

Here, we summarize the steps for the structure
optimization algorithm follows
(1) Initialize the matrices W and define minimum and maximum values of N . The initiation of W will be made at random among
nonnegative matrices. We choose a value for
the learning rate such as   0.1 for all simulations. The minimum value of N should take
account of the number of inputsoutputs.

(4) Exclude dominated solutions. As the system
is nonconvex and we do not have guarantee
of global solution, we have to exclude dominated solutions. In our case, as for each N
we have a vector of values for MSE, we developed a comparison regarding the mean values of such vectors. Moreover, we also test
the claim that two means of MSE vectors are
or are not different (Shayib, 2013). This procedure is undertaken as stated in Definition
3.3.
(5) Apply a decision-making task. In this work
we indicate three strategies to choose the
most suitable solution from a Pareto-set. The
first procedure stress the minimum size of the
network, that is, the solution of the Paretoset chosen will be that one which presents the
minimum value of N . This may be suitable
for implementations of RNN in hardware in
which the number of variables may be a constraint. The second possible solution relies
on minimum of mean square error. In this
case, it is stressed the fitness to system. Finally, the solution may be chosen as an Euclidean norm of the two objectives. First we
normalize objective function into a scale of 0
to 1. This solution represents the minimum
distance to utopian solution, that is a solution
which minimizes both f1 and f2 . This may be
used in occasion in that the user should take
account of the complexity and the fitness of
the RNN simultaneously.
This algorithm was implemented in Matlab
- Version 7.14.0.739 (R2012a), using a modified
version of toolbox proposed by (Abdelbaki, 1999).
5

Case Studies

In this section we present four case studies. For
each value of N the optimal solution following
the gradient descent with random initial condition was undertaken. The vertical bars represent
one standard deviation, which means that 95 of
solutions are expected to be inside this range.

(2) Define a stop criteria. In our case, we define
tolerance of MSE and a maximum number of
interactions. For all simulations, we consider
6000 a maximum number of interactions. For
each case study we define an appropriate tolerance for MSE.
(3) Perform an optimization process to find a
Pareto-set. This step consist in a MOP, in
which may be solved by several forms (Branke
et al., 2008 Deb et al., 2002 Miettinen and
Makela, 2002 Chankong and Haimes, 1983).
In this approach, we adopt the gradient descent method with multiple initiations, that
is, we try to initiate the system in different
areas of parameter space. As the system is
nonconvex, it is possible to achieve a suboptimal solution. We proceed in such way for

5.1

The RNN System

This theoretical case study aims at showing the
property of the proposed method in producing the
Pareto-set with optimal solution. The system was
designed with three inputs and one output. The
RNN contains 14 neurons and weights of the RNN
were generated arbitrary following a uniform distribution ranging from 0 to 0.2. The input consist
of random values of +1 and -1 as shown in Figure 2. The three inputs are applied in the first

3865

Anais do XX Congresso Brasileiro de Automática
Belo Horizonte, MG, 20 a 24 de Setembro de 2014

three neurons. It has been used 32 samples of the
input. The output collected from 14th neuron of
the RNN is shown in Figure 3.

x 10

4

9
8
7
6

0
1
0

5

10

15

20

25

30

MSE

Input 1

1

35

Input 2

1

5
4

0
3
1
0

5

10

15

20

25

30

35

2

Input 3

1

1
0
0.1
Output

1

0

0
5

10

15

20

25

30

35

5

10

15
20
Samples

25

30

35

10

15

20

25

N

0.05
0
0

Figure 3 The RNN system. Tolerance 0.0010.
The x-axis represents the number of neurons N
and the y-axis represents the mean square error
(MSE) of training. The solutions marked with 
are the final solutions belonged to the Pareto-set.

Figure 2 Input and output of the RNN system.
The RNN contains 14 neurons. Input is applied
in first three neurons and output is collected from
the 14th neuron.

is diabetes positive or not. The original dataset is
composed of 768 examples. In our study we use
70 examples for training and 192 examples for validation. In this case, only the minimum structure
possible, that is, with 9 neurons (one for output
and eight for inputs) was chosen to Pareto-set, as
seen in Figure 4. Values of N was from 9 to 20.
This best solution present a MSE of 0.219 and
regarding its capacity to classify, it was observed
that for a different set of examples, the error of
classification was 25.3. This result is close to values obtained by Alba and Chicano (2004), where
the authors found 21.8 and 25.8 for Backpropagation and LevenbergMarquardt respectively.

For this case, we examined a range from 4
to 24 neurons. Applying the concept of stochastic nondomination (Definition 3.3), only two number of neurons were considered nondonaminated
4 and 14. As one can see, if only the mean was
compared, the Pareto-set would also include numbers 5 and 13. However, as these means do not
present significant difference compared with other
means in this set, the values with lower number of
neurons were considered. It is important to emphasize that the precise number, that is 14, was
obtained by our method, which indicate an interesting behaviour of the RNN. It looks like that the
RNN presents some property for self-learning, as
it indicates the best structure as the lowest MSE.
Another feature is related to choose of the minimum number of neurons. That may indicate some
interesting feature of being able to represent a system using minimum information. Finally, the behaviour of MSE according to number of neurons
can be used to design some policy in situations
where resilience is required, for instance, in a real
application, if one of the neurons is disconnected,
according to Figure 3, it could be better to reduce the number of neurons from 13 to 4, which is
not an obvious policy. Moreover, it suggests the
optimal N has the minimum value of MSE.
5.2

5

5.3

Liver Disorder Classification

The BUPA Liver Disorders data set from Irvine
Machine Learning Database Repository consists of
345 data points with 6 features (Bache and Lichman, 2013). The first 5 variables are all blood
tests which are thought to be sensitive to liver disorders that might arise from excessive alcohol consumption. The sixth variable is the drinks number of half-pint equivalents of alcoholic beverages
drunk per day. We used the first 50 points to train
and the following 50 points to validate.
The third result presents values of N from 7
to 26. In this case, only the first value was pointed
out as a Pareto Minimizer. It can be seen in Figure 5 Values of MSE stayed in a close range. As
a consequence, there was no significant difference
between the mean of MSE vectors of the chosen
result and other solutions. Comparing to published solution, this method achieved an error of
26.19, while in the authors in (Georgiopoulos

Diabetes Classification

The Diabetes dataset was obtained from Proben1
(Prechelt, 1994). This consist of 8 inputs based
on personal data and medical examinations. The
output consist of 0 and 1 indicate if the individual

3866

Anais do XX Congresso Brasileiro de Automática
Belo Horizonte, MG, 20 a 24 de Setembro de 2014

0.225


f (x) 

0.224

0.223

MSE

(11)

Finally, the last case was piecewise function.
In this case, the Pareto-set was presented with
three solutions, as shown in Figure 6. N  2
represents the solution in which decision-making
task applies the minimum value for N as criteria.
If one applied the minimum value for MSE, the
best solution would be N  4, while the N  3 is
the best solution for the case that Euclidean norm
of the normalized objective functions is considered
as criteria.

0.222

0.221

0.22

0.219

0.218

0.217

0  x < 0.5
0.5  x  1

0.25x if
0.75x if

9

10 11 12 13 14 15 16 17 18 19 20
N
0.02
0.018

Figure 4 Diabetes Classification. Tolerance
0.5000. The x-axis represents the number of neurons N and the y-axis represents the mean square
error (MSE) of training. The solutions marked
with  are the final solutions belonged to the
Pareto-set.

0.016
0.014

MSE

0.012
0.01
0.008

0.25

0.006
0.004

0.245
0.002
0

0.24

2

3

4

5

6

7

8

9

10

11

MSE

N

0.235

Figure 6 Function Approximation. Tolerance
0.0001. The x-axis represents the number of neurons N and the y-axis represents the mean square
error (MSE) of training. The solutions marked
with  are the final solutions belonged to the
Pareto-set.

0.23

0.225

0.22

10

15

20

25

N

6
Figure 5 Liver Disorder Classification. Tolerance
0.100. The x-axis represents the number of neurons N and the y-axis represents the mean square
error (MSE) of training. The solutions marked
with  are the final solutions belonged to the
Pareto-set.

This paper presented structure optimization for
the recurrent RNN. We developed a multiobjective approach to find solutions in the ParetoSet that can be used to determine the number of
neurons in the RNN. Usually the number of neurons is chosen arbitrary. In this paper, we establish a procedure that offers to the user a set of
possible solutions to be chosen. Moreover, it was
also presented three ways to decide which best solution can be chosen.
The method is based on creation of Pareto-set
from a multiobjective optimization that aims at
minimizing the mean square error and the number of neurons. In the first of four study cases,
we showed that this method is able to identify a
Pareto-set in which the precise solution is present.
Two classification problems are also investigated,

et al., 2011) has achieved 35. In both two cases
of classification, this method was suitable to point
out the minimum feasible number of N as the best
solution for the RNN.
5.4

Conclusion

Function Approximation

The fourth case study is piecewise function described as in Eq. (11). For this single-input and
single-output system we have used 25 examples.

3867

Anais do XX Congresso Brasileiro de Automática
Belo Horizonte, MG, 20 a 24 de Setembro de 2014

in which the results shown compatibility with the
solutions present in the literature. The last case,
present three solutions in the Pareto-set, which
the best solution can be chosen by means of one
of the three decision making criteria presented.
The main contribution of this work was to
identify a possibility of optimizing the number of
neurons of the RNN. To the best knowledge of this
author, this is the first attempt to select the best
number of neurons of the RNN. In future works, it
is desired to compare this results with other mathematical representations, such as NARMAX, and
to apply other optimization techniques to build
the Pareto-Set, such as genetic algorithms.

Interactive and evolutionary approaches, Vol.
5252, Berlin Springer.
Chankong, V. and Haimes, Y. (1983). Multiobjective decision making theory and methodology, New York Noth-Holland .
Chong, E. K. and Zak, S. H. (2013). An introduction to optimization, Vol. 76, New York
John Wiley  Sons.
Costa, M. A., Braga, A. P., Menezes, B. R., Teixeira, R. A. and Parma, G. G. (2003). Training
neural networks with a multi-objective sliding mode control algorithm, Neurocomputing
51 467473.

Acknowledgments

Deb, K., Pratap, A., Agarwal, S. and Meyarivan, T. (2002). A fast and elitist multiobjective genetic algorithm NSGA-II, Evolutionary Computation, IEEE Transactions on
6(2) 182197.

This work was developed during a research visit
at the Intelligent Systems and Networks Group,
Dept. Electrical and Electronic Engineering, Imperial College London. I am in debt to Prof.
Erol Gelenbe, who gently indicated this research
field and gave to me important insights. It
was supported by Postdoctoral Scholarship from
CNPqINERGE, the Brazilian National Council
for Science and Technology Development. Thanks
to Fapemig for its financial support to present this
work.

Desideri, J.-A. (2012). Multiple-gradient descent
algorithm (MGDA) for multiobjective optimization, Comptes Rendus Mathematique
350(5) 313318.
Diwekar, U. (2008). Introduction to applied optimization, Vol. 22, Berlin Springer.
Gelenbe, E. (1989).
Random neural networks with negative and positive signals and
product form solution, Neural Computation
1(4) 502510.

References
Abbass, H. A. (2003). Speeding up backpropagation using multiobjective evolutionary algorithms, Neural Computation 15(11) 2705
2726.

Gelenbe, E. (1990). Stability of the random neural
network model, Neural Computation 2 239
247.

Abdelbaki, H. (1999). Random neural network
simulator for use with Matlab, Technical report, University of Central Florida.

Gelenbe, E. (1993). Learning in the Recurrent
Random Neural Network, Neural Computation 5(1) 154164.

Aguilar, J. and Gelenbe, E. (1997). Task assignment and transaction clustering heuristics for distributed systems, Information Sciences 97(1) 199219.

Gelenbe, E. (2009). Steps toward self-aware
networks, Communications of the ACM
52(7) 6675.

Alba, E. and Chicano, J. F. (2004). Training
neural networks with GA hybrid algorithms,
Genetic and Evolutionary Computation
GECCO 2004, Springer, pp. 852863.

Gelenbe, E. and Cramer, C. (1998). Oscillatory
corthico-thalamic response to somatosensory
input, Biosystems 48(1-3) 6775.

Bache,
K. and Lichman,
M. (2013).
UCI
machine
learning
repository.
httparchive.ics.uci.eduml.

Gelenbe, E. and Hussain, K. F. (2002). Learning in the multiple class random neural network, Neural Networks, IEEE Transactions
on 13(6) 12571267.

Basterrech, S., Mohammed, S., Rubino, G. and
Soliman, M. (2011). Levenberg  marquardt
training algorithms for random neural networks, The Computer Journal 54(1) 125
135.

Gelenbe, E., Koubi, V. and Pekergin, F. (1993).
Dynamical random neural network approach
to the traveling salesman problem, Systems,
Man and Cybernetics, 1993.Systems Engineering in the Service of Humans, Conference Proceedings., International Conference
on, IEEE, pp. 630635.

Branke, J., Deb, K., Miettinen, K. and Slowinski, R. (2008). Multiobjective optimization

3868

Anais do XX Congresso Brasileiro de Automática
Belo Horizonte, MG, 20 a 24 de Setembro de 2014

Gelenbe, E., Liu, P. and Laine, J. (2006). Genetic algorithms for route discovery, Systems,
Man, and Cybernetics, Part B Cybernetics,
IEEE Transactions on 36(6) 12471254.

network training algorithms, Technical Report 2194, Fakultat fur Informatik, Universitat Karlsruhe, D-76128 Karlsruhe, Germany. Anonymous FTP pubpaperstechreports19941994-21.ps.Z on ftp.ira.uka.de.

Gelenbe, E. and Loukas, G. (2007). A self-aware
approach to denial of service defence, Computer Networks 51(5) 12991314.

Riedmiller, M. and Braun, H. (1993). A direct
adaptive method for faster backpropagation
learning The RPROP algorithm, Neural
Networks, 1993., IEEE International Conference on, IEEE, pp. 586591.

Gelenbe, E., Mao, Z. and Li, Y. (1999). Function approximation with spiked random networks, IEEE Transactitons on Neural Networks 10(1) 39.

Shayib, M. A. (2013). Applied Statistics, Bookboon. ISBN 978-87-403-0493-0.

Gelenbe, E., Sakellari, G. and Darienzo, M.
(2008). Admission of QoS aware users in
a smart network, ACM Transactions on
Autonomous and Adaptive Systems (TAAS)
3(1) 4.

Takahashi, R. H., Peres, P. L. and Ferreira, P. A.
(1997). Multiobjective H2H-inf guaranteed
cost PID design, Control Systems, IEEE
17(5) 3747.
Timotheou, S. (2010). The Random Neural
Network A Survey, The computer journal
53(3) 251267.

Gelenbe, E., Sungur, M., Cramer, C. and Gelenbe, P. (1996). Traffic and video quality
with adaptive neural compression, Multimedia Systems 4(6) 357369.

Vieira, D. A., Vasconcelos, J. A. and Caminhas,
W. M. (2007). Controlling the parallel layer
perceptron complexity using a multiobjective
learning algorithm, Neural Computing and
Applications 16(4-5) 317325.

Gelenbe, E. and Timotheou, S. (2008). Synchronized interactions in spiked neuronal networks, The Computer Journal 51(6) 723
730.
Georgiopoulos, M., Li, C. and Kocak, T. (2011).
Learning in the feed-forward random neural network A critical review, Performance
Evaluation 68(4) 361384.
Likas, A. and Stafylopatis, A. (2000). Training the
random neural network using quasi-newton
methods, European Journal of Operational
Research 126(2) 331339.
Marler, R. T. and Arora, J. S. (2004). Survey
of multi-objective optimization methods for
engineering, Structural and multidisciplinary
optimization 26(6) 369395.
Martins, S. A. M., Nepomuceno, E. G. and Barroso, M. F. S. (2013). Improved structure detection for polynomial NARX models using a
multiobjective error reduction ratio, Journal
of Control, Automation and Electrical Systems 24 764772.
Miettinen, K. and Makela, M. M. (2002). On
scalarizing functions in multiobjective optimization, OR spectrum 24(2) 193213.
Nepomuceno, E. G. (2013). Multiobjective learning in the random neural network, International Journal of Advanced Intelligence
Paradigms 6(1) 6680.
Prechelt, L. (1994). PROBEN1  A set of benchmarks and benchmarking rules for neural

3869